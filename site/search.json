[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LiaoBingJie's Blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nCategories\n\n\n\n\n\n\n\n\n2025-10-16\n\n\nAbout Me\n\n\n \n\n\n\n\n\n\n2025-10-16\n\n\nPractical Econometrics with R\n\n\nEconometrics\n\n\n\n\n\n\n2025-10-16\n\n\nPractical Finance with R\n\n\nFinance\n\n\n\n\n\n\n2025-10-16\n\n\nPractical ML with R\n\n\nMachine Learning\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/about/about.html",
    "href": "posts/about/about.html",
    "title": "About Me",
    "section": "",
    "text": "我是廖冰杰，1988年生，广西桂林人。教育背景是：广西警察学院（大专，公安学） —— 公安大学（本科，公安学） —— 人民大学（硕士，公共管理）；职业背景是：某市公务员 —— 某大型房企审计 —— 某大型互联网金融审计 —— 自媒体教育创业。经过这么些年的工作、学习、生活，我对自己的未来和工业规划有了更深入的理解；想利用自己学习和工作的经历做一些公开课，从《计量经济学》为核心内容开展，包括多元回归 LM、双重差分 DID、时间序列 ARIMAX 三个主要板块。每个板块以原理性、理解性、应用性的视角开展讨论，而直接跳过数学公式证明和算法编辑，直接采取“拿来主义”，暂时不考虑底层逻辑，先把案例跑起来让同学们产生抽象的感性认知。我们是乘坐飞机的乘客而不是驾驶飞机的飞行员，只要飞机把我们安全送到目的地即可，我们完全不需要考虑飞行员、飞机信号、地面塔台的复杂关系。搞清楚这个原则，接下来所有的学习都会变得具体而明确。\n\n\n\n在宏观层面，我国高等教育资源失衡已经非常严重，北京上海两个城市占据了绝对优势的高等教育，再加上高考的省级名额限制，对西部孩子们形成了一道隐形的长城。中西部地区许多同学因为客观原因无法享受到优秀的高等教育，他们也有自己丰富美好的人生理想，但是他们的理想却被现实和政策所阻隔，这种隔阂在经济学领域尤为严重。《计量经济学》被当做宗教神器一样供奉在暗黑神坛上，暗黑法师在夜幕中故作玄虚、电闪雷鸣、狂风骤起、整个场面十分玄幻。我的使命是将它从邪恶神坛上拽下来，一把火烧掉它“欲练此功必先自宫”的迷信披风、摧毁它封建迷信的宗教崇拜；我深知无法从根本上改变教育资源失衡的现状，但至少可以通过自己小小努力帮助到更多在黑暗中摸索的孩子们；若能将教育公平正义向前推进一小步，“虽千万人，吾往矣。”\n\n\n\n在微观层面，我国高等教育的经济类课程依然采用理论主导的上课方式，老师们花很多时间精力在证明各种理论，对此我持保留意见；教员很久以前就指示我们“以歼灭敌人有生力量为主要目标，不以保守和夺取地方为主要目标”(毛泽东 1946)。因此我认为高等教育应该以“启发、探索、引导”为主，鼓励同学们主动发现事物背后的规律，这也是 James Stewart 教授所推崇的方法 (Stewart 2021)。我在每章分别关注一个独立完整的案例，由浅入深、提出问题、分析思路、寻找答案；这样的思路很像公安局调查刑事案件，让同学们以“经济侦查”的视角去调查案件，从最初的一团迷雾着手、逐步排除掉错误假设、逐渐逼近客观真相，锁定“嫌疑人”以及它所采取的“犯罪路径”；最终培养严密的“以事实为依据、以法律为准绳”的实证思维。\n\n\n\n在可持续化层面，我深知仅凭热情是远远不够的，但没有热情是注定失败的；因此成立了一家小企业：“德业恒新”，我希望不仅仅帮助同学们通过考试、发表论文、顺利毕业，这些都太短浅了；而是希望以更系统的方法帮助更多的同学们实现更远大的人生志向，让大家“恒守其德、日新其业”，成为更好的自己。本课程的视频和讲义都免费、开源、公开，欢迎个人随意使用、禁止商用；欢迎自媒体转发宣传，请注明出处；欢迎中小企业捐赠教育基金、接受个人小额捐款。所有捐赠通过公益基金管理，每季度公布财务状况，接受社会监督。若有更多盈余，则积极响应总书记号召的“到基层去、到西部去、到祖国最需要的地方去，做成一番事业，做好一番事业。” (习近平 2016) 并持续加大偏远乡村的基础教育投入、设立更多的线下教学点、提供必要的教学设备和生活帮助、将免费先进的基础教育向西部纵深推进。这听起来就像痴人说梦、愚公移山、夸父追日，但谁又能否认“星星之火可以燎原”的历史 (毛泽东 1930)，谁又敢说这个梦一定无法实现？“路虽远，行则将至；事虽难，做则必成。”(习近平 2013)\n\n博客官网：XXXXX\nB 站认证号：XXXXX\n淘宝天猫账号：XXXXX\n小红书认证号：XXXXX\n公益捐赠账号：XXXXX"
  },
  {
    "objectID": "posts/about/about.html#about-me",
    "href": "posts/about/about.html#about-me",
    "title": "About Me",
    "section": "",
    "text": "我是廖冰杰，1988年生，广西桂林人。教育背景是：广西警察学院（大专，公安学） —— 公安大学（本科，公安学） —— 人民大学（硕士，公共管理）；职业背景是：某市公务员 —— 某大型房企审计 —— 某大型互联网金融审计 —— 自媒体教育创业。经过这么些年的工作、学习、生活，我对自己的未来和工业规划有了更深入的理解；想利用自己学习和工作的经历做一些公开课，从《计量经济学》为核心内容开展，包括多元回归 LM、双重差分 DID、时间序列 ARIMAX 三个主要板块。每个板块以原理性、理解性、应用性的视角开展讨论，而直接跳过数学公式证明和算法编辑，直接采取“拿来主义”，暂时不考虑底层逻辑，先把案例跑起来让同学们产生抽象的感性认知。我们是乘坐飞机的乘客而不是驾驶飞机的飞行员，只要飞机把我们安全送到目的地即可，我们完全不需要考虑飞行员、飞机信号、地面塔台的复杂关系。搞清楚这个原则，接下来所有的学习都会变得具体而明确。"
  },
  {
    "objectID": "posts/about/about.html#why-this-course",
    "href": "posts/about/about.html#why-this-course",
    "title": "About Me",
    "section": "",
    "text": "在宏观层面，我国高等教育资源失衡已经非常严重，北京上海两个城市占据了绝对优势的高等教育，再加上高考的省级名额限制，对西部孩子们形成了一道隐形的长城。中西部地区许多同学因为客观原因无法享受到优秀的高等教育，他们也有自己丰富美好的人生理想，但是他们的理想却被现实和政策所阻隔，这种隔阂在经济学领域尤为严重。《计量经济学》被当做宗教神器一样供奉在暗黑神坛上，暗黑法师在夜幕中故作玄虚、电闪雷鸣、狂风骤起、整个场面十分玄幻。我的使命是将它从邪恶神坛上拽下来，一把火烧掉它“欲练此功必先自宫”的迷信披风、摧毁它封建迷信的宗教崇拜；我深知无法从根本上改变教育资源失衡的现状，但至少可以通过自己小小努力帮助到更多在黑暗中摸索的孩子们；若能将教育公平正义向前推进一小步，“虽千万人，吾往矣。”"
  },
  {
    "objectID": "posts/about/about.html#how-to-learn",
    "href": "posts/about/about.html#how-to-learn",
    "title": "About Me",
    "section": "",
    "text": "在微观层面，我国高等教育的经济类课程依然采用理论主导的上课方式，老师们花很多时间精力在证明各种理论，对此我持保留意见；教员很久以前就指示我们“以歼灭敌人有生力量为主要目标，不以保守和夺取地方为主要目标”(毛泽东 1946)。因此我认为高等教育应该以“启发、探索、引导”为主，鼓励同学们主动发现事物背后的规律，这也是 James Stewart 教授所推崇的方法 (Stewart 2021)。我在每章分别关注一个独立完整的案例，由浅入深、提出问题、分析思路、寻找答案；这样的思路很像公安局调查刑事案件，让同学们以“经济侦查”的视角去调查案件，从最初的一团迷雾着手、逐步排除掉错误假设、逐渐逼近客观真相，锁定“嫌疑人”以及它所采取的“犯罪路径”；最终培养严密的“以事实为依据、以法律为准绳”的实证思维。"
  },
  {
    "objectID": "posts/about/about.html#sustainability",
    "href": "posts/about/about.html#sustainability",
    "title": "About Me",
    "section": "",
    "text": "在可持续化层面，我深知仅凭热情是远远不够的，但没有热情是注定失败的；因此成立了一家小企业：“德业恒新”，我希望不仅仅帮助同学们通过考试、发表论文、顺利毕业，这些都太短浅了；而是希望以更系统的方法帮助更多的同学们实现更远大的人生志向，让大家“恒守其德、日新其业”，成为更好的自己。本课程的视频和讲义都免费、开源、公开，欢迎个人随意使用、禁止商用；欢迎自媒体转发宣传，请注明出处；欢迎中小企业捐赠教育基金、接受个人小额捐款。所有捐赠通过公益基金管理，每季度公布财务状况，接受社会监督。若有更多盈余，则积极响应总书记号召的“到基层去、到西部去、到祖国最需要的地方去，做成一番事业，做好一番事业。” (习近平 2016) 并持续加大偏远乡村的基础教育投入、设立更多的线下教学点、提供必要的教学设备和生活帮助、将免费先进的基础教育向西部纵深推进。这听起来就像痴人说梦、愚公移山、夸父追日，但谁又能否认“星星之火可以燎原”的历史 (毛泽东 1930)，谁又敢说这个梦一定无法实现？“路虽远，行则将至；事虽难，做则必成。”(习近平 2013)\n\n博客官网：XXXXX\nB 站认证号：XXXXX\n淘宝天猫账号：XXXXX\n小红书认证号：XXXXX\n公益捐赠账号：XXXXX"
  },
  {
    "objectID": "posts/about/about.html#uniformity",
    "href": "posts/about/about.html#uniformity",
    "title": "About Me",
    "section": "2.1 Uniformity",
    "text": "2.1 Uniformity\nR 的 tidyverse 经过近十年的高速发展，已经非常丰富成熟，是当前数据清理维度当之无愧的“蓝波汪”；R 的语法类似初中英语，采用动词主导、管道符链接的形式，逻辑清晰，上下文非常不言而喻。比如 df |&gt; select(names, sex, city) |&gt; filter(city == \"shanghai\") |&gt; rename(gender = sex) 这一段代码非常清晰地执行了几个命令，首先读取某个数据集 df 然后选择其中三列 names, sex, city 在筛选其中生源地是 上海 的同学，最后把 sex 更名为 gender。当然这只是 R 的冰山一角，基于 tidyverse 的语法产生了很多特别方便好用简单粗暴的工具包，具体内容我们暂时不深入讨论。反观 Python 的初期学习成本较大，配置虚拟环境就劝退很多非计算机的同学，另外 pandas 虽然初期上手容易，但是在相当多的细节之处存在明显的随意，对初学者的接受度和整体流畅度欠缺考虑。"
  },
  {
    "objectID": "posts/about/about.html#beauty",
    "href": "posts/about/about.html#beauty",
    "title": "About Me",
    "section": "2.2 Beauty",
    "text": "2.2 Beauty\nR 的 ggplot2 绘图就像乐高积木或者油画一样，一层一层堆叠各种需要的元素，每一层都是彼此独立的内容，轻松实现出版级统计图表；比如我们先绘制空白底层、再增加散点元素、再进行着色、再标注X轴Y轴主标题、再指定主题风格、再调整轴各种其他细节…每一层的操作都是相互独立互不打扰，保证整个出图的流畅性和美观性。在 Python 当中的 seaborn, matplotlib, plotly 等确实也很好用，但是它们在连贯性、流畅性、美观性等维度还有较大欠缺；如果是绘制普通图表，它们的差异性还不会很大，但是涉及到复杂图表，Python 的语法结构会明显比 ggplot2 更复杂和难以维护。另外 python 最近也有了 lets-plot 工具包，完完全全是 ggplot2 的镜像复刻，但这里面最大的问题不在于他们自身，而是底层跟其他工具包的兼容性问题，这是很大的挑战。"
  },
  {
    "objectID": "posts/about/about.html#statistics",
    "href": "posts/about/about.html#statistics",
    "title": "About Me",
    "section": "2.3 Statistics",
    "text": "2.3 Statistics\nR 的统计模型已经非常丰富，略带夸张地说“只有我们想不到，没有它做不到”。从最基础的多元回归 glm 到稍微复杂的双重差分 fixest 再到时间序列的 fpp3 再到火热的机器学习 mlr3，并且每一个工具包都是实实在在可以投入实战和生产环境直接带来效益的产品。虽然 Python 也有对应的工具包，比如 statsmodels, pyfixest, sktime 等等，但它们都是在 R 的原装基础上衍生而来，乍看之下很完美，一跑起来容易发生一些奇奇怪怪的兼容性小问题，虽然经过一些周折，也不是不能用，但是在小范围项目里的体验感觉要远差于 R 的原装工具包。"
  },
  {
    "objectID": "posts/about/about.html#community",
    "href": "posts/about/about.html#community",
    "title": "About Me",
    "section": "2.4 Community",
    "text": "2.4 Community\nWickham 横空出世，完成 tidyverse 的同时顺便写了两本书 R4DS 和 Advanced R，字里行间透露着浓浓的柏拉图式理想主义，其中有一段话给我初读的时候带来了很大的感动：\n\nThis book is my attempt to pass on what I’ve learned so that you can understand the intricacies of R as quickly and painlessly as possible. Reading it will help you avoid the mistakes I’ve made and dead ends I’ve gone down, and will teach you useful tools, techniques, and idioms that can help you to attack many types of problems.\n\n不管是有意还是无意，后面陆续加入了许多作者和他们创作的神包：\n\nRBookDown: 基于 rmd 格式而形成的许多重要而免费的教材，包括了 R 语言使用的方方面面，是 R 社区最大的宝藏；python 整体氛围越来越商业化、培训化、订阅化，而距离它最初的开源免费的初心有了很大的变化，并且它的社区在学术交流层面做的很封闭。\nRMarkDown: 将 md 语法和 R 语言完美结合起来，实现了一个 .rmd 文档可以包含文字、数学公式、表格、仅展示的代码、可执行的代码、输出结果、图片、超链接、等等元素。\nmodelsummary: 一个提取各种模型结果并进行美化输出的神包。\nmarginaleffects: 一个进行模型边际效益分析的神包，特别是面对非线性模型和异质性分析时，非常好用。\nfixest: 一个进行高维固定效应回归的神包，特别适合双重差分 DID 分析，经管人居家旅行必备良品。\ntidymodels: 基于 tidyverse 语法的机器学习工具包集合，涵盖了数据预处理、模型训练、模型调优、模型评估等各个环节，很简单但是非常实用。\ntidyverts: 一个进行时间序列分析的神包，独创的 tsibble 数据结构特别适合时间序列数据的处理和分析，完美遗传了 lubridate 的时间处理能力并且进一步融入了 tibble 的数据处理能力，这个包是博士阶段用来处理非金融时间序列分析的万金油。\ntidyquant: 一个进行量化金融分析的神包，完美融合了 tidyverse 和 quantmod 的能力，特别适合初级量化金融实战，但不是很适合高精度、高颗粒度、偏理论层面的量化金融分析。\ntsmodels: 一个进行金融时间序列分析的神包，包含了各种金融时间序列分析的工具和模型，非常适合高精度的量化金融实战，但是这个包的开发到目前还没有达到完全体的状态，期待作者能加速更新。\n\n这些工具包对于经管类的科研工作有很大的帮助，当然 R 的生态圈远不止于此，但是我们暂时不讨论太深入太复杂的内容。隔壁 Python 的工作流更偏向业界，高质量的免费教材很少，社区维护也集中在“高精尖”的包当中，而对于初级使用的包关注度很低。"
  },
  {
    "objectID": "posts/about/about.html#power",
    "href": "posts/about/about.html#power",
    "title": "About Me",
    "section": "2.5 Power",
    "text": "2.5 Power\nPython 借助 transformer, pytorch, CUDA 生态，在超大规模机器学习、深度学习、神经网络等场景具有明显优势，这一点是 R 最大的缺点，我们必须承认对方的实力强大。但在经管研究的大多数应用中，数据规模与模型复杂度通常不至于成为瓶颈，R 在 1 GB 左右的数据集处理速度时跟 Python 几乎没有性能差别，只有当数据超过 10 GB 的体量，Python 基于 CUDA 的算力优势才会逐渐体现出来。对大多数学术工作而言，R 现有的性能已完全满足需求，并且它的语法更简洁、更人性化、更流畅，非常适合经管类的学术科研工作。至于 stata，虽然它依然活跃在计量经济学领域，但它在复现文档、生态扩展、工程融合、编程效率方面都存在较大落后，因此暂时不讨论它。"
  },
  {
    "objectID": "posts/about/about.html#linear-models",
    "href": "posts/about/about.html#linear-models",
    "title": "About Me",
    "section": "3.1 Linear Models",
    "text": "3.1 Linear Models\n\n如何用人类语言描述一个 LM 案例？\n如何在 R 中进行一个完整的 Linear Model 分析流程？\n什么是外生性 (Exogeneity) 和内生性 (Endogeneity)，以及如何检验？\n什么是异质性 (Heterogeneity) 和同质性 (Homogeneity)，以及如何检验？\n什么是异方差性 (Heteroscedasticity) 和同方差性 (Homoscedasticity)，以及如何检验？\n什么是残差检验 (Residual Analysis)，以及如何检验？"
  },
  {
    "objectID": "posts/about/about.html#diff-in-diff",
    "href": "posts/about/about.html#diff-in-diff",
    "title": "About Me",
    "section": "3.2 Diff-in-Diff",
    "text": "3.2 Diff-in-Diff\n\n如何用人类语言描述一个 DID 案例？\n如何在 R 中进行一个完整的 DID 分析流程？\n什么是事件检验 (Event Study)，以及如何验证？\n什么是安慰剂检验 (Placebo Tests)，以及如何验证？\n什么是交错DID (Staggered DID)，以及如何执行它的流程？"
  },
  {
    "objectID": "posts/about/about.html#arima",
    "href": "posts/about/about.html#arima",
    "title": "About Me",
    "section": "3.3 ARIMA",
    "text": "3.3 ARIMA\n\n如何用人类语言描述一个 ARIMA 案例？\n如何在 R 中进行一个完整的 ARIMA 分析流程？\n什么是平稳性 (Stationarity) 和非平稳性 (Non-Stationarity)，以及如何检验？\n什么是自相关性 (Autocorrelation) 和独立性 (Non-Autocorrelation)，以及如何检验？\n什么是单位根检验 (Unit Root Test) 和协整 (Cointegration)，以及如何检验？"
  },
  {
    "objectID": "posts/fina/fina.html",
    "href": "posts/fina/fina.html",
    "title": "Practical Finance with R",
    "section": "",
    "text": "首先调取如下的工具包，暂时不做过多解释，每个都有其独特的功能，后续章节会逐步用到它们。\n\n\nShow Code\n# install and load pacman\nif (!requireNamespace(\"pacman\", quietly = TRUE)) {\n    install.packages(\"pacman\")\n}\n\n\n# load all common packages\npacman::p_load(\n    tidyverse, modelsummary, flextable, ggdag, dagitty, ggsci, ivreg, \n    marginaleffects, scales, pandoc, car, broom, nortest, lmtest, \n    patchwork, sandwich, GGally, fixest, ggfixest, fpp3, ggforce, urca, \n    readxl, writexl, tidyquant, PortfolioAnalytics, tsgarch, equatags,\n    ROI.plugin.quadprog, ROI.plugin.glpk, here\n)\n\n\n通过 tidyquant 包获取美国重要的股票的历史价格并保存到本地 Excel 文件，后续章节均基于该数据进行分析，免去每次都在线读取数据的麻烦，以便非常快速开启分析过程。一旦文件生成，后续就不需要再运行下面的代码了，所以我已经将他们注释掉了，我们直接往下开展分析工作。\n\n\nShow Code\n# read stocks price from tidyquant and save to local excel file\n# (\n#     tq_get(\n#         x = c(\n#             \"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"NVDA\", \"META\", \"V\",\n#             \"JNJ\", \"WMT\", \"JPM\", \"UNH\", \"PG\", \"MA\", \"XOM\", \"HD\", \"NKE\",\n#             \"PFE\", \"KO\", \"ABBV\", \"CVX\", \"CRM\", \"INTC\", \"DIS\", \"AVGO\"\n#         ),\n#         get = \"stock.prices\",\n#         from = \"2015-01-01\",\n#         to = \"2025-01-01\"\n#     ) %&gt;%\n#     group_by(symbol) %&gt;%\n#     tq_transmute(\n#         select = adjusted,\n#         mutate_fun = periodReturn,\n#         period = \"monthly\",\n#         col_rename = \"returns\"\n#     ) %&gt;%\n#     drop_na() %&gt;%\n#     select(symbol, date, returns) %&gt;%\n#     write_xlsx(\"./datasets/us_stocks_returns.xlsx\")\n# )\n\n\n从本地读取数据并将原始收盘价格转换成为股票收益率，金融分析的核心问题就是收益和风险的均衡，即 Returns vs Risks。许多专业文献已经证明了，金融市场不存在高收益低风险的投资，但是存在很多高风险低收益的投资，但实际上最多的是中低风险和中低收益的投资。\n\n\nShow Code\n# read data\nus_stocks = readxl::read_xlsx(here::here(\"datasets\", \"us_stocks_returns.xlsx\"))\n\n\n# plot return per symbol\n(\n    us_stocks %&gt;% \n    ggplot(aes(x = date, y = returns)) +\n    geom_line() +\n    labs(x = \"\", y = \"\", title = \"Monthly Returns of Top Companies\") +\n    theme_light() +\n    facet_wrap(~ symbol)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n收益就是股票价格波动的比例，而风险就是收益数列的方差；\n\\[\n\\begin{aligned}\n    \\mu_i &= \\frac{p_t - p_{t-1}}{p_{t-1}} \\\\\n    \\sigma_i &= \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (\\mu_i - \\bar{\\mu})^2}\n\\end{aligned}\n\\]\n假如单个股票的收益为 \\(E(R_i)\\)，权重为 \\(w_i\\)，共有 \\(n\\) 支股票；那么投资组合的收益 \\(\\mu_p\\) 和方差 \\(\\sigma_p\\) 就是所有单个股票对应的加权均值；\n\\[\n\\begin{aligned}\n\\mu_p &= \\sum_{i=1}^n w_i E(R_i) \\\\\n\\sigma^2_p &=\n    \\sum_{i=1}^{n} w_i^2 \\sigma^2_i +\n    \\sum_{i=1}^{n} \\sum_{j \\neq i}^n w_i w_j \\sigma_{ij}\n\\end{aligned}\n\\]\n投资最优就简化成以下两个殊途同归的方向，要么在保持低风险的同时尽量追求高收益，要么在保持高收益的同时尽量压缩风险。但是无论哪个方向，它们最终都是落地到同一个数值上，即求解最大的 Sharpe-Ratio：\n\\[\n\\text{Max Sharpe} = \\frac{\\mu_p - R_f}{\\sigma_p}\n\\]\n我们根据当前数据，进行投资预测。\n\n\nShow Code\n# aggregate to mean and stdv\nus_stocks_aggr = (\n    us_stocks %&gt;% \n    group_by(symbol) %&gt;% \n    summarise(\n        mean_returns = mean(returns), \n        stdv_returns = SD(returns)\n    )\n)\n\n\n\n# convert long to wide to xts\nus_stocks_xts = (\n    us_stocks %&gt;% \n    pivot_wider(\n        names_from = symbol, \n        values_from = returns\n    ) %&gt;% \n    as.xts()\n)\n\n\n# create a portfolio specification with constraints and objectives\nportfolio = (\n    portfolio.spec(assets = colnames(us_stocks_xts)) %&gt;%\n    add.constraint(type = \"full_investment\") %&gt;%\n    add.constraint(type = \"long_only\") %&gt;%\n    add.objective(type = \"risk\", name = \"StdDev\") %&gt;%\n    add.objective(type = \"return\", name = \"mean\") %&gt;% \n    add.constraint(type = \"box\", min = 0, max = 0.3)\n)\n\n\n# fit the Mean-Var-Opt model\nportfolio_opt = optimize.portfolio(\n    R = us_stocks_xts,\n    portfolio = portfolio,\n    optimize_method = \"ROI\",\n    trace = TRUE\n)\n\n\n# show optimized weights\n(\n    tibble(\n        symbol = portfolio_opt %&gt;% extractWeights() %&gt;% names(),\n        weights = portfolio_opt %&gt;% extractWeights()\n    ) %&gt;% \n    filter(weights &gt; 0.05) %&gt;%\n    datasummary_df()\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                symbol\n                weights\n              \n        \n        \n        \n                \n                  AMZN\n                  0.11\n                \n                \n                  TSLA\n                  0.29\n                \n                \n                  NVDA\n                  0.30\n                \n                \n                  AVGO\n                  0.30\n                \n        \n      \n    \n\n\n\n有了上述参数后，代入计算 Efficient Frontier 和最大 Sharpe 点，即得到如下曲线。\n\n\nShow Code\n# calculate efficient frontier and max sharpe\nef_tidy = (\n    extractEfficientFrontier(portfolio_opt, n.portfolios = 50) %&gt;% \n    pluck(\"frontier\") %&gt;% \n    unclass() %&gt;% \n    as_tibble() %&gt;% \n    rename(return = mean, risk = ES) %&gt;% \n    mutate(sharpe = (return - 0.01) / risk)\n)\n\n\n# plot efficient frontier and max sharpe\n(\n    ggplot() +\n    geom_point(\n        data = us_stocks_aggr, \n        aes(x = stdv_returns, y = mean_returns), \n        color = \"gray\"\n    ) +\n    geom_text(\n        data = us_stocks_aggr, \n        aes(x = stdv_returns, y = mean_returns, label = symbol), \n        color = \"gray\"\n    )+ \n    geom_line(\n        data = ef_tidy, \n        aes(x = risk, y = return)\n    ) +\n    geom_point(\n        data = ef_tidy %&gt;% slice_max(sharpe),\n        aes(x = risk, y = return),\n        color = \"steelblue\", size = 4, shape = 16,\n    ) +\n    labs(\n        x = \"Risks\", y = \"Returns\", \n        title = \"Portfolio Optimization of Top US Stocks\",\n        caption = \"Risks: Stdv of Returns \\n Returns: Mean of Returns\"\n    ) +\n    theme_light()\n)\n\n\n\n\n\n\n\n\n\n\n\n\nto be continued…\n\n\n\n\n\n\n上一节讨论的 ARIMA 模型处理的数据是 stationary 且跟踪样本的均值，这个模型比较适用于除了金融数据以外的宏观层面的具有一定周期性的数据；但是如果一个数据是金融类的、微观的、不具有显著周期性的，那是否有模型可以进行一定程度的预测？这就是我们本节进行讨论的 GARCH 模型，它对比 ARIMA 有非常重要的区别，首先 GARCH 模型处理的数据是不平稳的 non-stationary 即意味着它在某种程度上不符合正态分布，或者说不严格符合正态分布，其次它跟踪的对象是样本的方差，而不是均值，即意味着它更加看重样本的波动性，而不是平均值的高低大小。\nGARCH 计算理论很深奥且仅限于金融工程专业，简要过一遍模型即可，不对其进行深入讨论了；GARCH(p, q) 受限于两个组成部分：AR(p)和MA(q)。\n计算整个序列的方差：\n\\[\n\\text{Base} = Var(y_t) = \\alpha_0\n\\]\n计算过去某些期数的AR数值：\n\\[\nAR(p) = \\sum_{i = 1}^p \\alpha_i e_{t - 1}^2\n\\]\n其中 \\(\\alpha_1\\) 是过去方差对当前方差的短期冲击效应，理论上，这个数值越小越好。\n计算往期数据的 MA 数值：\n\\[\nMA(q) = \\sum_{j = 1}^q \\beta_j \\sigma_{t - j}^2\n\\]\n其中 \\(\\beta_1\\) 是过去方差对未来方差的长期冲击效应，理论上，这是个数值是越大越好。\n将所有部分相加，即可得到GARCH模型：\n\\[\n\\begin{align}\n\\text{Dif(d)} &= \\text{Base} + AR(p) + MA(q) \\\\\n\\sigma_t^2\n&=\n    \\alpha_0 +\n    \\sum_{i=1}^{q} \\beta_i \\epsilon_{t-i}^2 +\n    \\sum_{j=1}^{p} \\gamma_j \\sigma_{t-j}^2\n\\end{align}\n\\]\n在数学公式的基础上，结合前文已经整理的数据组，对 MSFT 单只股票进行 GARCH 建模。\n\n\nShow Code\n# convert tibble to xts form\nus_stocks_xts = (\n    us_stocks %&gt;% \n    pivot_wider(names_from = \"symbol\", values_from = \"returns\") %&gt;% \n    as.xts()\n)\n\n\n# create GARCH model\ngarch_msft = garch_modelspec(\n    y = us_stocks_xts$MSFT, \n    model = \"garch\", \n    constant = TRUE, \n    init = \"unconditional\", \n    distribution = \"jsu\"\n)\n\n\n# show coefficients\ngarch_msft_coef = garch_msft %&gt;% estimate()\n\ngarch_msft_coef %&gt;% summary() %&gt;% as_flextable()\n\n\nEstimateStd. Errort valuePr(&gt;|t|) \nμ\\muμ0.02440.00534.62180.0000*** \nω\\omegaω0.00040.00031.36000.1738 \nα1\\alpha_1α1​0.11260.09471.18880.2345 \nβ1\\beta_1β1​0.76970.12965.93900.0000*** \nζ\\zetaζ3.343517.10840.19540.8451 \nν\\nuν10.000021.20030.47170.6371 \nPPP0.88230.083710.54440.0000***Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1variance targeting: FALSEinitialization value: 0.003982LogLik: 163.5736AIC: -313.1 | BIC: -293.6Model Equation \nεt∼JSU(0,σt,ζ,ν)\\varepsilon_t \\sim JSU\\left(0,\\sigma_t,\\zeta, \\nu\\right)εt​∼JSU(0,σt​,ζ,ν) \nσt2=ω+α1εt−12+β1σt−12\\sigma^2_t = \\omega + \\alpha_1\\varepsilon^2_{t-1} + \\beta_1\\sigma^2_{t-1}σt2​=ω+α1​εt−12​+β1​σt−12​ Persistence (P) and Unconditional Variance Equations \nP=∑j=1qαj+∑j=1pβjP = \\sum_{j=1}^q \\alpha_j + \\sum_{j=1}^p \\beta_jP=∑j=1q​αj​+∑j=1p​βj​ \nE[εt2]=ω1−PE\\left[\\varepsilon^2_t\\right] = \\frac{\\omega}{1 - P}E[εt2​]=1−Pω​ \n\n\nShow Code\ngarch_msft_coef %&gt;% plot()\n\n\n\n\n\n\n\n\n\n\n\n\n继续…\n\n\nShow Code\n# predict 10 periods by 100 simulations\ngarch_msft_pred = predict(garch_msft_coef, h = 10, nsim = 100)\n\n\n# plot by default method, not beautiful, just for quick check\n# garch_msft_pred %&gt;% plot()\n\n\n# plot by ggplot2\n## extract CI to tibble format\ngarch_msft_pred_tidy = (\n    garch_msft_pred %&gt;% \n    pluck(\"distribution\") %&gt;% \n    tsconvert() %&gt;% \n    as_tibble() %&gt;% \n    mutate(\n        date = ymd(index), \n        pred_value = as.numeric(value)\n    ) %&gt;% \n    select(date, pred_value) %&gt;% \n    group_by(date) %&gt;% \n    summarize(\n        pred_mean = mean(pred_value),\n        pred_lo = quantile(pred_value, 0.1),\n        pred_hi = quantile(pred_value, 0.9)\n    )\n)\n\n\n## plot prediction on conditional variance\n(\n    ggplot() + \n    geom_point(\n        data = us_stocks %&gt;% filter(symbol == \"MSFT\"), \n        aes(x = ymd(date), y = returns),\n        color = \"gray\",\n    ) +\n    geom_line(\n        data = garch_msft_pred_tidy,\n        aes(x = date, y = pred_mean), \n        color = \"orange\", linewidth = 2\n    ) + \n    geom_ribbon(\n        data = garch_msft_pred_tidy,\n        aes(x = date, ymin = pred_lo, ymax = pred_hi), \n        fill = \"gray\", alpha = 0.3\n    ) +\n    labs(x = \"\", y = \"\", title = \"GARCH Prediction of MSFT\") +\n    theme_light() +\n    scale_x_date(date_labels = \"%Y-%m\")\n)"
  },
  {
    "objectID": "posts/fina/fina.html#overview",
    "href": "posts/fina/fina.html#overview",
    "title": "Practical Finance with R",
    "section": "",
    "text": "首先调取如下的工具包，暂时不做过多解释，每个都有其独特的功能，后续章节会逐步用到它们。\n\n\nShow Code\n# install and load pacman\nif (!requireNamespace(\"pacman\", quietly = TRUE)) {\n    install.packages(\"pacman\")\n}\n\n\n# load all common packages\npacman::p_load(\n    tidyverse, modelsummary, flextable, ggdag, dagitty, ggsci, ivreg, \n    marginaleffects, scales, pandoc, car, broom, nortest, lmtest, \n    patchwork, sandwich, GGally, fixest, ggfixest, fpp3, ggforce, urca, \n    readxl, writexl, tidyquant, PortfolioAnalytics, tsgarch, equatags,\n    ROI.plugin.quadprog, ROI.plugin.glpk, here\n)\n\n\n通过 tidyquant 包获取美国重要的股票的历史价格并保存到本地 Excel 文件，后续章节均基于该数据进行分析，免去每次都在线读取数据的麻烦，以便非常快速开启分析过程。一旦文件生成，后续就不需要再运行下面的代码了，所以我已经将他们注释掉了，我们直接往下开展分析工作。\n\n\nShow Code\n# read stocks price from tidyquant and save to local excel file\n# (\n#     tq_get(\n#         x = c(\n#             \"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"NVDA\", \"META\", \"V\",\n#             \"JNJ\", \"WMT\", \"JPM\", \"UNH\", \"PG\", \"MA\", \"XOM\", \"HD\", \"NKE\",\n#             \"PFE\", \"KO\", \"ABBV\", \"CVX\", \"CRM\", \"INTC\", \"DIS\", \"AVGO\"\n#         ),\n#         get = \"stock.prices\",\n#         from = \"2015-01-01\",\n#         to = \"2025-01-01\"\n#     ) %&gt;%\n#     group_by(symbol) %&gt;%\n#     tq_transmute(\n#         select = adjusted,\n#         mutate_fun = periodReturn,\n#         period = \"monthly\",\n#         col_rename = \"returns\"\n#     ) %&gt;%\n#     drop_na() %&gt;%\n#     select(symbol, date, returns) %&gt;%\n#     write_xlsx(\"./datasets/us_stocks_returns.xlsx\")\n# )\n\n\n从本地读取数据并将原始收盘价格转换成为股票收益率，金融分析的核心问题就是收益和风险的均衡，即 Returns vs Risks。许多专业文献已经证明了，金融市场不存在高收益低风险的投资，但是存在很多高风险低收益的投资，但实际上最多的是中低风险和中低收益的投资。\n\n\nShow Code\n# read data\nus_stocks = readxl::read_xlsx(here::here(\"datasets\", \"us_stocks_returns.xlsx\"))\n\n\n# plot return per symbol\n(\n    us_stocks %&gt;% \n    ggplot(aes(x = date, y = returns)) +\n    geom_line() +\n    labs(x = \"\", y = \"\", title = \"Monthly Returns of Top Companies\") +\n    theme_light() +\n    facet_wrap(~ symbol)\n)"
  },
  {
    "objectID": "posts/fina/fina.html#sharpe",
    "href": "posts/fina/fina.html#sharpe",
    "title": "Practical Finance with R",
    "section": "",
    "text": "收益就是股票价格波动的比例，而风险就是收益数列的方差；\n\\[\n\\begin{aligned}\n    \\mu_i &= \\frac{p_t - p_{t-1}}{p_{t-1}} \\\\\n    \\sigma_i &= \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (\\mu_i - \\bar{\\mu})^2}\n\\end{aligned}\n\\]\n假如单个股票的收益为 \\(E(R_i)\\)，权重为 \\(w_i\\)，共有 \\(n\\) 支股票；那么投资组合的收益 \\(\\mu_p\\) 和方差 \\(\\sigma_p\\) 就是所有单个股票对应的加权均值；\n\\[\n\\begin{aligned}\n\\mu_p &= \\sum_{i=1}^n w_i E(R_i) \\\\\n\\sigma^2_p &=\n    \\sum_{i=1}^{n} w_i^2 \\sigma^2_i +\n    \\sum_{i=1}^{n} \\sum_{j \\neq i}^n w_i w_j \\sigma_{ij}\n\\end{aligned}\n\\]\n投资最优就简化成以下两个殊途同归的方向，要么在保持低风险的同时尽量追求高收益，要么在保持高收益的同时尽量压缩风险。但是无论哪个方向，它们最终都是落地到同一个数值上，即求解最大的 Sharpe-Ratio：\n\\[\n\\text{Max Sharpe} = \\frac{\\mu_p - R_f}{\\sigma_p}\n\\]\n我们根据当前数据，进行投资预测。\n\n\nShow Code\n# aggregate to mean and stdv\nus_stocks_aggr = (\n    us_stocks %&gt;% \n    group_by(symbol) %&gt;% \n    summarise(\n        mean_returns = mean(returns), \n        stdv_returns = SD(returns)\n    )\n)\n\n\n\n# convert long to wide to xts\nus_stocks_xts = (\n    us_stocks %&gt;% \n    pivot_wider(\n        names_from = symbol, \n        values_from = returns\n    ) %&gt;% \n    as.xts()\n)\n\n\n# create a portfolio specification with constraints and objectives\nportfolio = (\n    portfolio.spec(assets = colnames(us_stocks_xts)) %&gt;%\n    add.constraint(type = \"full_investment\") %&gt;%\n    add.constraint(type = \"long_only\") %&gt;%\n    add.objective(type = \"risk\", name = \"StdDev\") %&gt;%\n    add.objective(type = \"return\", name = \"mean\") %&gt;% \n    add.constraint(type = \"box\", min = 0, max = 0.3)\n)\n\n\n# fit the Mean-Var-Opt model\nportfolio_opt = optimize.portfolio(\n    R = us_stocks_xts,\n    portfolio = portfolio,\n    optimize_method = \"ROI\",\n    trace = TRUE\n)\n\n\n# show optimized weights\n(\n    tibble(\n        symbol = portfolio_opt %&gt;% extractWeights() %&gt;% names(),\n        weights = portfolio_opt %&gt;% extractWeights()\n    ) %&gt;% \n    filter(weights &gt; 0.05) %&gt;%\n    datasummary_df()\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                symbol\n                weights\n              \n        \n        \n        \n                \n                  AMZN\n                  0.11\n                \n                \n                  TSLA\n                  0.29\n                \n                \n                  NVDA\n                  0.30\n                \n                \n                  AVGO\n                  0.30\n                \n        \n      \n    \n\n\n\n有了上述参数后，代入计算 Efficient Frontier 和最大 Sharpe 点，即得到如下曲线。\n\n\nShow Code\n# calculate efficient frontier and max sharpe\nef_tidy = (\n    extractEfficientFrontier(portfolio_opt, n.portfolios = 50) %&gt;% \n    pluck(\"frontier\") %&gt;% \n    unclass() %&gt;% \n    as_tibble() %&gt;% \n    rename(return = mean, risk = ES) %&gt;% \n    mutate(sharpe = (return - 0.01) / risk)\n)\n\n\n# plot efficient frontier and max sharpe\n(\n    ggplot() +\n    geom_point(\n        data = us_stocks_aggr, \n        aes(x = stdv_returns, y = mean_returns), \n        color = \"gray\"\n    ) +\n    geom_text(\n        data = us_stocks_aggr, \n        aes(x = stdv_returns, y = mean_returns, label = symbol), \n        color = \"gray\"\n    )+ \n    geom_line(\n        data = ef_tidy, \n        aes(x = risk, y = return)\n    ) +\n    geom_point(\n        data = ef_tidy %&gt;% slice_max(sharpe),\n        aes(x = risk, y = return),\n        color = \"steelblue\", size = 4, shape = 16,\n    ) +\n    labs(\n        x = \"Risks\", y = \"Returns\", \n        title = \"Portfolio Optimization of Top US Stocks\",\n        caption = \"Risks: Stdv of Returns \\n Returns: Mean of Returns\"\n    ) +\n    theme_light()\n)\n\n\n\n\n\n\n\n\n\n\n\n\nto be continued…"
  },
  {
    "objectID": "posts/fina/fina.html#garch",
    "href": "posts/fina/fina.html#garch",
    "title": "Practical Finance with R",
    "section": "",
    "text": "上一节讨论的 ARIMA 模型处理的数据是 stationary 且跟踪样本的均值，这个模型比较适用于除了金融数据以外的宏观层面的具有一定周期性的数据；但是如果一个数据是金融类的、微观的、不具有显著周期性的，那是否有模型可以进行一定程度的预测？这就是我们本节进行讨论的 GARCH 模型，它对比 ARIMA 有非常重要的区别，首先 GARCH 模型处理的数据是不平稳的 non-stationary 即意味着它在某种程度上不符合正态分布，或者说不严格符合正态分布，其次它跟踪的对象是样本的方差，而不是均值，即意味着它更加看重样本的波动性，而不是平均值的高低大小。\nGARCH 计算理论很深奥且仅限于金融工程专业，简要过一遍模型即可，不对其进行深入讨论了；GARCH(p, q) 受限于两个组成部分：AR(p)和MA(q)。\n计算整个序列的方差：\n\\[\n\\text{Base} = Var(y_t) = \\alpha_0\n\\]\n计算过去某些期数的AR数值：\n\\[\nAR(p) = \\sum_{i = 1}^p \\alpha_i e_{t - 1}^2\n\\]\n其中 \\(\\alpha_1\\) 是过去方差对当前方差的短期冲击效应，理论上，这个数值越小越好。\n计算往期数据的 MA 数值：\n\\[\nMA(q) = \\sum_{j = 1}^q \\beta_j \\sigma_{t - j}^2\n\\]\n其中 \\(\\beta_1\\) 是过去方差对未来方差的长期冲击效应，理论上，这是个数值是越大越好。\n将所有部分相加，即可得到GARCH模型：\n\\[\n\\begin{align}\n\\text{Dif(d)} &= \\text{Base} + AR(p) + MA(q) \\\\\n\\sigma_t^2\n&=\n    \\alpha_0 +\n    \\sum_{i=1}^{q} \\beta_i \\epsilon_{t-i}^2 +\n    \\sum_{j=1}^{p} \\gamma_j \\sigma_{t-j}^2\n\\end{align}\n\\]\n在数学公式的基础上，结合前文已经整理的数据组，对 MSFT 单只股票进行 GARCH 建模。\n\n\nShow Code\n# convert tibble to xts form\nus_stocks_xts = (\n    us_stocks %&gt;% \n    pivot_wider(names_from = \"symbol\", values_from = \"returns\") %&gt;% \n    as.xts()\n)\n\n\n# create GARCH model\ngarch_msft = garch_modelspec(\n    y = us_stocks_xts$MSFT, \n    model = \"garch\", \n    constant = TRUE, \n    init = \"unconditional\", \n    distribution = \"jsu\"\n)\n\n\n# show coefficients\ngarch_msft_coef = garch_msft %&gt;% estimate()\n\ngarch_msft_coef %&gt;% summary() %&gt;% as_flextable()\n\n\nEstimateStd. Errort valuePr(&gt;|t|) \nμ\\muμ0.02440.00534.62180.0000*** \nω\\omegaω0.00040.00031.36000.1738 \nα1\\alpha_1α1​0.11260.09471.18880.2345 \nβ1\\beta_1β1​0.76970.12965.93900.0000*** \nζ\\zetaζ3.343517.10840.19540.8451 \nν\\nuν10.000021.20030.47170.6371 \nPPP0.88230.083710.54440.0000***Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1variance targeting: FALSEinitialization value: 0.003982LogLik: 163.5736AIC: -313.1 | BIC: -293.6Model Equation \nεt∼JSU(0,σt,ζ,ν)\\varepsilon_t \\sim JSU\\left(0,\\sigma_t,\\zeta, \\nu\\right)εt​∼JSU(0,σt​,ζ,ν) \nσt2=ω+α1εt−12+β1σt−12\\sigma^2_t = \\omega + \\alpha_1\\varepsilon^2_{t-1} + \\beta_1\\sigma^2_{t-1}σt2​=ω+α1​εt−12​+β1​σt−12​ Persistence (P) and Unconditional Variance Equations \nP=∑j=1qαj+∑j=1pβjP = \\sum_{j=1}^q \\alpha_j + \\sum_{j=1}^p \\beta_jP=∑j=1q​αj​+∑j=1p​βj​ \nE[εt2]=ω1−PE\\left[\\varepsilon^2_t\\right] = \\frac{\\omega}{1 - P}E[εt2​]=1−Pω​ \n\n\nShow Code\ngarch_msft_coef %&gt;% plot()\n\n\n\n\n\n\n\n\n\n\n\n\n继续…\n\n\nShow Code\n# predict 10 periods by 100 simulations\ngarch_msft_pred = predict(garch_msft_coef, h = 10, nsim = 100)\n\n\n# plot by default method, not beautiful, just for quick check\n# garch_msft_pred %&gt;% plot()\n\n\n# plot by ggplot2\n## extract CI to tibble format\ngarch_msft_pred_tidy = (\n    garch_msft_pred %&gt;% \n    pluck(\"distribution\") %&gt;% \n    tsconvert() %&gt;% \n    as_tibble() %&gt;% \n    mutate(\n        date = ymd(index), \n        pred_value = as.numeric(value)\n    ) %&gt;% \n    select(date, pred_value) %&gt;% \n    group_by(date) %&gt;% \n    summarize(\n        pred_mean = mean(pred_value),\n        pred_lo = quantile(pred_value, 0.1),\n        pred_hi = quantile(pred_value, 0.9)\n    )\n)\n\n\n## plot prediction on conditional variance\n(\n    ggplot() + \n    geom_point(\n        data = us_stocks %&gt;% filter(symbol == \"MSFT\"), \n        aes(x = ymd(date), y = returns),\n        color = \"gray\",\n    ) +\n    geom_line(\n        data = garch_msft_pred_tidy,\n        aes(x = date, y = pred_mean), \n        color = \"orange\", linewidth = 2\n    ) + \n    geom_ribbon(\n        data = garch_msft_pred_tidy,\n        aes(x = date, ymin = pred_lo, ymax = pred_hi), \n        fill = \"gray\", alpha = 0.3\n    ) +\n    labs(x = \"\", y = \"\", title = \"GARCH Prediction of MSFT\") +\n    theme_light() +\n    scale_x_date(date_labels = \"%Y-%m\")\n)"
  },
  {
    "objectID": "posts/econ/econ.html",
    "href": "posts/econ/econ.html",
    "title": "Practical Econometrics with R",
    "section": "",
    "text": "首先调取如下的工具包，暂时不做过多解释，每个都有其独特的功能，后续章节会逐步用到它们。\n\n\nShow Code\n# install and load pacman\nif (!requireNamespace(\"pacman\", quietly = TRUE)) {\n    install.packages(\"pacman\")\n}\n\n\n# load all common packages\npacman::p_load(\n    palmerpenguins, tidyverse, modelsummary, flextable, ggdag, dagitty, \n    ggsci, ivreg, marginaleffects, scales, pandoc, car, broom, nortest, \n    lmtest, patchwork, sandwich, GGally, fixest, ggfixest, fpp3, ggforce, \n    urca, readxl, writexl, tidyquant, PortfolioAnalytics, tsgarch, equatags, \n    ROI.plugin.quadprog, ROI.plugin.glpk, here\n)\n\n\n我们直接讨论基础的多元回归，导入 tidyverse 调用 penguins 数据组并且进行标准化，随机选择 10 行数据进行观测，我们不在乎这个数据具体数值内容，而是从列的角度看待数据。列 columns 也可以被称为变量 variables，而行 rows 也可以被称为观测 observations；变量一般包括数值型 numeric (1,2,3)，分类型 catgorical (CHN, USA, UK, JPN)，时间型 temporal (2025-1-4, 12:31:44), 字符型 string (Jordan, Kobe, James) 等基本分类，不同的编程语言都有大同小异的处理方法，暂不深入展开。本案数据组 pegs 有 7 列，包括 3 个分类变量和 4 个数值变量，结构清晰简单，很适合初学。\n\n\nShow Code\n# clean data\npegs = (\n    penguins %&gt;% \n    transmute(\n        species,\n        island,\n        sex,\n        bill_length = as.numeric(scale(bill_length_mm)),\n        bill_depth = as.numeric(scale(bill_depth_mm)),\n        flipper_length = as.numeric(scale(flipper_length_mm)),\n        body_mass = as.numeric(scale(body_mass_g)),\n    ) %&gt;%\n    drop_na()\n)\n\n\n# show sample\n(\n    pegs %&gt;% \n    slice_sample(n = 10) %&gt;% \n    datasummary_df(title = \"Sample of Penguins\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Sample of Penguins\n              \n                species\n                island\n                sex\n                bill_length\n                bill_depth\n                flipper_length\n                body_mass\n              \n        \n        \n        \n                \n                  Gentoo\n                  Biscoe\n                  male\n                  1.19\n                  -0.73\n                  1.50\n                  1.93\n                \n                \n                  Adelie\n                  Torgersen\n                  male\n                  0.03\n                  0.43\n                  0.65\n                  -0.25\n                \n                \n                  Chinstrap\n                  Dream\n                  female\n                  -0.08\n                  0.48\n                  0.08\n                  -1.00\n                \n                \n                  Gentoo\n                  Biscoe\n                  male\n                  0.40\n                  -1.04\n                  1.00\n                  1.12\n                \n                \n                  Adelie\n                  Dream\n                  male\n                  -1.12\n                  0.48\n                  -0.56\n                  -0.56\n                \n                \n                  Chinstrap\n                  Dream\n                  male\n                  0.97\n                  0.53\n                  -0.42\n                  0.25\n                \n                \n                  Chinstrap\n                  Dream\n                  male\n                  1.61\n                  1.34\n                  -0.28\n                  -0.59\n                \n                \n                  Chinstrap\n                  Dream\n                  female\n                  -0.55\n                  -0.28\n                  -0.99\n                  -1.25\n                \n                \n                  Gentoo\n                  Biscoe\n                  male\n                  0.23\n                  -0.38\n                  1.57\n                  2.18\n                \n                \n                  Gentoo\n                  Biscoe\n                  male\n                  0.09\n                  0.08\n                  1.29\n                  1.31\n                \n        \n      \n    \n\n\n\n我们通过 ggplot 绘图，对数据的分布进行初步观察，通过 boxchart 观察一个数值变量在多个分类变量当中的分布情况，通过 density plot 观察数值变量在多个分类变量当中的密度分布情况，通过 scatter plot 观察两个数值变量在多个分类变量之间的相关性。当然还有很多其他类型的图表，但这三种是学术论文当中最常见的类型，我们暂时先掌握这几个核心图表，后续在其他课程中再讨论其他类型的图表。下图提醒以下特征：\n\nflipper_length 在三个 species 当中分布基本均匀，且 Gentoo 的 flipper_length 明显高于另外两个 species；Adelie 和 Chinstrap 之间虽然有差异，但是差异较小。\nbody_mass 在三个 species 当中分布也基本均匀，且 Gentoo 的 body_mass 明显高于另外两个 species；Adelie 和 Chinstrap 之间虽然有差异，但是差异较小。\nbody_mass 很明显受到 flipper_length 的影响，即随着 flipper_length 的增加，body_mass 也增加，并且该特征在多个 species 当中保持基本稳定；或者说 flipper_length 和 body_mass 之间具有较强的正相关性。\n\n\n\nShow Code\n# plot box figure\nsub1 = (\n    pegs %&gt;% \n    ggplot(aes(x = species, y = flipper_length, fill = species)) +\n    geom_boxplot() +\n    scale_fill_iterm() +\n    labs(x = \"Species\", y = \"Flipper Length\", title = \"Boxchart\") +\n    theme_light()\n)\n\n\n# plot KDE figure\nsub2 = (\n    pegs %&gt;% \n    ggplot(aes(x = body_mass, color = species)) +\n    geom_density() +\n    scale_color_iterm() +\n    labs(x = \"Body Mass\", y = \"Density\", title = \"KDE\") +\n    theme_light()\n)\n\n\n# plot scatter figure\nsub3 = (\n    pegs %&gt;% \n    ggplot(aes(x = bill_length, y = body_mass, color = species)) +\n    geom_point() +\n    scale_color_iterm() +\n    labs(x = \"Bill Length\", y = \"Body Mass\", title = \"Scatter\") +\n    theme_light() +\n    facet_wrap(~ species)\n)\n\n\n# combine into single figure\n(\n    wrap_plots(sub1, sub2, sub3) + \n    plot_layout(ncol = 1, nrow = 3) + \n    plot_annotation(title = \"Fig 2.1: Sample of Penguins\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n一个标准的多元回归模型可以用矩阵表示为：\n\\[\n\\mathbf{Y} = \\mathbf{X} \\mathbf{B} + \\mathbf{e}\n\\]\n其中\n\n\\(\\mathbf{Y}\\) 是因变量，即输出的结果。\n\\(\\mathbf{X}\\) 是自变量，即输入的原因。\n\\(\\mathbf{B}\\) 是回归系数，即自变量对于因变量的影响程度大小。\n\\(\\mathbf{e}\\) 是残差项，即预测模型与真实模型的差异程度。\n\n我们不深入展开公式的历史背景，继续往下推，它等价的广义算法模型就是 ：\nlm(formula = Y ~ X, data = df)\n回到本案，我们建立一个简单的多元回归模型，即通过 flipper_length 和 species 两个自变量来预测企鹅的 body_mass。此时，传统教科书会进行一大堆抽象的数学公式计算，但我觉得数学证明对经管同学并不是很有必要。牢记“我们是乘坐飞机的乘客而不是驾驶飞机的飞行员”这个根本原则，抓住重点、纲举目张、不要陷入数学迷宫无法自拔；因此我们不会手搓数学而是直接调用 R 进行计算，如果得到 \\(p &lt; 0.05\\)，那么就是显著影响；反之那么就没有显著影响；对理论背景有兴趣的同学可以参考 (Angrist2009?), (Stock2018?), (Wooldridge2019?), (Gujarati2020?)。\npegs_mod1 = lm(\n    formula = body_mass ~ flipper_length + species, \n    data = pegs\n)\n代码运算完成后显示，三个主要变量都显著 (\\(p &lt; 0.05\\))，这意味着模型总体上具有很好的结果，即 flipper_lengh 的变化会导致 body_mass 的变化；species 是分类变量而不是数值变量，因此应该描述为“企鹅从 A 种类变为 B 种类后，体重也会随之变化 X 个单位”；至于它们具体的回归参数数值大小和 R2, AIC, BIC, F, RMSE 等参数，我们暂不解释。这个显著结果对于普通本科同学的论文就基本足够了，但是对于硕士同学是不足够的，我们还需要进一步研究一些深度的问题。\n\n\nShow Code\n# fit models\npegs_mod1 = lm(body_mass ~ flipper_length + species, data = pegs)\n\n\n# show model summary\nmodelsummary(\n    models = pegs_mod1, \n    stars = TRUE, \n    statistic = NULL,\n    coef_omit = \"Intercept\",\n    title = \"Summary of pegs_mod1\"\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Summary of pegs_mod1\n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  flipper_length\n                  0.712***\n                \n                \n                  speciesChinstrap\n                  -0.256***\n                \n                \n                  speciesGentoo\n                  0.355**\n                \n                \n                  Num.Obs.\n                  333\n                \n                \n                  R2\n                  0.787\n                \n                \n                  R2 Adj.\n                  0.785\n                \n                \n                  AIC\n                  441.7\n                \n                \n                  BIC\n                  460.7\n                \n                \n                  Log.Lik.\n                  -215.845\n                \n                \n                  F\n                  405.281\n                \n                \n                  RMSE\n                  0.46\n                \n        \n      \n    \n\n\n\nShow Code\n# plot model coefficients\n(\n    pegs_mod1 %&gt;% \n    modelplot(coef_omit = \"Intercept\") + \n    labs(\n        x = \"Estimate and 95% CI\", \n        title = \"Fig 2.2: Summary of pegs_mod1\"\n    ) +\n    theme_light()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n相关性 Correlation 是计量经济学的起点，但仅有相关性是不够严谨的，更重要的是因果性 Causality。从法学的角度来讨论，张明楷老师在《刑法学》提出：“因果关系是引起与被引起的关系，前者是后者的原因，后者是前者的结果。”因果关系的理论存在比较复杂的讨论，张明楷老师偏向于认同主流的“条件说”：如果没有 X，就必然不会有 Y，那么 X 是 Y 的原因，Y 是 X 的结果。这个原理很容易理解，假如张三用刀把李四杀死，那么“张三用刀捅杀”是“李四死亡”的必要原因，“李四死亡”是“张三用刀捅杀”的危害结果；反之，如果没有“张三用刀”，那么就不会有“李四死亡”的危害结果，这就是典型的因果关系(Zhang2024?)。但是刑事案件往往不会这么简单，而是多个原因共同地、动态地、复杂地发生作用，最终导致了严重法益侵害结果。我们请出“法外狂徒张三”带来以下几个小案例：\n\n张三欲杀害李四，尾随李四下夜班，到路口用水果刀朝着李四胸口捅了几刀，李四倒地；张三认为李四已经死了，便逃离现场，但李四只是装死而没有真的死亡，10 分钟后，李四爬起来正准备打 110 和 120 报警急救，却被酒后驾驶汽车的王五撞倒；李四多处骨折受重伤，王五逃逸，几小时后路人发现血泊中的李四，送医院抢救，但不治身亡。\n张三欲杀害李四，在一起打牌时，张三乘机把李四喝的饮用水换成无色无味的剧毒鹤顶红，所有人都不知情；王五突然说口渴想喝水，李四就自然而然地把自己的水给王五喝，张三见状大喊：“哎别喝那个，我这有更好的”，王五不以为然：“没事，我跟李四发小儿，来打牌打牌”；王五一饮而尽，半小时后毒药起效，心脏骤停，王五当场毙命。\n张三欲杀害李四，将李四汽车的刹车线剪断，希望李四在高速公路发生交通事故而身亡；李四在高速行驶中，因为刹车失灵，撞上另外的汽车，导致汽车翻个底朝天，司机王五被困在车中无法解救，汽车起火迅速燃烧，李四尝试拯救也无能为力；司机王五被烧死在车里，而李四的车辆虽然受到严重损毁，但他本人并未受到严重的人身伤害。\n\n这一系列都是法哲学对于“原因——结果”的深入思考，它们原理对于后期掌握复杂的因果推断和各种假设大有裨益，但有一个主要区别。刑法所讨论的“因果关系”是定性的而非定量的，而经济学讨论的“因果关系”是统计意义上的定量的而非定性的。我们不问：“如果没有 X，是否必然不会发生 Y 的结果”，而是问：“在保持其他因素不变的情况下，X 的变化对 Y 的平均影响有多大？”。换言之，在刑法上，张三朝李四胸口捅了一刀，就构成故意杀人罪，假如张三捅十刀，依然只能构成一个故意杀人罪，而不会构成十个故意杀人罪；在经济学上，张三捅一刀导致李四死亡的概率必然远小于捅十刀致死的概率。一个法官需要确认的是：“张三是否真的朝着李四捅了这一刀？”而一个经济学家需要确认的是：“张三捅的第几刀是导致李四死亡的最关键伤害？”\n有点复杂了，先刹车，回到经济学当中。我们通过 R 当中强大的包 ggdag，很方便地绘制 DAG 图，以非常明确的方式对“因果推断 Causal Inference” 进行抽象维度的理解。具体而言，本案例有两个自变量，即原因 flipper_length, specie，可以认为分别是 \\((X1, X2)\\)；同时本案有一个因变量，即结果 body_mass，可以认为是 \\(Y\\)。那么，假设本案例当中没有任何其他的干扰因素，即“有且仅有 2 个原因，它们导致了 1 个结果”，那么本案的因果关系便是 \\((X1, X2) \\to Y\\)，其 DAG 图如下。需要注意的是，这个假设是很理想注意的假设，实际情况下一定存在未知的干扰因素，而且这样的干扰因素往往会造成严重的结果偏差，具体我们会在后续讨论。\n\n\nShow Code\n# create a DAG\ndag1 = dagitty(\"dag {\n    X1 -&gt; Y\n    X2 -&gt; Y\n    }\n\")\n\n\n# set coordinates\ncoordinates(dag1) = list(\n    x = c(X1 = -1, X2 = -1, Y = 0),\n    y = c(X1 = 1,  X2 = -1, Y = 0)\n)\n\n\n# plot the DAG\n(\n    ggdag(dag1, node = FALSE) +\n    geom_dag_text(color = \"black\", size = 5) +\n    geom_dag_edges(edge_colour = \"gray\") +\n    labs(x = \"\", y = \"\", title = \"Fig 2.3: DAG of Simple LM\") +\n    theme_dag_grid() +\n    coord_fixed(clip = \"off\")\n)\n\n\n\n\n\n\n\n\n\n根据这个 DAG 图，有以下几个问题需要我们深入思考：\n\n回归系数 Coefficients：flipper_length 对于 body_mass 的促进或者抑制作用具体是多少？即，假如 flipper_length 增加或减少一个单位，body_mass 会增加或减少多少？\n内生性 Endogeneity：flipper_length 对于 body_mass 的促进或者抑制作用是否还有其他的影响因素？即，我们当前假设是 flipper_length 导致 body_mass，但是否有可能是 unknown_var 同时影响了 flipper_length 和 body_mass？或者 flipper_length 同时影响了 unknown_var 和 body_mass？我们的模型是否可能遗漏了一些重要变量，或者把一个并没有本质关系的变量牵强地让它成为了一个相关变量？\n异质性 Heterogeneity：在不同的 species 之间，上述的相关性是否会发生变化？即，假如 Adelie 的促进作用是 1.67，那么 Chinstrap 和 Gentoo 的促进作用分别是多少？它们的促进作用是大于 1.67 还是小于 1.67 ？一个适用于所有群体的平均结论，是否掩盖了某些特定群体的重要特征？吃大锅饭的一群人当中，总有一些雷锋同志，也总有一些南郭先生，我们如何将其分辨出来？\n稳健性 Robustness：模型完成后，通过什么方法检测模型本身的正确性和严谨性？即，模型是否满足 Gauss-Markov 所提出的经典假设 Best Linear Unbiased Estimator (BLUE)? 我们得到的这个显著结果是否可能只是巧合，而不是严格的因果关系？假如我们经过艰难险阻，只能得到一个边缘显著的模型 \\(0.050 &lt; p &lt; 0.060\\)，那这个结果是应该接受还是拒绝？\n\n以上几个问题分别对应了应用计量经济学的几个不同的层次，它们以铰链的形式共同构成了一个完整的“因果推断”分析流程，将一个简单线性回归的结论逐渐加深加强，将证据效力从“它小概率是这样”推到“它大概率是这样”最后推到“它很大概率是这样”的维度。因果推断是计量经济学的高级内容，它原本不应该出现的这么早，我故意将其在此处先埋个线索，以便更方便、更快捷、更高效地衔接上下文；但是因果关系的数学部分非常复杂，因此我们不展开理论部分，而是直接介绍原理然后跳入实战，以战养兵、战练结合。对理论部分很感兴趣的博士同学可以参考该领域的权威且前沿的文献 (Imbens2015?)，非博士同学普遍没必要深入阅读因果推断的文献。\n\n\n\n\n\n\n我们已经完成了一个古典回归模型，即结果 body_mass 完全是由原因 flipper_length, species 所导致；换言之，body_mass 没有受到其他因素如 flipper_depth, island, sex 等的影响。但这个模型是不严谨的，它基于一个过于理想主义的假设，即，本模型的因果链条当中没有任何其他外界干扰因素；用学术词语表示为，本模型所有的自变量都是“外生的”，本模型中没有“内生的”自变量。此时我们需要对“外生性 Exogeneity”和“内生性 Endogeneity”进行简要解释。\n\n外生性 Exgeneity: 自变量跟误差项几乎没有相关性 \\(\\text{Cov}(X, \\epsilon) = 0\\)，结果是由该原因所导致。\n内生性 Endogeneity: 自变量跟误差项有一定相关性 \\(\\text{Cov}(X, \\epsilon) \\neq 0\\)，结果并不是完全由该原因所致，还有其他原因共同影响。\n\n“外生性假设”大幅度地简化了数学运算和算法逻辑，但它忽略了一个不可能避免的现实问题，即模型的自变量至少有一些是内生的；在真实的世界里，很少存在某个事件是绝对外生性的，大部分事件都是由多个复杂因素共同作用而成。经济学家往往处于简化角度，将复杂事件想当然地认为是 X -&gt; Y，但是这样的归纳方式很容易得出一个“高度显著但是一派胡言”的结论。这就是内生性所导致的典型偏误，它是所有计量经济学模型所需要面临的第一个严峻挑战，许多经典文献进行了深度讨论，如 (Theil1958?), (Sargan1958?), (Tinbergen1959?), (Hausman1978?), (Imbens1994?), (Staiger1997?)。\n既然明白了内生性的弊端，接下来就必须讨论如何解决它，一个比较符合硕博阶段的方法是通过“工具变量 Instrument Variables, Z”来处理。然而要构建一组好的工具变量并不容易，因为它的因果关系链条受到严格限制，它只能是 Z -&gt; X -&gt; Y，而不能是任何其他的路径；即，它必须满足以下两个必要条件。\n\n相关性 Relevance: 工具变量必须与内生变量显著相关，即 \\(\\text{Cov}(Z, X) \\neq 0\\)。\n排他性 Exclusion: 工具变量不能与因变量或误差项有相关性，即 \\(\\text{Cov}(Z, Y) = \\text{Cov}(Z, \\epsilon) = 0\\)。\n\n我们现在通过 DAG 图解释最基础的 2SLS 原理，下图中的 X -&gt; Y 衍生出了两个新的变量 U, Z：\n\nU 代表未知的协同变量 (Unknown Cofounders)，它们必须同时影响 X 和 Y，但是它永远无法具体计算，因此我们无法知道它对于 X 和 Y 的影响的具体数值是多少，但我们也并不关心这个；同时需要注意 U 不能影响 Z，否则工具变量就失效了。\nZ 代表工具变量 (Instrument Variables)，它们只影响 X 再传递到 Y；特别注意 Z 不能直接影响 Y 或 U，这是工具变量成立的必要条件。另外，工具变量的数量必须大于等于内生变量的数量，否则算法会报错无法执行。\n\n\n\nShow Code\n# create a DAG for 2SLS\ndag3 = dagitty(\"dag{\n    Z -&gt; X -&gt; Y;\n\n    U -&gt; X;\n    U -&gt; Y;\n    }\n\")\n\n\n# set coordinates for variables\ncoordinates(dag3) = list(\n    x = c(Y = 0, X = -1, Z = -2, U = -1),\n    y = c(Y = 0, X = 0,  Z = 0,  U = 1)\n)\n\n\n# plot DAG of 2SLS\n(\n    ggdag(dag3, node = FALSE) +\n    geom_dag_text(color = \"black\", size = 5) +\n    geom_dag_edges(edge_color = \"gray\") +\n    labs(x = \"\", y = \"\", title = \"Fig 2.4 DAG of 2SLS\") +\n    theme_dag_grid() +\n    coord_fixed(clip = \"off\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n我们在前面简要讨论了如何从原理上解决内生性问题，现在介绍如何运用具体的算法完整地实现一个工具变量回归模型，也称 IV-LM 模型，我们需要借用 ivreg() 包以及内部的 Two-Stage Least Square, 2SLS 原理，其细节比较复杂，继续战略性跳过，有兴趣的同学请参考官网。简而言之，2SLS 包括两个阶段，第一阶段是 Purification，第二阶段是 Estimation，他们共同作用完成对内生变量的剥离和对模型的修正。\n第一阶段是 Purification，因为我们模型中的内生变量 X 是“污染的、不纯粹的、有杂质的”变量，因此需要一个工具变量 Z，它起到过滤器的作用，将内生变量的“污染物、杂质、杂音”过滤出去，只保留当中“纯洁、干净、有效的”内容。因此，这个工具变量的“相关性”就非常重要，当且仅当工具变量对内生自变量是强显著和关联的情况下，这个工具变量才具有“过滤污染物”的能力，否则这个工具变量就没有任何意义；但是我们也必须保证工具变量跟干净的结果之间没有相关性，否则就会由于工具变量而干扰了结果，那样我们就无法判断结果到底是由于内生变量所致还是工具变量所致。即，工具变量生效的必要条件是：\n\\[\n\\begin{aligned}\n\\text{Cov}(Z, X) & \\neq 0 \\\\\n\\text{Cov}(Z, Y) & = \\text{Cov}(Z, \\epsilon) = 0\n\\end{aligned}\n\\]\n此时可以带入第一阶段，得到公式：\nfitsr_stage = lm(formula = X ~ Z, data = df)\n\nX_hat = predict(lm(formula = X ~ Z, data = df))\n第二阶段 Estimation，我们需要计算自变量的预测值 X_hat 对于因变量 Y 的显著性，意义在于，当内生变量的“污染物”被剥离出去以后，模型对一个“干净”的因变量能否依然保持强相关性、因果性、解释性？如果此时结果依然显著，那就皆大欢喜、打卡下班；但如果结果不显著，那我们必须继续调整工具变量，然后多次尝试直到搭配出一套满意的组合。总体来说，第二阶段比第一阶段更简单、更直观，但是它的结果必须建立在第一阶段的显著结果之上；如果第一阶段不显著，那么即使第二阶段显著，也没有说服力；但是如果第一阶段显著，而第二阶段不显著，也具有一定说服力，只是这样的情况容易被审稿人盯上；当然最美好的结果是第一阶段和第二阶段都显著。\nsecond_stage = lm(formula = Y ~ X_hat, data = df)\n虽然这两个 lm() 原理上是正确的，但我们并不能手动分两步计算 2SLS，因为这样做会得到错误的置信区间和 p 值；跳过复杂的数学原理，最科学的方案是直接使用 ivreg() 计算，它会在内部完成这两个步骤并自动修正第二阶段的标准误偏差，其表达如下。\niv_mod = ivreg(formula = Y ~ X | Z, data = df)\n这个函数返回一个复杂的合并结果，但是并未提供独立的结果，因此我们必须曲线救国，编写两个简单的调用函数分别提取不同阶段的回归结果，再整理成干净表格。虽然有点绕，但是由于 tidyverse 奠定了强大的底层处理流程，因此整段代码的思路很清晰。其中第一个阶段结果包括以下几项：\n\nWeak Instrument Test：检验工具变量是否显著相关，若显著则说明工具变量有效，反之则无效。\nWu-Hausman Test：检验模型是否存在内生性，若显著则说明存在内生性，需要使用工具变量进行控制，反之则不需要。\nSargan Test：检验工具变量是否满足外生性条件，若显著则说明工具变量是无效的，反之则说明工具变量是有效的。\n\n略过证明细节，我们希望 WI 显著、WH 显著、Sargan 不显著，即可证明原模型是有内生性问题的、工具变量才是有效的、模型才是严谨的。第二阶段追求的结果与普通的线性模型结果类似，主要关注回归系数、显著性、标准误即可。至此，我们已经从概念上完成了一个 IV 模型的主要流程，本小节的内容不可避免的比较抽象，同学们如果感觉不太清楚也无所谓，我们继续用案例加深实际理解；跑完案例再回过头复习此处的抽象内容，相信大家会更加清楚。\n\n\n\n我们现在进行案例分析，回到最基础的 pegs_mod1 模型是：\npegs_mod1 = lm(\n    formula = body_mass ~ flipper_length + species,\n    data = pegs\n)\n其中自变量是 flipper_length 代表企鹅鳍长，species 代表企鹅种类，结果变量是 body_mass 代表企鹅重量；那么根据我们上一节讨论的内容，假设所有的自变量都是内生变量，即 X = flipper_length, species，再假设工具变量是 Z = bill_depth, island, sex，分别代表企鹅的嘴长度、生活岛屿、性别。即可以得到下面的 DAG 图。特别注意，此处的工具变量严格意义上只能满足第一个条件：相关性，而无法满足第二个必要条件：排斥性。我们此处采纳它们仅仅是出于简化的教学角度，在实战分析中，一定要先检验工具变量的两个缺一不可的必要条件；不能想当然地抓一些变量就拿来充当工具变量，那样非常容易得到 False Positive 的结论。扯远了，回本案例中，我们的 DAG 如下图所示：\n\n\nShow Code\n# create DAG\ndag4 = dagitty(\"dag {\n    Z -&gt; X -&gt; Y\n    U -&gt; X\n    U -&gt; Y\n    }\n\")\n\n\n# set coordinates\ncoordinates(dag4) = list(\n    x = c(Y = 2, X = 1, Z = 0, U = 1),\n    y = c(Y = 0, X = 0, Z = 0, U = 1)\n)\n\n\n# set labels\ndag4_label = data.frame(\n    name = c(\"Y\", \"X\", \"Z\", \"U\"),\n    label = c(\n        \"body_mass\", \n        \"flipper_length \\n species\", \n        \"bill_length \\n island \\n sex\", \n        \"U\"\n    )\n)\n\n\n# convert to ggdag type\ndag4 = dag4 %&gt;% tidy_dagitty() %&gt;% left_join(dag4_label, by = \"name\")\n\n\n# plot DAG\n(\n    ggdag(dag4, node = FALSE, text = FALSE) +\n    geom_dag_edges(edge_colour = \"gray\") +\n    geom_dag_text(aes(label = label), color = \"black\", size = 4) +\n    labs(title = \"Fig 2.5: DAG of pegs_mod2\") +\n    theme_dag_grid() +\n    coord_fixed(clip = \"off\")\n)\n\n\n\n\n\n\n\n\n\n再根据 DAG 图和 ivreg() 原理，我们构建如下模型；结果显示，第一阶段的 WI 显著（好结果），WH 显著（好结果），Sargan 不显著（好结果）；因此说明我们选择的工具变量是合适的。第二阶段的自变量对于结果具有更强的促进作用，且显著性更高，这说明我们选择的工具变量有效地剥离了内生变量的污染，从而增强了模型的解释力和预测力。因此我们可以认为，原 OLS-LM 具有较强内生性，需要增加工具变量进行调节；新增的工具变量具有较强效果，经工具变量调节后的 IV-LM 具备更高显著性和稳健性。至此，我们系统地完成了一个 2SLS 计算流程。\nivreg(formula = Y ~ X | Z, data = df)\n\n\nivreg(\n    formula = body_mass ~ species + flipper_length | bill_depth + island + sex, \n    data = pegs\n)\n\n\nShow Code\n# create IV model\npegs_mod2 = ivreg(\n    formula = body_mass ~ species + flipper_length  | bill_depth + island + sex,\n    data = pegs\n)\n\n\n# create UDF to get first stage IV test results\nget_first_stage_test = function(model) {\n    model %&gt;%\n    summary(diagnostics = TRUE) %&gt;%\n    pluck(\"diagnostics\") %&gt;%\n    as.data.frame() %&gt;%\n    rownames_to_column(var = \"terms\") %&gt;%\n    as_tibble() %&gt;%\n    rename(pv = `p-value`) %&gt;%\n    select(terms, statistic, pv) %&gt;%\n    datasummary_df(title = \"First Stage Test\")\n}\n\n\n# create UDF to get second stage IV test results\nget_second_stage_test = function(model) {\n    model %&gt;%\n    summary() %&gt;%\n    pluck(\"coefficients\") %&gt;%\n    as.data.frame() %&gt;%\n    rownames_to_column(var = \"terms\") %&gt;%\n    as_tibble() %&gt;%\n    rename(\n        estimate = Estimate, SE = `Std. Error`, pv = `Pr(&gt;|t|)`\n    ) %&gt;%\n    select(terms, estimate, SE, pv) %&gt;%\n    datasummary_df(title = \"Second Stage Test\")\n}\n\n\n# show test results\nget_first_stage_test(pegs_mod2)\n\n\n\n\n    \n\n    \n    \n      \n        \n        First Stage Test\n              \n                terms\n                statistic\n                pv\n              \n        \n        \n        \n                \n                  Weak instruments (speciesChinstrap)\n                  64.22\n                  0.00\n                \n                \n                  Weak instruments (speciesGentoo)\n                  430.08\n                  0.00\n                \n                \n                  Weak instruments (flipper_length)\n                  129.62\n                  0.00\n                \n                \n                  Wu-Hausman\n                  68.93\n                  0.00\n                \n                \n                  Sargan\n                  2.32\n                  0.13\n                \n        \n      \n    \n\n\n\nShow Code\nget_second_stage_test(pegs_mod2)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Second Stage Test\n              \n                terms\n                estimate\n                SE\n                pv\n              \n        \n        \n        \n                \n                  (Intercept)\n                  0.61\n                  0.13\n                  0.00\n                \n                \n                  speciesChinstrap\n                  -0.49\n                  0.18\n                  0.01\n                \n                \n                  speciesGentoo\n                  -1.42\n                  0.29\n                  0.00\n                \n                \n                  flipper_length\n                  1.70\n                  0.15\n                  0.00\n                \n        \n      \n    \n\n\n\n我们现在有标准线性模型 OLS-LM 和工具变量模型 IV-LM，分别用数值方法和绘图方法对比他们的差异，发现 IV 模型的回归系数有一些优化，但优化效果并不是特别明显；主要原因在于原有的线性模型 pegs_mod1 已经是显著的了，工具变量的最大意义是将一个不显著的结果“优化”成为显著的结果，而不是将一个已经显著的结果再次强化其显著性。好的工具变量就像优秀的公检法机关，它通过严肃论证和周密调查得到铁证如山，将犯罪嫌疑人定罪量刑；而坏的工具变量就像不负责任的公检法机关，它在程序上有诸多漏洞和瑕疵，疏忽大意甚至故意造成冤假错案，此时就算是已经在程序上将嫌疑人定罪量刑甚至执行了死刑，但是由于严重的程序违法，这些案件最终还是要被翻案。此图下发提供了两个中国刑事司法历史中两个真实的案例作为附录，引导同学们从法学、哲学、经济学、公平正义的底层视角对“因果关推断”有更加透彻的理解。\n\n\nShow Code\n# combine into list\npegs_mods = list(\n    \"pegs_mod1\" = pegs_mod1, \n    \"pegs_mod2\" = pegs_mod2\n)\n\n\n# show comparison table\nmodelsummary(\n    models = pegs_mods, \n    stars = TRUE,\n    statistic = NULL,\n    coef_omit = \"Intercept\",\n    title = \"Summary of pegs_mods\"\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Summary of pegs_mods\n              \n                 \n                pegs_mod1\n                pegs_mod2\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  flipper_length\n                  0.712***\n                  1.701***\n                \n                \n                  speciesChinstrap\n                  -0.256***\n                  -0.493**\n                \n                \n                  speciesGentoo\n                  0.355**\n                  -1.419***\n                \n                \n                  Num.Obs.\n                  333\n                  333\n                \n                \n                  R2\n                  0.787\n                  0.564\n                \n                \n                  R2 Adj.\n                  0.785\n                  0.560\n                \n                \n                  AIC\n                  441.7\n                  680.0\n                \n                \n                  BIC\n                  460.7\n                  699.1\n                \n                \n                  Log.Lik.\n                  -215.845\n                  \n                \n                \n                  F\n                  405.281\n                  \n                \n                \n                  RMSE\n                  0.46\n                  0.66\n                \n        \n      \n    \n\n\n\nShow Code\n# plot comparison figure\n(\n    pegs_mods %&gt;% \n    modelplot(coef_omit = \"Intercept\") + \n    labs(\n        x = \"Estimate and 95% CI\", \n        title = \"Fig 2.6: Summary of pegs_mods\"\n    ) +\n    theme_light()\n)\n\n\n\n\n\n\n\n\n\n附录案件：\n\n聂树斌案件：1994 年 8 月，石家庄某玉米地发现一具女尸；9 月，公安局认为聂树斌有重大作案嫌疑；1995 年 4 月，经过“侦查起诉”环节，石家庄中院审判认定聂树斌犯故意杀人罪，判处死刑立即执行。但是案件卷宗缺失了一些关键的定性证据，聂树斌的家人和辩护律师李树亭都认为他不是真凶，于是走上漫漫维权路。2005 年 5 月，另案嫌疑人王书金交代了 1994 年 8 月的石家庄玉米地女尸案件的全部事实，并称自己是案件的唯一真凶，没有其他人参与，案件引起最高法院重视，进入重审阶段。2016 年 12 月，经过极其复杂的司法环节，最高法院巡回法庭改判聂树斌无罪。此案件的王书金就是重要的工具变量，他从根本上改变了聂树斌案件的性质，假如王书金没有被抓、李树亭律师没有奔走呼号、郑成月警官没有坚持真理，那么聂树斌永远都会背上“杀人犯”的罪名。可惜从聂树斌被错误地执行死刑到被平反，已经过过去了整整 20 年，此案的真凶最终也无从查证。河北高院没有被认定王书金是此案的罪犯，且最高法院已经宣判聂树斌无罪，到底是谁杀害了这位可怜的女青年？公检法机关并没有回答这个问题。她和聂树斌都无辜地成为了我国法治改革滚滚洪流当中的一粒带血的尘埃，愿逝者安息。 CCTV 专访 锵锵三人行 访谈\n佘祥林案件：1994 年 1 月，佘祥林的其中张在玉走失；4 月，当地鱼塘里发现一具高度腐烂的女尸，公安局法医经过“检验”认为是走失的张在玉，公安局认为佘祥林具有重大犯罪嫌疑；9 月，经过简单粗暴的“调查审讯”，荆州中院判处佘祥林犯故意杀人罪，死刑缓期两年执行；1998 年 荆门中院顶住巨大社会道德法律的压力，改判佘祥林有期徒刑 15 年；2005 年 3 月，张在玉“死而复生”返回老家，证明当年鱼塘里的女尸并非张在玉本人，也证明佘祥林是完完全全的冤案；5 月，湖北高院改判佘祥林无罪。佘祥林的运气比聂树斌好太多了，佘祥林的三次审判都在逐级降刑而没有死刑立即执行；并且在他服刑的第 11 年，最关键的工具变量，即本案的“死者”张在玉突然“死而复生”，给当时的政法系统狠狠地上了一课。如果张在玉没有出现，佘祥林再服刑 4 年也就刑满释放出狱了，但余生都要背上“杀妻”的罪名。CCTV 专访 锵锵三人行 访谈\n\n\n\n\n\n\n\n根据前面的内容，我们的基础模型 OLS-LM 是：\npegs_mod1 = lm(\n    formula = body_mass ~ flipper_length + species, \n    data = pegs\n)\n已知模型具有显著性，即 flipper_length对 body_mass 具有积极的促进效果。现在需要进一步思考，在不同的 species 分组中（Adelie, Chinstrap, Gentoo），模型的回归显著性是否存在差异？换言之，我们需要回答一些问题：“模型的平均效应 Average Treatment Effects，ATE 在多个分组间是否保持稳定？是否存在某个分组效应很强，但是另外分组效应很弱的情况？是否存在某个分组实际上是弱效应，但是“被平均”而成为了强效应的情况？或者反之，是否存在某个分组实际上是强效应，但是“被平均”而成为了弱效应的情况？”这些奇奇怪怪的问题所探究的本质都是同一个问题，即条件平均效应 Conditional Average Treatment Effects, CATE 的变化情况。同学们不要看到这个名词就恐惧，CATE 非常简单，只不过是 ATE 在某个特定条件下的子集而已，我们现在来拆解它。\n本案中，我们的 ATE 就是 flipper_length 对 body_mass 的平均促进效应，又因为 species 包含三个分类 Adelie, Chinstrap, Gentoo，因此如果我们不考虑全局的平均情况，而考虑每个分类下的局部情况，我们就会得到三个分类各自的回归效应。这就是 flipper_length 对 body_mass 在 Adelie, Chinstrap, Gentoo 三个分组下的促进效应，再简化即可得到三个 CATE 分别是 CATE(Ade), CATE(Chi), CATE(Gen)。它们的简化的数学表达式如下，请注意这是非常抽象的表达式，其中的 ATE 是必须通过另外的模型计算而得到的，而不是直接可以从这个表达式中计算出来。\n\\[\n\\begin{aligned}\n\\text{CATE(Ade)} &= [\\text{ATE} | \\text{species} = \\text{Adelie}] \\\\\n\\text{CATE(Chi)} &= [\\text{ATE} | \\text{species} = \\text{Chinstrap}] \\\\\n\\text{CATE(Gen)} &= [\\text{ATE} | \\text{species} = \\text{Gentoo}]\n\\end{aligned}\n\\]\n此时，我们需要思考的一个重要问题就是 ATE 和 CATE 在多个分组之间的差异程度，即以下的不等式：\n\\[\n\\text{CATE(Ade)} \\quad \\stackrel{?}{=} \\quad\n\\text{CATE(Chi)} \\quad \\stackrel{?}{=} \\quad\n\\text{CATE(Gen)} \\quad \\stackrel{?}{=}\n\\text{ATE}\n\\]\n这就顺理成章地引出了“同质性 Homogeneity”与“异质性 Heterogeneity”两个重要概念，它们的数学表达式如下：\n\n同质性 Homogeneity：条件平均效应在多个组之间不存在显著差异，即，\\(\\text{CATE}_i = \\text{CATE}_j, \\quad \\forall i,j\\)。\n异质性 Heterogeneity：条件平均效应在多个组之间存在较大差异，此时又可以继续划分为以下两种情况：\n\nGroupwise: 某个条件回归效应与平均回归效应存在较大差异，即 \\(\\text{CATE}_i \\neq \\text{ATE}, \\quad \\exists i\\)。\nPairwise: 任意两个条件平均效应之间存在较大差异，即 \\(\\text{CATE}_i \\neq \\text{CATE}_j, \\quad \\exists i,j\\)。\n\n\n至此，我们就从原理上完成了对同质性和异质性的定义和区分，至于它的实际计算会在下一小节讨论，接下来需要思考的是为什么要区分它们以及分别适用于什么样的场景？“异质性”在经济研究领域具重要意义，比如国家出台“延迟退休政策”，“职工”有很多不同分类：“公务员、事业单位、军人、民营企业职员、自由职业者、残障人士、进城务工人群、乡村农民”等等，我们必须考虑“延迟退休”政策对于不同人群的平均效果是否存在显著差异？我们当然不希望中国的年轻人成为发达国家那种不想上班的躺平状态，但更不希望中年人成为日本那种不敢退休的社会恐惧。这就要求政府机关深入研究政策效力的“异质性”，即不同组别的劳动者在受到延迟退休的政策刺激时应该具有不一样的反馈，一方面，年轻人应该积极响应认可这份政策并努力工作；另一方面，适龄离休干部可能存在一定程度的抵触和不理解，这不仅是合情合理的，而且也是完全意料之中的。政策制定者必须充分考虑到组间差异，制定出“弹性退休政策”而不是一刀切的“强制退休政策”，否则容易得到一个“失之毫厘谬之千里”的结论。总之，同质性和异质性是实证分析的第二个重点问题，它们并没有道德上的好坏之分，我们必须根据具体情况开展具体分析，更多关于异质性的权威文献包括 (Rosen1974?), (Mundlak1978?), (Heckman1979?), (Anderson1992?), (Heckman1998?)。\n\n\n\n回到本案，我们现在讨论如何检验模型的异质性。其实很简单，就是通过“交互项 Interaction Terms”，在原模型当中增加一个自变量与分类变量的乘积项目。即把 num_var + cat_var 改为 num_var * cat_var 即可，然后再分析这个新模型的交互项的显著性；假如分类变量有很多子类和很多数值变量，这时的交互项目就会非常复杂，因此出于简便角度，我们并不需要完整写清楚所有的交互项目，重点在于修改符号即可。这部分数学公式比较复杂但是算法原理很简单，因此我们继续跳过数学证明部分，直接给出广义的算法原理：\nlm(formula = Y ~ X1 + X2, data = df)\n\n\nlm(formula = Y ~ X1 * X2, data = df)\n那么根据已有的 pegs_mod1, pegs_mod2 可以推理得到 pegs_mod3, pegs_mod4：\npegs_mod3 = lm(\n    formula = body_mass ~ flipper_length * species, \n    data = pegs\n)\n\n\npegs_mod4 = ivreg(\n    formula = body_mass ~ species | species * flipper_length  | species * (bill_depth + bill_length),\n    data = pegs\n)\n由此，我们从原有的不带交互项的模型 pegs_mod1, pegs_mod2 升级得到带交互项的模型 pegs_mod3 和 pegs_mod4，完成了最核心的四个模型，它们涵盖了本硕阶段的计量经济的所有重要知识点。我们接下来调用 modelplot() 和 modelsummary() 观察它们的回归参数的差异，发现 pegs_mod3 的结果只有两个显著变量，而 pegs_mod4 的结果全都是显著变量，很显然第四模型的效果优于第三模型。究其原因是因为 bill_depth 和 bill_length 作为工具变量对模型进行了深度调节和限制作用，IV 比 OLS 模型更好地处理了异质性问题。但是它们的具体作用机制很复杂了，我们不需要考虑那些理论层次，谨记教员的训话“集中优势兵力歼敌”，我们先把小坦克的油门踩到底，用闪电战一举歼灭敌人。但是马上我们就遇到了一个严肃的问题，这个图并没有回到我们的核心问题：异质性 Heterogeneity，即虽然已经知道 pegs_mod4 的所有变量都显著，但是它并没有告诉我们每个 species 之间的差异性和显著性的程度。如果同学们能思考到这个层次，就已经到达很高的辩证思维层次，我们继续往下推动。\n\n\nShow Code\n# create two models with interaction terms\npegs_mod3 = lm(\n    formula = body_mass ~ flipper_length * species, \n    data = pegs\n)\n\n\npegs_mod4 = ivreg(\n    formula = \n        body_mass ~ \n        species | species * flipper_length | species * (bill_depth + bill_length),\n    data = pegs\n)\n\n\n# combine models\npegs_mods = list(\n    \"pegs_mod1\" = pegs_mod1, \n    \"pegs_mod2\" = pegs_mod2, \n    \"pegs_mod3\" = pegs_mod3, \n    \"pegs_mod4\" = pegs_mod4\n)\n\n\n# compare models\nmodelsummary(\n    models = pegs_mods, \n    stars = TRUE, \n    statistic = NULL,\n    coef_omit = \"Intercept\",\n    title = \"Summary of pegs_mods\"\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Summary of pegs_mods\n              \n                 \n                pegs_mod1\n                pegs_mod2\n                pegs_mod3\n                pegs_mod4\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  flipper_length\n                  0.712***\n                  1.701***\n                  0.573***\n                  2.145***\n                \n                \n                  speciesChinstrap\n                  -0.256***\n                  -0.493**\n                  -0.188+\n                  -1.256***\n                \n                \n                  speciesGentoo\n                  0.355**\n                  -1.419***\n                  0.186\n                  -1.489***\n                \n                \n                  flipper_length × speciesChinstrap\n                  \n                  \n                  0.033\n                  \n                \n                \n                  flipper_length × speciesGentoo\n                  \n                  \n                  0.377**\n                  \n                \n                \n                  speciesChinstrap × flipper_length\n                  \n                  \n                  \n                  -1.151**\n                \n                \n                  speciesGentoo × flipper_length\n                  \n                  \n                  \n                  -0.794*\n                \n                \n                  Num.Obs.\n                  333\n                  333\n                  333\n                  333\n                \n                \n                  R2\n                  0.787\n                  0.564\n                  0.794\n                  0.543\n                \n                \n                  R2 Adj.\n                  0.785\n                  0.560\n                  0.791\n                  0.536\n                \n                \n                  AIC\n                  441.7\n                  680.0\n                  435.0\n                  699.7\n                \n                \n                  BIC\n                  460.7\n                  699.1\n                  461.6\n                  726.4\n                \n                \n                  Log.Lik.\n                  -215.845\n                  \n                  -210.488\n                  \n                \n                \n                  F\n                  405.281\n                  \n                  251.731\n                  \n                \n                \n                  RMSE\n                  0.46\n                  0.66\n                  0.46\n                  0.68\n                \n        \n      \n    \n\n\n\nShow Code\n# plot models\n(\n    modelplot(pegs_mods, coef_omit = \"Intercept\") +\n    scale_color_iterm() +\n    labs(x = \"Estimate\", title = \"Fig 2.7: Summary of pegs_mods\") + \n    theme_light()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n我们前面已经得到了两个显著模型，pegs_mod3, pegs_mod4，现在需要讨论如何检测其每个组之间的回归系数的差异性，即回答“它们是同质的还是异质的？”。我们需要借助另一个神包 marginaleffects 进行一些计算，算法比较复杂，不用考虑细节，一通运算后得到数值结果，将结果导入 ggplot 并绘制四张子图。其中第一行是 pegs_mod3 的检测结果，左侧绘制总体异质性对比，即 CATE vs ATE 的对比，它反应的是某个组的效应相对于总体效应的差异性；右侧绘制组间异质性对比，即 CATE(a) vs CATE(b) vs CATE(c)，它反应的是两两成对的分组之间的差异性。第二行是 pegs_mod4 的检测结果。简要分析如下。\npegs_mod3 的 species 三个分类 (Adelie, Chinstrap, Gentoo) 相对于零效应都具有显著差异，但是 Gentoo 的系数明显高于 Adelie, Chinstrap，并且 Adelie, Chinstrap 的系数很接近，我们可以大胆推测它们二者之间的组间异质性应该是不显著的。这个推测在右侧子图得到了证实，它显示 Gentoo vs Adelie, Gentoo vs Chinstrap 两组的组间异质性显著，也就是说，这两个配对组之间的回归系数存在较大差异性；但是 Adelie vs Chinstrap 这组的组间异质性 \\(pv = 0.787\\) 不显著，也就是说，这两个配对组之间的回归系数不存在较大差异性。\npegs_mod4 的 species 三个分类的单独的异质性依然显著，但是系数的排序发生了变化，Adelie 的系数最高，Gentoo 的系数最低，而 Chinstrap 的系数居中；并且 Chinstrap, Gentoo 的系数很接近，我们也可以大胆推测它们之间的组间异质性很有可能是不显著的，即它们之间可能是同质性的。右侧的子图再次证明我们的推测，Chinstrap, Adelie 的异质性结果显著，Gentoo, Adelie 的异质性结果显著，但是 Gentoo, Chinstrap 的异质性结果不显著 \\(pv = 0.074\\)，如果它的数值再接近 \\(pv = 0.05\\)，我们或许可以谨慎地认为是“边缘显著”；但本案例它依然比较大，不应当认为是边缘显著，而应当直接认为是不显著。\n综上所述，我们在原有的 OLS-LM 和 IV-LM 当中分别加入交互项，我们不仅观察到了平均效应 ATE 更观察到了条件平均效应 CATE，并且进一步讨论了如何通过 CATE 的两两配对图形来辨析异质性；从数值层面和图表层面全方面地证明，有交互项的模型相比无交互项的模型，在异质性的分析流程当中具有更高的实证价值。这个结论提醒我们，如果条件允许，我们应当尽量构建带有交互项的回归模型，而尽量避免单纯的简单回归模型，即 cat_var * num_var 模型优先于 cat_var + num_var 模型。而至于更深度的“如何解决异质性”的问题已经超过当前的框架，我们暂且不论，继续踩着小坦克的油门突突突往纵深推进，冲鸭。\n\n\nShow Code\n# calculate groupwise heterogeneity of pegs_mod3\npegs_mod3_group_hete = (\n    slopes(\n        model = pegs_mod3, \n        variables = \"flipper_length\", \n        by = \"species\",\n        hypothesis = 0,\n        vcov = vcovHC(pegs_mod3, type = \"HC1\")\n    ) %&gt;% \n    tidy() %&gt;% \n    rename(\n        pv = p.value, \n        conf_lo = conf.low, \n        conf_hi = conf.high\n    ) %&gt;%\n    select(term, species, pv, estimate, conf_lo, conf_hi)\n)\n\n\n# plot groupwise heterogeneity of pegs_mod3\nsub1 = (\n    pegs_mod3_group_hete %&gt;%\n    ggplot(aes(x = species, y = estimate, color = species)) +\n    geom_errorbar(\n        aes(ymin = conf_lo, ymax = conf_hi), linewidth = 1, width = 0.2\n    ) +\n    geom_label(aes(label = scales::pvalue(pv, add_p = TRUE))) +\n    scale_color_iterm() +\n    labs(\n        x = \"\", y = \"Estimate\", title = \"Groupwise Test of pegs_mod3\"\n    ) +\n    theme_light() +\n    theme(\n        legend.position = \"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1)\n    )\n)\n\n\n# calculate pairwise heterogeneity of pegs_mod3\npegs_mod3_pair_hete = (\n    slopes(\n        model = pegs_mod3, \n        variables = \"flipper_length\", \n        by = \"species\",\n        hypothesis = difference ~ pairwise,\n        vcov = vcovHC(pegs_mod3, type = \"HC1\")\n    ) %&gt;% \n    tidy() %&gt;% \n    rename(\n        pairs = hypothesis,\n        pv = p.value, \n        conf_lo = conf.low, \n        conf_hi = conf.high\n    ) %&gt;%\n    select(pairs, pv, estimate, conf_lo, conf_hi)\n)\n\n\n# plot pairwise heterogeneity of pegs_mod3\nsub2 = (\n    pegs_mod3_pair_hete %&gt;% \n    ggplot(aes(x = pairs, y = estimate, color = pairs)) +\n    geom_errorbar(\n        aes(ymin = conf_lo, ymax = conf_hi), linewidth = 1, width = 0.2\n    ) +\n    geom_label(aes(label = scales::pvalue(pv, add_p = TRUE))) +\n    scale_color_iterm() +\n    labs(\n        x = \"\", y = \"Estimate\", title = \"Pairwise Test of pegs_mod3\"\n    ) +\n    theme_light() +\n    theme(\n        legend.position = \"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1)\n    )\n)\n\n\n# calculate zero heterogeneity of pegs_mod4\npegs_mod4_group_hete = (\n    slopes(\n        model = pegs_mod4, \n        variables = \"flipper_length\", \n        by = \"species\",\n        hypothesis = 0,\n        vcov = vcovHC(pegs_mod4, type = \"HC1\")\n    ) %&gt;% \n    tidy() %&gt;% \n    rename(\n        pv = p.value, \n        conf_lo = conf.low, \n        conf_hi = conf.high\n    ) %&gt;%\n    select(term, species, pv, estimate, conf_lo, conf_hi)\n)\n\n\n# plot group heterogeneity of pegs_mod4\nsub3 = (\n    pegs_mod4_group_hete %&gt;%\n    ggplot(aes(x = species, y = estimate, color = species)) +\n    geom_errorbar(\n        aes(ymin = conf_lo, ymax = conf_hi), linewidth = 1, width = 0.2\n    ) +\n    geom_label(aes(label = scales::pvalue(pv, add_p = TRUE))) +\n    scale_color_iterm() +\n    labs(\n        x = \"\", y = \"Estimate\", title = \"Group Test of pegs_mod4\"\n    ) +\n    theme_light() +\n    theme(\n        legend.position = \"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1)\n    )\n)\n\n\n# calculate pairwise heterogeneity of pegs_mod4\npegs_mod4_pair_hete = (\n    slopes(\n        model = pegs_mod4, \n        variables = \"flipper_length\", \n        by = \"species\",\n        hypothesis = difference ~ pairwise,\n        vcov = vcovHC(pegs_mod4, type = \"HC1\")\n    ) %&gt;% \n    tidy() %&gt;% \n    rename(\n        pairs = hypothesis,\n        pv = p.value, \n        conf_lo = conf.low, \n        conf_hi = conf.high\n    ) %&gt;%\n    select(pairs, pv, estimate, conf_lo, conf_hi)\n)\n\n\n# plot pairwise heterogeneity of pegs_mod4\nsub4 = (\n    pegs_mod4_pair_hete %&gt;% \n    ggplot(aes(x = pairs, y = estimate, color = pairs)) +\n    geom_errorbar(\n        aes(ymin = conf_lo, ymax = conf_hi), linewidth = 1, width = 0.2\n    ) +\n    geom_label(aes(label = scales::pvalue(pv, add_p = TRUE))) +\n    scale_color_iterm() +\n    labs(\n        x = \"\", y = \"Estimate\", title = \"Pairwise Test of pegs_mod4\"\n    ) +\n    theme_light() +\n    theme(\n        legend.position = \"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1)\n    )\n)\n\n\n# combine into single figure\n(\n    wrap_plots(sub1, sub2, sub3, sub4) + \n    plot_layout(ncol = 2, nrow = 2) + \n    plot_annotation(title = \"Fig 2.8: Heterogeneity Test\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n残差检验是模型的最后一步，为了更好理解它，我们需要解释两个重要的基础概念。前面已经提过“预测矩阵”，但是没有进行精确界定，所谓的“预测矩阵 Prediction Matrix” \\(\\mathbf{\\widehat{Y}}\\) 就是当我们计算得到了回归参数矩阵 \\(\\mathbf{\\widehat{B}}\\) 之后，将这个参数矩阵与目标矩阵 \\(\\mathbf{X}\\) 相乘，即可得到基于模型的预测值。\n\\[\n\\mathbf{\\widehat{Y}} = \\mathbf{X} \\mathbf{\\widehat{B}}\n\\]\n那么根据常识我们知道，任何预测都不可能完全等于实际数据，因此预测值跟观测值必然有一定差异，而这部分差异就是模型的“残差 Residuals”。在计量经济学中，残差并不仅仅代表预测的“好坏”，它更重要的意义是衡量模型未能捕捉到的随机变动项；残差分析的目的就是检查这些未被解释的部分是否符合统计假设，从而评估模型的有效性。\n\\[\n\\mathbf{e} = \\mathbf{Y} - \\mathbf{\\hat{Y}}\n\\]\n先简单回归我们现有的四个模型：\npegs_mod1: lm(\n    formula = body_mass ~ flipper_length + species, \n    data = pegs\n)\n\n\npegs_mod2: ivreg(\n    formula = body_mass ~ flipper_length | species | bill_depth + bill_length, \n    data = pegs\n)\n\n\npegs_mod3: lm(\n    formula = body_mass ~ flipper_length * species, \n    data = pegs\n)\n\n\npegs_mod4: ivreg(\n    formula = body_mass ~ species | species * flipper_length  | species * (bill_depth + bill_length),\n    data = pegs\n)\n为了更清晰的观察每个模型的综合表现，我们分别计算每一个模型对应的残差和预测值，产生 8 个新的变量；我们将他们合并到原始数据组 pegs 当中，最后的表格结构如下。本小节的残差分析将基于这个扩展的数据组 pegs 进行，而不是原有的 pegs 进行。\n\n\nShow Code\n# calculate residuals and predictions\npegs = (\n    pegs %&gt;% \n    mutate(\n        res_mod1 = residuals(pegs_mod1), fit_mod1 = fitted(pegs_mod1),\n        res_mod2 = residuals(pegs_mod2), fit_mod2 = fitted(pegs_mod2),\n        res_mod3 = residuals(pegs_mod3), fit_mod3 = fitted(pegs_mod3),\n        res_mod4 = residuals(pegs_mod4), fit_mod4 = fitted(pegs_mod4)\n    )\n)\n\n\n# show dataframe\n(\n    pegs %&gt;% \n    slice_sample(n = 10) %&gt;% \n    datasummary_df(title = \"Appended Sample of Penguins\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Appended Sample of Penguins\n              \n                species\n                island\n                sex\n                bill_length\n                bill_depth\n                flipper_length\n                body_mass\n                res_mod1\n                fit_mod1\n                res_mod2\n                fit_mod2\n                res_mod3\n                fit_mod3\n                res_mod4\n                fit_mod4\n              \n        \n        \n        \n                \n                  Chinstrap\n                  Dream\n                  female\n                  0.47\n                  0.38\n                  -0.63\n                  -0.88\n                  -0.10\n                  -0.78\n                  0.09\n                  -0.96\n                  -0.13\n                  -0.75\n                  -0.02\n                  -0.85\n                \n                \n                  Gentoo\n                  Biscoe\n                  female\n                  0.45\n                  -1.09\n                  1.07\n                  0.62\n                  -0.43\n                  1.05\n                  -0.39\n                  1.01\n                  -0.41\n                  1.03\n                  -0.37\n                  0.99\n                \n                \n                  Gentoo\n                  Biscoe\n                  male\n                  1.00\n                  -0.68\n                  1.07\n                  0.90\n                  -0.15\n                  1.05\n                  -0.11\n                  1.01\n                  -0.13\n                  1.03\n                  -0.09\n                  0.99\n                \n                \n                  Gentoo\n                  Biscoe\n                  male\n                  1.11\n                  -0.94\n                  1.36\n                  1.68\n                  0.43\n                  1.25\n                  0.18\n                  1.50\n                  0.38\n                  1.30\n                  0.30\n                  1.38\n                \n                \n                  Chinstrap\n                  Dream\n                  male\n                  1.46\n                  1.19\n                  0.36\n                  -0.31\n                  -0.24\n                  -0.07\n                  -1.04\n                  0.73\n                  -0.17\n                  -0.15\n                  -0.45\n                  0.14\n                \n                \n                  Adelie\n                  Torgersen\n                  female\n                  -0.79\n                  0.02\n                  -0.35\n                  -0.81\n                  -0.49\n                  -0.32\n                  -0.83\n                  0.01\n                  -0.44\n                  -0.38\n                  -1.09\n                  0.28\n                \n                \n                  Gentoo\n                  Biscoe\n                  female\n                  0.34\n                  -1.49\n                  1.29\n                  0.62\n                  -0.58\n                  1.20\n                  -0.76\n                  1.38\n                  -0.61\n                  1.23\n                  -0.66\n                  1.28\n                \n                \n                  Adelie\n                  Dream\n                  female\n                  -1.36\n                  0.43\n                  -1.35\n                  -1.31\n                  -0.28\n                  -1.03\n                  0.37\n                  -1.68\n                  -0.36\n                  -0.95\n                  0.54\n                  -1.85\n                \n                \n                  Adelie\n                  Dream\n                  male\n                  -0.44\n                  0.68\n                  0.01\n                  -0.25\n                  -0.19\n                  -0.07\n                  -0.87\n                  0.62\n                  -0.08\n                  -0.17\n                  -1.30\n                  1.04\n                \n                \n                  Gentoo\n                  Biscoe\n                  male\n                  0.23\n                  -0.38\n                  1.57\n                  2.18\n                  0.78\n                  1.40\n                  0.32\n                  1.86\n                  0.68\n                  1.50\n                  0.52\n                  1.66\n                \n        \n      \n    \n\n\n\n基于这个扩展的数据组 pegs，我们对残差进行以下三项检验流程，分别是：\n\n正态性 Normality: 残差的分布应当符合正态分布规律。\n多重共线性 Multi-colinearity: 多个自变量之间应该不存在显著的线性相关。\n异方差性 Heteroskedasticity: 残差的方差在多个自变量之间应该保持基本稳定。\n\n我们希望检验的结果是，残差符合正态分布、变量不存在多重共线性、残差没有异方差性。这样就可以基本推定我们的模型具备较高的严谨性、稳健性、可靠性；我们现在逐个简要介绍这三项检验。\n\n\n\n我们现在讨论残差分析的第一个方面：正态性，即残差的分布应该呈现基本的正态分布特征。一般通过 Shapiro-Wilk 进行数值检测和 QQ Plot 进行图形观察，这部分检验的理论已经非常成熟，经典文献可以参考 (Shapiro1965?), (Royston1982?), (Royston1995?)。如果 sw_test() 结果显著，则残差不符合正态分布；反之则符合。而 QQ Plot 能直观地展示残差分布与理论正态分布的吻合程度，它揭示数据点是否紧密地沿着一条直线分布，有助于判断残差是否存在偏离正态分布。但是根据中心极限定理 (Central Limit Theorem)，当样本数量足够大 \\(n &gt; 100\\) 的情况下，即使正态性检验显著，只要 QQ 图没有显示出极端异常的偏离，我们通常也可以接受这个结果，而不必机械地纠结于它在数值上是否“显著”。\n回到本案，我们计算 pegs 四个模型的残差 res_mod1, res_mod2, res_mod3, res_mod4 并进行正态性检验，数值结果显示前两个模型 pegs_mod1 和 pegs_mod2 满足正态性要求，但是后两个模型 pegs_mod3 和 pegs_mod4 不满足；而图形结果则提示模型没有特别明显的偏离值，应该是符合正态分布。如前文所述，由于我们的样本量 \\(N=333\\) 足够大，虽然 sw_test() 呈显著结果，但是这种程度的非正态性并不会对我们的最终结论构成严重威胁；我们将此检验结果作为模型稳健性诊断的一个信号，然后踩着小坦克油门继续往下推。\n\n\nShow Code\n# test normality of residuals\n(\n    bind_rows(\n        shapiro.test(pegs$res_mod1) %&gt;% tidy(),\n        shapiro.test(pegs$res_mod2) %&gt;% tidy(),\n        shapiro.test(pegs$res_mod3) %&gt;% tidy(),\n        shapiro.test(pegs$res_mod4) %&gt;% tidy()\n    ) %&gt;%\n    rename(pv = p.value) %&gt;%\n    mutate(model = c(\"pegs_mod1\", \"pegs_mod2\", \"pegs_mod3\", \"pegs_mod4\")) %&gt;%\n    select(model, statistic, pv, -method) %&gt;%\n    datasummary_df(title = \"Normality Test\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Normality Test\n              \n                model\n                statistic\n                pv\n              \n        \n        \n        \n                \n                  pegs_mod1\n                  0.99\n                  0.20\n                \n                \n                  pegs_mod2\n                  0.99\n                  0.09\n                \n                \n                  pegs_mod3\n                  0.99\n                  0.02\n                \n                \n                  pegs_mod4\n                  0.98\n                  0.00\n                \n        \n      \n    \n\n\n\n\n\nShow Code\n# plot QQ of mod1\nsub1 = (\n    pegs %&gt;%\n    ggplot(aes(sample = res_mod1, color = species)) +\n    stat_qq() +\n    stat_qq_line() +\n    scale_color_iterm() +\n    labs(x = \"Theoretical\", y = \"Sample\", title = \"pegs_mod1\") +\n    theme_light() +\n    facet_wrap(~ species)\n)\n\n\n# plot QQ of mod2\nsub2 = (\n    pegs %&gt;% \n    ggplot(aes(sample = res_mod2, color = species)) +\n    stat_qq() +\n    stat_qq_line() +\n    scale_color_iterm() +\n    labs(x = \"Theoretical\", y = \"Sample\", title = \"pegs_mod2\") +\n    theme_light() +\n    facet_wrap(~ species)\n)\n\n\n# plot QQ of mod3\nsub3 = (\n    pegs %&gt;% \n    ggplot(aes(sample = res_mod3, color = species)) +\n    stat_qq() +\n    stat_qq_line() +\n    scale_color_iterm() +\n    labs(x = \"Theoretical\", y = \"Sample\", title = \"pegs_mod3\") +\n    theme_light() +\n    facet_wrap(~ species)\n)\n\n\n# plot QQ of mod4\nsub4 = (\n    pegs %&gt;% \n    ggplot(aes(sample = res_mod4, color = species)) +\n    stat_qq() +\n    stat_qq_line() +\n    scale_color_iterm() +\n    labs(x = \"Theoretical\", y = \"Sample\", title = \"pegs_mod4\") +\n    theme_light() +\n    facet_wrap(~ species)\n)\n\n\n# combine into single figure\n(\n    wrap_plots(sub1, sub2, sub3, sub4) + \n    plot_layout(ncol = 2, nrow = 2, guides = \"collect\") + \n    plot_annotation(title = \"Fig 2.9: Normality Test\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n我们现在讨论残差分析的第二个话题：多重共线性，即某个自变量与另外的自变量之间存在高度相关性。在因果判断时，它的存在会让我们无法精确地辨识出每个自变量的独立影响，即结果 Y 是由因素 D1 所致，但是 D1 又跟 D2 高度线性相关，那说明 D2 也对结果 Y 产生了影响。这是我们不希望遇到的情况，各个自变量的系数估计值会变得不稳定、标准误增大，从而降低了我们对模型结果的信心和精确度。我们主要通过 vif() 函数检测共线性，这部分的经典文献来自于 (Belsley1980?), (Fox2016?)，算法得出的 GVIF 作为主要判断标准：\n\n如 GVIF &lt; 5，则自变量基本上不存在多重共线性。\n如 5 &lt; GVIF &lt; 10，则自变量具有一定共线性。\n如 10 &lt; GVIF，则自变量存在严重共线性。\n\n从模型结果来看，只有 pegs_mod1 不具有共线性，而其他三个模型都存在共线性，pegs_mod4 的最高共线性甚至冲破了 60。这个问题的主要原因是，我们的自变量当中包含了分类变量，而分别变量被当成哑变量，它与数值变量相乘，会产生很多交互数据，所以导致严重共线性。为了更好地解释模型，我们不再依赖 GVIF 而采信 adj_GVIF，其判断标准依然以 \\(\\text{adj\\_GVIF} = 10\\) 作为重度共线性的分界。另外，对于已经存在共线性的模型，有时候可以通过 scales() 函数进行处理，但如果共线性是由于自变量本身所引起的，那么即使进行了 scales() 也无法从根本上消除。我们在数据处理的第一步就执行了 scales()，这说明当前观察到的多重共线性是更深层次的结构性问题，而非简单的变量尺度差异所致，因此通过进一步的标准化也难以消除。\n回到本案当中，在我们最关心的变量 flipper_length:species 当中，pegs_mod3 具有较少的共线性，但是 pegs_mod4 具有一定的共线性；因此我们虽然可以继续使用 pegs_mod4 但必须谨慎地汇报结果，而不能过于乐观地解读。导致这种较高共线性的原因主要有两个方面：（1）模型交互项，pegs_mod3 和 pegs_mod4 都包含了交互项，交互项很容易与原始的主效应产生相关性。（2）样本量限制，总数据样本只有 3 百多行，这对于 pegs_mod4 这样一个包含了多重交互项的复杂工具变量模型而言，很容易出现高度相关性，导致了在复杂模型中难以避免的共线性问题。另外，需要注意的是，增加分类变量的维度并不会降低共线性，反而通常会因为引入更多的哑变量和交互项而增加多重共线性的风险，特别是在小样本量的情况下。\n\n\nShow Code\n# combine into a list\npegs_mods = list(\n    \"pegs_mod1\" = pegs_mod1,\n    \"pegs_mod2\" = pegs_mod2,\n    \"pegs_mod3\" = pegs_mod3,\n    \"pegs_mod4\" = pegs_mod4\n)\n\n\n# create UDF to re-run calculation\nget_gvif = function(model, model_name) {\n    vif(model) %&gt;% \n    as.data.frame() %&gt;% \n    rownames_to_column(var = \"terms\") %&gt;% \n    rename(\n        DF = Df, \n        adj_GVIF = `GVIF^(1/(2*Df))`\n    ) %&gt;%\n    mutate(\n        model = model_name,\n        adj_GVIF = (adj_GVIF)^2\n    ) %&gt;% \n    select(model, terms, DF, GVIF, adj_GVIF)\n}\n\n\n# get gvif results\npegs_gvif = map2(pegs_mods, names(pegs_mods), get_gvif) %&gt;% list_rbind()\n\n\n# show gvif table\npegs_gvif %&gt;% datasummary_df(title = \"Multi-colinearity Test\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        Multi-colinearity Test\n              \n                model\n                terms\n                DF\n                GVIF\n                adj_GVIF\n              \n        \n        \n        \n                \n                  pegs_mod1\n                  flipper_length\n                  1.00\n                  4.44\n                  4.44\n                \n                \n                  pegs_mod1\n                  species\n                  2.00\n                  4.44\n                  2.11\n                \n                \n                  pegs_mod2\n                  species\n                  2.00\n                  10.17\n                  3.19\n                \n                \n                  pegs_mod2\n                  flipper_length\n                  1.00\n                  10.17\n                  10.17\n                \n                \n                  pegs_mod3\n                  flipper_length\n                  1.00\n                  10.57\n                  10.57\n                \n                \n                  pegs_mod3\n                  species\n                  2.00\n                  12.36\n                  3.52\n                \n                \n                  pegs_mod3\n                  flipper_length:species\n                  2.00\n                  16.19\n                  4.02\n                \n                \n                  pegs_mod4\n                  species\n                  2.00\n                  76.89\n                  8.77\n                \n                \n                  pegs_mod4\n                  flipper_length\n                  1.00\n                  60.24\n                  60.24\n                \n                \n                  pegs_mod4\n                  species:flipper_length\n                  2.00\n                  88.05\n                  9.38\n                \n        \n      \n    \n\n\n\nShow Code\n# plot gvif chart\n(\n    pegs_gvif %&gt;% \n    ggplot(aes(x = terms, y = adj_GVIF, fill = model)) +\n    geom_col(position = \"dodge\") +\n    geom_hline(yintercept = 10, color = \"gray\", linetype = \"dashed\") +\n    scale_fill_iterm() +\n    labs(\n        x = \"\", y = \"Adjusted GVIF\", \n        title = \"Fig 2.10: Multi-colinearity Test\"\n    ) +\n    theme_light()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n我们现在讨论残差分析的最后一个方面：异方差性，即残差在整个样本当中的稳定性。\n\n异方差性 Heteroskedasticity: 残差的方差随自变量变化而存在显著差异，即残差的变动幅度不是恒定的。\n同方差性 Homoskedasticity: 残差的方差随自变量变化而没有显著差异，即残差的变动幅度是恒定的。\n\n在线性模型中，我们总是希望模型满足同方差性，而拒绝异方差性；可以简单的认为“异方差性”总是“不好的”、“同方差性”总是“好的”。假如我们一个模型当中有很多个残差，它们被划分为多个小组，选择任意其中的第 \\(i, \\ j\\) 两个小组，那么它们各自的方差应该非常接近样本的平均方差，即可得到异质性检验的重要标准：\n\\[\n\\sigma_i^2 = \\sigma_j^2 = \\sigma^2, \\quad \\forall i \\neq j\n\\]\n注意，\\(i,\\ j\\) 一般都是分类变量，如汽车品牌、车辆颜色、经销商名称、注册城市等等；但这并不是排斥数值变量，某些特定情况的数值变量也可以作为分组依据，但是需要谨慎对待；绝大多数经济学的实证分析依然以分类变量的维度来讨论异方差性。回到本案中，我们有四个模型，但是其中 pegs_mod1, pegs_mod3 是没有交互项的模型，而 pegs_mod2, pegs_mod4 是具有交互项的模型。因此，我们必须根据模型性质分别处理，用现成的 bptest() 检测 OLS-LM 的同方差性，再手动编写一个半自动的 phtest() 检测 IV-LM 的同方差性，有关经典文献可以参考 (Breusch1979?), (Pagan1983?), (Cameron2005?)。\n我们继续略过算法细节，跑出结果显示 pegs_mod1 显著，即模型中存在异方差；pegs_mod2 处于边缘显著，我们可以从显著或者不显著的视角同时开展分析，但是必须非常小心；pegs_mod3, pegs_mod4 都显著，即模型都存在异方差。但是需要特别注意，因为我们的模型中包含了分类变量的交互项，此时异方差性是较为常见的现象。而至于异方差在模型当中实际反应的深层问题或意义，已经牵扯到它与“研究设计 Research Design”的复杂关联，通常需要对该研究设计具有更深度的掌握和理解；我们暂且不论这些复杂的“研究设计”问题，重点在于理解异方差检验的框架，如果能掌握到这个程度，对于本硕博的论文写作已经很足够了。\n\n\nShow Code\n##### test heterosckedasticity for OLS models\n(\n    bind_rows(\n        bptest(pegs_mod1) %&gt;% tidy() %&gt;% mutate(model = \"pegs_mod1\"),\n        bptest(pegs_mod3) %&gt;% tidy() %&gt;% mutate(model = \"pegs_mod3\")\n    ) %&gt;% \n    rename(pv = p.value) %&gt;%\n    select(model, statistic, pv) %&gt;% \n    datasummary_df(title = \"Heterosckedasticity Test of OLS Models\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Heterosckedasticity Test of OLS Models\n              \n                model\n                statistic\n                pv\n              \n        \n        \n        \n                \n                  pegs_mod1\n                  8.77\n                  0.03\n                \n                \n                  pegs_mod3\n                  10.95\n                  0.05\n                \n        \n      \n    \n\n\n\nShow Code\n##### test heterosckedasticity for IV models\n# create UDF to run Pagan-Hall test\nphtest = function(iv_model) {\n\n    # calculate Pagan-Hall test statistic\n    res_squared = I(residuals(iv_model)^2)\n    instruments = model.matrix(iv_model, component = \"instruments\")\n    aux_model = lm(res_squared ~ instruments - 1)\n    n = nobs(iv_model)\n    r2 = summary(aux_model)$r.squared\n    test_stat = n * r2\n    df = ncol(instruments) - 1\n    pv_chi = pchisq(test_stat, df = df, lower.tail = FALSE)\n\n    # combine results\n    results = tibble(\n        statistic = test_stat,\n        df = df,\n        pv = pv_chi\n    )\n\n    # show results\n    return(results)\n}\n\n\n# run Pagan-Hall test\n(\n    bind_rows(\n        phtest(pegs_mod2) %&gt;% mutate(model = \"pegs_mod2\"),\n        phtest(pegs_mod4) %&gt;% mutate(model = \"pegs_mod4\")\n    ) %&gt;%\n    select(model, statistic, pv) %&gt;% \n    datasummary_df(title = \"Heterosckedasticity Test of IV Models\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Heterosckedasticity Test of IV Models\n              \n                model\n                statistic\n                pv\n              \n        \n        \n        \n                \n                  pegs_mod2\n                  111.56\n                  0.00\n                \n                \n                  pegs_mod4\n                  109.81\n                  0.00\n                \n        \n      \n    \n\n\n\n如果模型具有异方差，我们无法通过简单的模型变换以消除异方差性，最常见的处理方法是使用异方差稳健标准误 (Robust Standard Error)。虽然听起来它很强大，但它实际上并不能消除异方差性。即，它不能调整回归系数 \\(\\hat{\\beta_i}\\) 本身，它只能适当地调整回归系数的标准误。这意味着，回归系数估计保持不变，但其对应的标准误会被重新计算，以纠正异方差造成的偏差；这个调整后的标准误会用于后续的假设检验和置信区间构建，如一个 95% 的置信区间的计算方法是：\\(\\hat{\\beta_i} \\pm 1.96 * \\text{SE}(\\hat{\\beta_i})\\)。这个区间的结果，就是 RSE 所返回的经过调整后的结果；更多深入理论可以参考经典文献： (White1980?), (Zeileis2004?), (CribariNeto2011?)。\n换个类比，监狱的劳动改造无法将已经判刑的罪犯直接改判为无罪，被判刑 10 年的犯人依然需要服刑 10 年，但是监狱会对犯人进行劳动改造和就业教育，比如法律学习、电气维修、农业种植、生活服务等等，罪犯依然享受与正常人类似的基本人权。一旦他刑满释放，他的劳动技能可以为其提供基本的生活保障，而不会让他出狱后完全脱离社会节奏；异方差稳健标准误并不改变模型的基本设定或系数估计，而是通过修正标准误，确保我们对系数的假设检验和置信区间依然是有效的和可信的。经过劳动改造的罪犯出狱后应当被“合理预期”是正常人，而不应当被歧视为罪犯；经过稳健控制的实证结果应当被“合理预期”是符合要求的结果汇报在结论中。因此，本案例的四个模型对应的结果表格如下。\n\n\nShow Code\n# show robust table\nmodelsummary(\n    model = pegs_mods, \n    stars = TRUE,\n    statistic = NULL,\n    vcov = \"robust\",\n    title = \"Robust SE of pegs_mods\",\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Robust SE of pegs_mods\n              \n                 \n                pegs_mod1\n                pegs_mod2\n                pegs_mod3\n                pegs_mod4\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  -0.070\n                  0.608***\n                  -0.177*\n                  1.031***\n                \n                \n                  flipper_length\n                  0.712***\n                  1.701***\n                  0.573***\n                  2.145***\n                \n                \n                  speciesChinstrap\n                  -0.256***\n                  -0.493*\n                  -0.188+\n                  -1.256***\n                \n                \n                  speciesGentoo\n                  0.355**\n                  -1.419***\n                  0.186\n                  -1.489***\n                \n                \n                  flipper_length × speciesChinstrap\n                  \n                  \n                  0.033\n                  \n                \n                \n                  flipper_length × speciesGentoo\n                  \n                  \n                  0.377***\n                  \n                \n                \n                  speciesChinstrap × flipper_length\n                  \n                  \n                  \n                  -1.151**\n                \n                \n                  speciesGentoo × flipper_length\n                  \n                  \n                  \n                  -0.794*\n                \n                \n                  Num.Obs.\n                  333\n                  333\n                  333\n                  333\n                \n                \n                  R2\n                  0.787\n                  0.564\n                  0.794\n                  0.543\n                \n                \n                  R2 Adj.\n                  0.785\n                  0.560\n                  0.791\n                  0.536\n                \n                \n                  AIC\n                  441.7\n                  680.0\n                  435.0\n                  699.7\n                \n                \n                  BIC\n                  460.7\n                  699.1\n                  461.6\n                  726.4\n                \n                \n                  Log.Lik.\n                  -215.845\n                  \n                  -210.488\n                  \n                \n                \n                  F\n                  464.091\n                  \n                  318.591\n                  \n                \n                \n                  RMSE\n                  0.46\n                  0.66\n                  0.46\n                  0.68\n                \n                \n                  Std.Errors\n                  HC3\n                  HC3\n                  HC3\n                  HC3\n                \n        \n      \n    \n\n\n\nShow Code\n# plot robust models\n(\n    modelplot(model = pegs_mods, vcov = \"robust\") +\n    scale_color_iterm() +\n    labs(\n        x = \"Coefficient and 95% CI\", y = \"\", \n        title = \"Fig 2.11: Summary of Robust Models\"\n    ) +\n    theme_light()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n至此，我们已经跑完了一个 Linear Model 最主要的环节，有必要进行一些小结；我们开着小坦克突突突往前冲，略过了很多细节，是为了集中大家的注意力在最核心的原理上。希望同学们在学习过程中，不要被花里胡哨的数学证明所迷惑，始终关注在基础原理和逻辑思维上；抓基层、打基础、苦练基本功。\n\nModeling\n\n对原始数据进行必要的清洗，形成干净数据 tidy data；\n严肃的文章应当保留完整的数据清理代码，而不能只提供一个数据结果；\n明确研究设计的主体架构，初步梳理可能的因果关系传导机制和影响强度；\n建立简单的多元回归模型 lm(Y ~ X) 观察基准回归结果；\n\nExogeneity\n\n将简单模型扩展至工具变量模型 ivreg(Y ~ X_exo | X_edo | Z)；\n根据 2SLS 开展 Weak Instrument, Wu-Hausman, Sargan 检验；\n如果不存在内生性，则继续使用 OLS-LM；如果存在内生性，则考虑使用 IV-LM；\n工具变量需要耐心、运气、想象力，不要着急，在失败中寻找正确的路径；\n\nHeterogeneity\n\n在原有模型中增加交互项目，将 “+” 改成 “*“；\n若是 OLS-LM: lm(Y ~ X1 * X2)；\n若是 IV-LM: ivreg(Y ~ X_exo | X_exo * X_edo | X_exo * Z)；\n注意观察 Groupwise VS Pairwise 差异性，有时候会爆大装备；\n\nResiduals\n\n正态性，使用 shapiro.test() 检验，同时参考 QQ 图形；\n多重共线性，使用 vif() 检验，主要参考 adj_GVIF 的结果；\n异方差性，使用 bptest() 检验 OLS-LM，使用 phtest() 检验 IV-LM；\n模型异方差无法根治，可以通过 vcov = \"robust\"() 进行稳健性调整；"
  },
  {
    "objectID": "posts/econ/econ.html#sec-lm-overview",
    "href": "posts/econ/econ.html#sec-lm-overview",
    "title": "Practical Econometrics with R",
    "section": "",
    "text": "首先调取如下的工具包，暂时不做过多解释，每个都有其独特的功能，后续章节会逐步用到它们。\n\n\nShow Code\n# install and load pacman\nif (!requireNamespace(\"pacman\", quietly = TRUE)) {\n    install.packages(\"pacman\")\n}\n\n\n# load all common packages\npacman::p_load(\n    palmerpenguins, tidyverse, modelsummary, flextable, ggdag, dagitty, \n    ggsci, ivreg, marginaleffects, scales, pandoc, car, broom, nortest, \n    lmtest, patchwork, sandwich, GGally, fixest, ggfixest, fpp3, ggforce, \n    urca, readxl, writexl, tidyquant, PortfolioAnalytics, tsgarch, equatags, \n    ROI.plugin.quadprog, ROI.plugin.glpk, here\n)\n\n\n我们直接讨论基础的多元回归，导入 tidyverse 调用 penguins 数据组并且进行标准化，随机选择 10 行数据进行观测，我们不在乎这个数据具体数值内容，而是从列的角度看待数据。列 columns 也可以被称为变量 variables，而行 rows 也可以被称为观测 observations；变量一般包括数值型 numeric (1,2,3)，分类型 catgorical (CHN, USA, UK, JPN)，时间型 temporal (2025-1-4, 12:31:44), 字符型 string (Jordan, Kobe, James) 等基本分类，不同的编程语言都有大同小异的处理方法，暂不深入展开。本案数据组 pegs 有 7 列，包括 3 个分类变量和 4 个数值变量，结构清晰简单，很适合初学。\n\n\nShow Code\n# clean data\npegs = (\n    penguins %&gt;% \n    transmute(\n        species,\n        island,\n        sex,\n        bill_length = as.numeric(scale(bill_length_mm)),\n        bill_depth = as.numeric(scale(bill_depth_mm)),\n        flipper_length = as.numeric(scale(flipper_length_mm)),\n        body_mass = as.numeric(scale(body_mass_g)),\n    ) %&gt;%\n    drop_na()\n)\n\n\n# show sample\n(\n    pegs %&gt;% \n    slice_sample(n = 10) %&gt;% \n    datasummary_df(title = \"Sample of Penguins\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Sample of Penguins\n              \n                species\n                island\n                sex\n                bill_length\n                bill_depth\n                flipper_length\n                body_mass\n              \n        \n        \n        \n                \n                  Gentoo\n                  Biscoe\n                  male\n                  1.19\n                  -0.73\n                  1.50\n                  1.93\n                \n                \n                  Adelie\n                  Torgersen\n                  male\n                  0.03\n                  0.43\n                  0.65\n                  -0.25\n                \n                \n                  Chinstrap\n                  Dream\n                  female\n                  -0.08\n                  0.48\n                  0.08\n                  -1.00\n                \n                \n                  Gentoo\n                  Biscoe\n                  male\n                  0.40\n                  -1.04\n                  1.00\n                  1.12\n                \n                \n                  Adelie\n                  Dream\n                  male\n                  -1.12\n                  0.48\n                  -0.56\n                  -0.56\n                \n                \n                  Chinstrap\n                  Dream\n                  male\n                  0.97\n                  0.53\n                  -0.42\n                  0.25\n                \n                \n                  Chinstrap\n                  Dream\n                  male\n                  1.61\n                  1.34\n                  -0.28\n                  -0.59\n                \n                \n                  Chinstrap\n                  Dream\n                  female\n                  -0.55\n                  -0.28\n                  -0.99\n                  -1.25\n                \n                \n                  Gentoo\n                  Biscoe\n                  male\n                  0.23\n                  -0.38\n                  1.57\n                  2.18\n                \n                \n                  Gentoo\n                  Biscoe\n                  male\n                  0.09\n                  0.08\n                  1.29\n                  1.31\n                \n        \n      \n    \n\n\n\n我们通过 ggplot 绘图，对数据的分布进行初步观察，通过 boxchart 观察一个数值变量在多个分类变量当中的分布情况，通过 density plot 观察数值变量在多个分类变量当中的密度分布情况，通过 scatter plot 观察两个数值变量在多个分类变量之间的相关性。当然还有很多其他类型的图表，但这三种是学术论文当中最常见的类型，我们暂时先掌握这几个核心图表，后续在其他课程中再讨论其他类型的图表。下图提醒以下特征：\n\nflipper_length 在三个 species 当中分布基本均匀，且 Gentoo 的 flipper_length 明显高于另外两个 species；Adelie 和 Chinstrap 之间虽然有差异，但是差异较小。\nbody_mass 在三个 species 当中分布也基本均匀，且 Gentoo 的 body_mass 明显高于另外两个 species；Adelie 和 Chinstrap 之间虽然有差异，但是差异较小。\nbody_mass 很明显受到 flipper_length 的影响，即随着 flipper_length 的增加，body_mass 也增加，并且该特征在多个 species 当中保持基本稳定；或者说 flipper_length 和 body_mass 之间具有较强的正相关性。\n\n\n\nShow Code\n# plot box figure\nsub1 = (\n    pegs %&gt;% \n    ggplot(aes(x = species, y = flipper_length, fill = species)) +\n    geom_boxplot() +\n    scale_fill_iterm() +\n    labs(x = \"Species\", y = \"Flipper Length\", title = \"Boxchart\") +\n    theme_light()\n)\n\n\n# plot KDE figure\nsub2 = (\n    pegs %&gt;% \n    ggplot(aes(x = body_mass, color = species)) +\n    geom_density() +\n    scale_color_iterm() +\n    labs(x = \"Body Mass\", y = \"Density\", title = \"KDE\") +\n    theme_light()\n)\n\n\n# plot scatter figure\nsub3 = (\n    pegs %&gt;% \n    ggplot(aes(x = bill_length, y = body_mass, color = species)) +\n    geom_point() +\n    scale_color_iterm() +\n    labs(x = \"Bill Length\", y = \"Body Mass\", title = \"Scatter\") +\n    theme_light() +\n    facet_wrap(~ species)\n)\n\n\n# combine into single figure\n(\n    wrap_plots(sub1, sub2, sub3) + \n    plot_layout(ncol = 1, nrow = 3) + \n    plot_annotation(title = \"Fig 2.1: Sample of Penguins\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n一个标准的多元回归模型可以用矩阵表示为：\n\\[\n\\mathbf{Y} = \\mathbf{X} \\mathbf{B} + \\mathbf{e}\n\\]\n其中\n\n\\(\\mathbf{Y}\\) 是因变量，即输出的结果。\n\\(\\mathbf{X}\\) 是自变量，即输入的原因。\n\\(\\mathbf{B}\\) 是回归系数，即自变量对于因变量的影响程度大小。\n\\(\\mathbf{e}\\) 是残差项，即预测模型与真实模型的差异程度。\n\n我们不深入展开公式的历史背景，继续往下推，它等价的广义算法模型就是 ：\nlm(formula = Y ~ X, data = df)\n回到本案，我们建立一个简单的多元回归模型，即通过 flipper_length 和 species 两个自变量来预测企鹅的 body_mass。此时，传统教科书会进行一大堆抽象的数学公式计算，但我觉得数学证明对经管同学并不是很有必要。牢记“我们是乘坐飞机的乘客而不是驾驶飞机的飞行员”这个根本原则，抓住重点、纲举目张、不要陷入数学迷宫无法自拔；因此我们不会手搓数学而是直接调用 R 进行计算，如果得到 \\(p &lt; 0.05\\)，那么就是显著影响；反之那么就没有显著影响；对理论背景有兴趣的同学可以参考 (Angrist2009?), (Stock2018?), (Wooldridge2019?), (Gujarati2020?)。\npegs_mod1 = lm(\n    formula = body_mass ~ flipper_length + species, \n    data = pegs\n)\n代码运算完成后显示，三个主要变量都显著 (\\(p &lt; 0.05\\))，这意味着模型总体上具有很好的结果，即 flipper_lengh 的变化会导致 body_mass 的变化；species 是分类变量而不是数值变量，因此应该描述为“企鹅从 A 种类变为 B 种类后，体重也会随之变化 X 个单位”；至于它们具体的回归参数数值大小和 R2, AIC, BIC, F, RMSE 等参数，我们暂不解释。这个显著结果对于普通本科同学的论文就基本足够了，但是对于硕士同学是不足够的，我们还需要进一步研究一些深度的问题。\n\n\nShow Code\n# fit models\npegs_mod1 = lm(body_mass ~ flipper_length + species, data = pegs)\n\n\n# show model summary\nmodelsummary(\n    models = pegs_mod1, \n    stars = TRUE, \n    statistic = NULL,\n    coef_omit = \"Intercept\",\n    title = \"Summary of pegs_mod1\"\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Summary of pegs_mod1\n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  flipper_length\n                  0.712***\n                \n                \n                  speciesChinstrap\n                  -0.256***\n                \n                \n                  speciesGentoo\n                  0.355**\n                \n                \n                  Num.Obs.\n                  333\n                \n                \n                  R2\n                  0.787\n                \n                \n                  R2 Adj.\n                  0.785\n                \n                \n                  AIC\n                  441.7\n                \n                \n                  BIC\n                  460.7\n                \n                \n                  Log.Lik.\n                  -215.845\n                \n                \n                  F\n                  405.281\n                \n                \n                  RMSE\n                  0.46\n                \n        \n      \n    \n\n\n\nShow Code\n# plot model coefficients\n(\n    pegs_mod1 %&gt;% \n    modelplot(coef_omit = \"Intercept\") + \n    labs(\n        x = \"Estimate and 95% CI\", \n        title = \"Fig 2.2: Summary of pegs_mod1\"\n    ) +\n    theme_light()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n相关性 Correlation 是计量经济学的起点，但仅有相关性是不够严谨的，更重要的是因果性 Causality。从法学的角度来讨论，张明楷老师在《刑法学》提出：“因果关系是引起与被引起的关系，前者是后者的原因，后者是前者的结果。”因果关系的理论存在比较复杂的讨论，张明楷老师偏向于认同主流的“条件说”：如果没有 X，就必然不会有 Y，那么 X 是 Y 的原因，Y 是 X 的结果。这个原理很容易理解，假如张三用刀把李四杀死，那么“张三用刀捅杀”是“李四死亡”的必要原因，“李四死亡”是“张三用刀捅杀”的危害结果；反之，如果没有“张三用刀”，那么就不会有“李四死亡”的危害结果，这就是典型的因果关系(Zhang2024?)。但是刑事案件往往不会这么简单，而是多个原因共同地、动态地、复杂地发生作用，最终导致了严重法益侵害结果。我们请出“法外狂徒张三”带来以下几个小案例：\n\n张三欲杀害李四，尾随李四下夜班，到路口用水果刀朝着李四胸口捅了几刀，李四倒地；张三认为李四已经死了，便逃离现场，但李四只是装死而没有真的死亡，10 分钟后，李四爬起来正准备打 110 和 120 报警急救，却被酒后驾驶汽车的王五撞倒；李四多处骨折受重伤，王五逃逸，几小时后路人发现血泊中的李四，送医院抢救，但不治身亡。\n张三欲杀害李四，在一起打牌时，张三乘机把李四喝的饮用水换成无色无味的剧毒鹤顶红，所有人都不知情；王五突然说口渴想喝水，李四就自然而然地把自己的水给王五喝，张三见状大喊：“哎别喝那个，我这有更好的”，王五不以为然：“没事，我跟李四发小儿，来打牌打牌”；王五一饮而尽，半小时后毒药起效，心脏骤停，王五当场毙命。\n张三欲杀害李四，将李四汽车的刹车线剪断，希望李四在高速公路发生交通事故而身亡；李四在高速行驶中，因为刹车失灵，撞上另外的汽车，导致汽车翻个底朝天，司机王五被困在车中无法解救，汽车起火迅速燃烧，李四尝试拯救也无能为力；司机王五被烧死在车里，而李四的车辆虽然受到严重损毁，但他本人并未受到严重的人身伤害。\n\n这一系列都是法哲学对于“原因——结果”的深入思考，它们原理对于后期掌握复杂的因果推断和各种假设大有裨益，但有一个主要区别。刑法所讨论的“因果关系”是定性的而非定量的，而经济学讨论的“因果关系”是统计意义上的定量的而非定性的。我们不问：“如果没有 X，是否必然不会发生 Y 的结果”，而是问：“在保持其他因素不变的情况下，X 的变化对 Y 的平均影响有多大？”。换言之，在刑法上，张三朝李四胸口捅了一刀，就构成故意杀人罪，假如张三捅十刀，依然只能构成一个故意杀人罪，而不会构成十个故意杀人罪；在经济学上，张三捅一刀导致李四死亡的概率必然远小于捅十刀致死的概率。一个法官需要确认的是：“张三是否真的朝着李四捅了这一刀？”而一个经济学家需要确认的是：“张三捅的第几刀是导致李四死亡的最关键伤害？”\n有点复杂了，先刹车，回到经济学当中。我们通过 R 当中强大的包 ggdag，很方便地绘制 DAG 图，以非常明确的方式对“因果推断 Causal Inference” 进行抽象维度的理解。具体而言，本案例有两个自变量，即原因 flipper_length, specie，可以认为分别是 \\((X1, X2)\\)；同时本案有一个因变量，即结果 body_mass，可以认为是 \\(Y\\)。那么，假设本案例当中没有任何其他的干扰因素，即“有且仅有 2 个原因，它们导致了 1 个结果”，那么本案的因果关系便是 \\((X1, X2) \\to Y\\)，其 DAG 图如下。需要注意的是，这个假设是很理想注意的假设，实际情况下一定存在未知的干扰因素，而且这样的干扰因素往往会造成严重的结果偏差，具体我们会在后续讨论。\n\n\nShow Code\n# create a DAG\ndag1 = dagitty(\"dag {\n    X1 -&gt; Y\n    X2 -&gt; Y\n    }\n\")\n\n\n# set coordinates\ncoordinates(dag1) = list(\n    x = c(X1 = -1, X2 = -1, Y = 0),\n    y = c(X1 = 1,  X2 = -1, Y = 0)\n)\n\n\n# plot the DAG\n(\n    ggdag(dag1, node = FALSE) +\n    geom_dag_text(color = \"black\", size = 5) +\n    geom_dag_edges(edge_colour = \"gray\") +\n    labs(x = \"\", y = \"\", title = \"Fig 2.3: DAG of Simple LM\") +\n    theme_dag_grid() +\n    coord_fixed(clip = \"off\")\n)\n\n\n\n\n\n\n\n\n\n根据这个 DAG 图，有以下几个问题需要我们深入思考：\n\n回归系数 Coefficients：flipper_length 对于 body_mass 的促进或者抑制作用具体是多少？即，假如 flipper_length 增加或减少一个单位，body_mass 会增加或减少多少？\n内生性 Endogeneity：flipper_length 对于 body_mass 的促进或者抑制作用是否还有其他的影响因素？即，我们当前假设是 flipper_length 导致 body_mass，但是否有可能是 unknown_var 同时影响了 flipper_length 和 body_mass？或者 flipper_length 同时影响了 unknown_var 和 body_mass？我们的模型是否可能遗漏了一些重要变量，或者把一个并没有本质关系的变量牵强地让它成为了一个相关变量？\n异质性 Heterogeneity：在不同的 species 之间，上述的相关性是否会发生变化？即，假如 Adelie 的促进作用是 1.67，那么 Chinstrap 和 Gentoo 的促进作用分别是多少？它们的促进作用是大于 1.67 还是小于 1.67 ？一个适用于所有群体的平均结论，是否掩盖了某些特定群体的重要特征？吃大锅饭的一群人当中，总有一些雷锋同志，也总有一些南郭先生，我们如何将其分辨出来？\n稳健性 Robustness：模型完成后，通过什么方法检测模型本身的正确性和严谨性？即，模型是否满足 Gauss-Markov 所提出的经典假设 Best Linear Unbiased Estimator (BLUE)? 我们得到的这个显著结果是否可能只是巧合，而不是严格的因果关系？假如我们经过艰难险阻，只能得到一个边缘显著的模型 \\(0.050 &lt; p &lt; 0.060\\)，那这个结果是应该接受还是拒绝？\n\n以上几个问题分别对应了应用计量经济学的几个不同的层次，它们以铰链的形式共同构成了一个完整的“因果推断”分析流程，将一个简单线性回归的结论逐渐加深加强，将证据效力从“它小概率是这样”推到“它大概率是这样”最后推到“它很大概率是这样”的维度。因果推断是计量经济学的高级内容，它原本不应该出现的这么早，我故意将其在此处先埋个线索，以便更方便、更快捷、更高效地衔接上下文；但是因果关系的数学部分非常复杂，因此我们不展开理论部分，而是直接介绍原理然后跳入实战，以战养兵、战练结合。对理论部分很感兴趣的博士同学可以参考该领域的权威且前沿的文献 (Imbens2015?)，非博士同学普遍没必要深入阅读因果推断的文献。"
  },
  {
    "objectID": "posts/econ/econ.html#sec-lm-endogenity",
    "href": "posts/econ/econ.html#sec-lm-endogenity",
    "title": "Practical Econometrics with R",
    "section": "",
    "text": "我们已经完成了一个古典回归模型，即结果 body_mass 完全是由原因 flipper_length, species 所导致；换言之，body_mass 没有受到其他因素如 flipper_depth, island, sex 等的影响。但这个模型是不严谨的，它基于一个过于理想主义的假设，即，本模型的因果链条当中没有任何其他外界干扰因素；用学术词语表示为，本模型所有的自变量都是“外生的”，本模型中没有“内生的”自变量。此时我们需要对“外生性 Exogeneity”和“内生性 Endogeneity”进行简要解释。\n\n外生性 Exgeneity: 自变量跟误差项几乎没有相关性 \\(\\text{Cov}(X, \\epsilon) = 0\\)，结果是由该原因所导致。\n内生性 Endogeneity: 自变量跟误差项有一定相关性 \\(\\text{Cov}(X, \\epsilon) \\neq 0\\)，结果并不是完全由该原因所致，还有其他原因共同影响。\n\n“外生性假设”大幅度地简化了数学运算和算法逻辑，但它忽略了一个不可能避免的现实问题，即模型的自变量至少有一些是内生的；在真实的世界里，很少存在某个事件是绝对外生性的，大部分事件都是由多个复杂因素共同作用而成。经济学家往往处于简化角度，将复杂事件想当然地认为是 X -&gt; Y，但是这样的归纳方式很容易得出一个“高度显著但是一派胡言”的结论。这就是内生性所导致的典型偏误，它是所有计量经济学模型所需要面临的第一个严峻挑战，许多经典文献进行了深度讨论，如 (Theil1958?), (Sargan1958?), (Tinbergen1959?), (Hausman1978?), (Imbens1994?), (Staiger1997?)。\n既然明白了内生性的弊端，接下来就必须讨论如何解决它，一个比较符合硕博阶段的方法是通过“工具变量 Instrument Variables, Z”来处理。然而要构建一组好的工具变量并不容易，因为它的因果关系链条受到严格限制，它只能是 Z -&gt; X -&gt; Y，而不能是任何其他的路径；即，它必须满足以下两个必要条件。\n\n相关性 Relevance: 工具变量必须与内生变量显著相关，即 \\(\\text{Cov}(Z, X) \\neq 0\\)。\n排他性 Exclusion: 工具变量不能与因变量或误差项有相关性，即 \\(\\text{Cov}(Z, Y) = \\text{Cov}(Z, \\epsilon) = 0\\)。\n\n我们现在通过 DAG 图解释最基础的 2SLS 原理，下图中的 X -&gt; Y 衍生出了两个新的变量 U, Z：\n\nU 代表未知的协同变量 (Unknown Cofounders)，它们必须同时影响 X 和 Y，但是它永远无法具体计算，因此我们无法知道它对于 X 和 Y 的影响的具体数值是多少，但我们也并不关心这个；同时需要注意 U 不能影响 Z，否则工具变量就失效了。\nZ 代表工具变量 (Instrument Variables)，它们只影响 X 再传递到 Y；特别注意 Z 不能直接影响 Y 或 U，这是工具变量成立的必要条件。另外，工具变量的数量必须大于等于内生变量的数量，否则算法会报错无法执行。\n\n\n\nShow Code\n# create a DAG for 2SLS\ndag3 = dagitty(\"dag{\n    Z -&gt; X -&gt; Y;\n\n    U -&gt; X;\n    U -&gt; Y;\n    }\n\")\n\n\n# set coordinates for variables\ncoordinates(dag3) = list(\n    x = c(Y = 0, X = -1, Z = -2, U = -1),\n    y = c(Y = 0, X = 0,  Z = 0,  U = 1)\n)\n\n\n# plot DAG of 2SLS\n(\n    ggdag(dag3, node = FALSE) +\n    geom_dag_text(color = \"black\", size = 5) +\n    geom_dag_edges(edge_color = \"gray\") +\n    labs(x = \"\", y = \"\", title = \"Fig 2.4 DAG of 2SLS\") +\n    theme_dag_grid() +\n    coord_fixed(clip = \"off\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n我们在前面简要讨论了如何从原理上解决内生性问题，现在介绍如何运用具体的算法完整地实现一个工具变量回归模型，也称 IV-LM 模型，我们需要借用 ivreg() 包以及内部的 Two-Stage Least Square, 2SLS 原理，其细节比较复杂，继续战略性跳过，有兴趣的同学请参考官网。简而言之，2SLS 包括两个阶段，第一阶段是 Purification，第二阶段是 Estimation，他们共同作用完成对内生变量的剥离和对模型的修正。\n第一阶段是 Purification，因为我们模型中的内生变量 X 是“污染的、不纯粹的、有杂质的”变量，因此需要一个工具变量 Z，它起到过滤器的作用，将内生变量的“污染物、杂质、杂音”过滤出去，只保留当中“纯洁、干净、有效的”内容。因此，这个工具变量的“相关性”就非常重要，当且仅当工具变量对内生自变量是强显著和关联的情况下，这个工具变量才具有“过滤污染物”的能力，否则这个工具变量就没有任何意义；但是我们也必须保证工具变量跟干净的结果之间没有相关性，否则就会由于工具变量而干扰了结果，那样我们就无法判断结果到底是由于内生变量所致还是工具变量所致。即，工具变量生效的必要条件是：\n\\[\n\\begin{aligned}\n\\text{Cov}(Z, X) & \\neq 0 \\\\\n\\text{Cov}(Z, Y) & = \\text{Cov}(Z, \\epsilon) = 0\n\\end{aligned}\n\\]\n此时可以带入第一阶段，得到公式：\nfitsr_stage = lm(formula = X ~ Z, data = df)\n\nX_hat = predict(lm(formula = X ~ Z, data = df))\n第二阶段 Estimation，我们需要计算自变量的预测值 X_hat 对于因变量 Y 的显著性，意义在于，当内生变量的“污染物”被剥离出去以后，模型对一个“干净”的因变量能否依然保持强相关性、因果性、解释性？如果此时结果依然显著，那就皆大欢喜、打卡下班；但如果结果不显著，那我们必须继续调整工具变量，然后多次尝试直到搭配出一套满意的组合。总体来说，第二阶段比第一阶段更简单、更直观，但是它的结果必须建立在第一阶段的显著结果之上；如果第一阶段不显著，那么即使第二阶段显著，也没有说服力；但是如果第一阶段显著，而第二阶段不显著，也具有一定说服力，只是这样的情况容易被审稿人盯上；当然最美好的结果是第一阶段和第二阶段都显著。\nsecond_stage = lm(formula = Y ~ X_hat, data = df)\n虽然这两个 lm() 原理上是正确的，但我们并不能手动分两步计算 2SLS，因为这样做会得到错误的置信区间和 p 值；跳过复杂的数学原理，最科学的方案是直接使用 ivreg() 计算，它会在内部完成这两个步骤并自动修正第二阶段的标准误偏差，其表达如下。\niv_mod = ivreg(formula = Y ~ X | Z, data = df)\n这个函数返回一个复杂的合并结果，但是并未提供独立的结果，因此我们必须曲线救国，编写两个简单的调用函数分别提取不同阶段的回归结果，再整理成干净表格。虽然有点绕，但是由于 tidyverse 奠定了强大的底层处理流程，因此整段代码的思路很清晰。其中第一个阶段结果包括以下几项：\n\nWeak Instrument Test：检验工具变量是否显著相关，若显著则说明工具变量有效，反之则无效。\nWu-Hausman Test：检验模型是否存在内生性，若显著则说明存在内生性，需要使用工具变量进行控制，反之则不需要。\nSargan Test：检验工具变量是否满足外生性条件，若显著则说明工具变量是无效的，反之则说明工具变量是有效的。\n\n略过证明细节，我们希望 WI 显著、WH 显著、Sargan 不显著，即可证明原模型是有内生性问题的、工具变量才是有效的、模型才是严谨的。第二阶段追求的结果与普通的线性模型结果类似，主要关注回归系数、显著性、标准误即可。至此，我们已经从概念上完成了一个 IV 模型的主要流程，本小节的内容不可避免的比较抽象，同学们如果感觉不太清楚也无所谓，我们继续用案例加深实际理解；跑完案例再回过头复习此处的抽象内容，相信大家会更加清楚。\n\n\n\n我们现在进行案例分析，回到最基础的 pegs_mod1 模型是：\npegs_mod1 = lm(\n    formula = body_mass ~ flipper_length + species,\n    data = pegs\n)\n其中自变量是 flipper_length 代表企鹅鳍长，species 代表企鹅种类，结果变量是 body_mass 代表企鹅重量；那么根据我们上一节讨论的内容，假设所有的自变量都是内生变量，即 X = flipper_length, species，再假设工具变量是 Z = bill_depth, island, sex，分别代表企鹅的嘴长度、生活岛屿、性别。即可以得到下面的 DAG 图。特别注意，此处的工具变量严格意义上只能满足第一个条件：相关性，而无法满足第二个必要条件：排斥性。我们此处采纳它们仅仅是出于简化的教学角度，在实战分析中，一定要先检验工具变量的两个缺一不可的必要条件；不能想当然地抓一些变量就拿来充当工具变量，那样非常容易得到 False Positive 的结论。扯远了，回本案例中，我们的 DAG 如下图所示：\n\n\nShow Code\n# create DAG\ndag4 = dagitty(\"dag {\n    Z -&gt; X -&gt; Y\n    U -&gt; X\n    U -&gt; Y\n    }\n\")\n\n\n# set coordinates\ncoordinates(dag4) = list(\n    x = c(Y = 2, X = 1, Z = 0, U = 1),\n    y = c(Y = 0, X = 0, Z = 0, U = 1)\n)\n\n\n# set labels\ndag4_label = data.frame(\n    name = c(\"Y\", \"X\", \"Z\", \"U\"),\n    label = c(\n        \"body_mass\", \n        \"flipper_length \\n species\", \n        \"bill_length \\n island \\n sex\", \n        \"U\"\n    )\n)\n\n\n# convert to ggdag type\ndag4 = dag4 %&gt;% tidy_dagitty() %&gt;% left_join(dag4_label, by = \"name\")\n\n\n# plot DAG\n(\n    ggdag(dag4, node = FALSE, text = FALSE) +\n    geom_dag_edges(edge_colour = \"gray\") +\n    geom_dag_text(aes(label = label), color = \"black\", size = 4) +\n    labs(title = \"Fig 2.5: DAG of pegs_mod2\") +\n    theme_dag_grid() +\n    coord_fixed(clip = \"off\")\n)\n\n\n\n\n\n\n\n\n\n再根据 DAG 图和 ivreg() 原理，我们构建如下模型；结果显示，第一阶段的 WI 显著（好结果），WH 显著（好结果），Sargan 不显著（好结果）；因此说明我们选择的工具变量是合适的。第二阶段的自变量对于结果具有更强的促进作用，且显著性更高，这说明我们选择的工具变量有效地剥离了内生变量的污染，从而增强了模型的解释力和预测力。因此我们可以认为，原 OLS-LM 具有较强内生性，需要增加工具变量进行调节；新增的工具变量具有较强效果，经工具变量调节后的 IV-LM 具备更高显著性和稳健性。至此，我们系统地完成了一个 2SLS 计算流程。\nivreg(formula = Y ~ X | Z, data = df)\n\n\nivreg(\n    formula = body_mass ~ species + flipper_length | bill_depth + island + sex, \n    data = pegs\n)\n\n\nShow Code\n# create IV model\npegs_mod2 = ivreg(\n    formula = body_mass ~ species + flipper_length  | bill_depth + island + sex,\n    data = pegs\n)\n\n\n# create UDF to get first stage IV test results\nget_first_stage_test = function(model) {\n    model %&gt;%\n    summary(diagnostics = TRUE) %&gt;%\n    pluck(\"diagnostics\") %&gt;%\n    as.data.frame() %&gt;%\n    rownames_to_column(var = \"terms\") %&gt;%\n    as_tibble() %&gt;%\n    rename(pv = `p-value`) %&gt;%\n    select(terms, statistic, pv) %&gt;%\n    datasummary_df(title = \"First Stage Test\")\n}\n\n\n# create UDF to get second stage IV test results\nget_second_stage_test = function(model) {\n    model %&gt;%\n    summary() %&gt;%\n    pluck(\"coefficients\") %&gt;%\n    as.data.frame() %&gt;%\n    rownames_to_column(var = \"terms\") %&gt;%\n    as_tibble() %&gt;%\n    rename(\n        estimate = Estimate, SE = `Std. Error`, pv = `Pr(&gt;|t|)`\n    ) %&gt;%\n    select(terms, estimate, SE, pv) %&gt;%\n    datasummary_df(title = \"Second Stage Test\")\n}\n\n\n# show test results\nget_first_stage_test(pegs_mod2)\n\n\n\n\n    \n\n    \n    \n      \n        \n        First Stage Test\n              \n                terms\n                statistic\n                pv\n              \n        \n        \n        \n                \n                  Weak instruments (speciesChinstrap)\n                  64.22\n                  0.00\n                \n                \n                  Weak instruments (speciesGentoo)\n                  430.08\n                  0.00\n                \n                \n                  Weak instruments (flipper_length)\n                  129.62\n                  0.00\n                \n                \n                  Wu-Hausman\n                  68.93\n                  0.00\n                \n                \n                  Sargan\n                  2.32\n                  0.13\n                \n        \n      \n    \n\n\n\nShow Code\nget_second_stage_test(pegs_mod2)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Second Stage Test\n              \n                terms\n                estimate\n                SE\n                pv\n              \n        \n        \n        \n                \n                  (Intercept)\n                  0.61\n                  0.13\n                  0.00\n                \n                \n                  speciesChinstrap\n                  -0.49\n                  0.18\n                  0.01\n                \n                \n                  speciesGentoo\n                  -1.42\n                  0.29\n                  0.00\n                \n                \n                  flipper_length\n                  1.70\n                  0.15\n                  0.00\n                \n        \n      \n    \n\n\n\n我们现在有标准线性模型 OLS-LM 和工具变量模型 IV-LM，分别用数值方法和绘图方法对比他们的差异，发现 IV 模型的回归系数有一些优化，但优化效果并不是特别明显；主要原因在于原有的线性模型 pegs_mod1 已经是显著的了，工具变量的最大意义是将一个不显著的结果“优化”成为显著的结果，而不是将一个已经显著的结果再次强化其显著性。好的工具变量就像优秀的公检法机关，它通过严肃论证和周密调查得到铁证如山，将犯罪嫌疑人定罪量刑；而坏的工具变量就像不负责任的公检法机关，它在程序上有诸多漏洞和瑕疵，疏忽大意甚至故意造成冤假错案，此时就算是已经在程序上将嫌疑人定罪量刑甚至执行了死刑，但是由于严重的程序违法，这些案件最终还是要被翻案。此图下发提供了两个中国刑事司法历史中两个真实的案例作为附录，引导同学们从法学、哲学、经济学、公平正义的底层视角对“因果关推断”有更加透彻的理解。\n\n\nShow Code\n# combine into list\npegs_mods = list(\n    \"pegs_mod1\" = pegs_mod1, \n    \"pegs_mod2\" = pegs_mod2\n)\n\n\n# show comparison table\nmodelsummary(\n    models = pegs_mods, \n    stars = TRUE,\n    statistic = NULL,\n    coef_omit = \"Intercept\",\n    title = \"Summary of pegs_mods\"\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Summary of pegs_mods\n              \n                 \n                pegs_mod1\n                pegs_mod2\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  flipper_length\n                  0.712***\n                  1.701***\n                \n                \n                  speciesChinstrap\n                  -0.256***\n                  -0.493**\n                \n                \n                  speciesGentoo\n                  0.355**\n                  -1.419***\n                \n                \n                  Num.Obs.\n                  333\n                  333\n                \n                \n                  R2\n                  0.787\n                  0.564\n                \n                \n                  R2 Adj.\n                  0.785\n                  0.560\n                \n                \n                  AIC\n                  441.7\n                  680.0\n                \n                \n                  BIC\n                  460.7\n                  699.1\n                \n                \n                  Log.Lik.\n                  -215.845\n                  \n                \n                \n                  F\n                  405.281\n                  \n                \n                \n                  RMSE\n                  0.46\n                  0.66\n                \n        \n      \n    \n\n\n\nShow Code\n# plot comparison figure\n(\n    pegs_mods %&gt;% \n    modelplot(coef_omit = \"Intercept\") + \n    labs(\n        x = \"Estimate and 95% CI\", \n        title = \"Fig 2.6: Summary of pegs_mods\"\n    ) +\n    theme_light()\n)\n\n\n\n\n\n\n\n\n\n附录案件：\n\n聂树斌案件：1994 年 8 月，石家庄某玉米地发现一具女尸；9 月，公安局认为聂树斌有重大作案嫌疑；1995 年 4 月，经过“侦查起诉”环节，石家庄中院审判认定聂树斌犯故意杀人罪，判处死刑立即执行。但是案件卷宗缺失了一些关键的定性证据，聂树斌的家人和辩护律师李树亭都认为他不是真凶，于是走上漫漫维权路。2005 年 5 月，另案嫌疑人王书金交代了 1994 年 8 月的石家庄玉米地女尸案件的全部事实，并称自己是案件的唯一真凶，没有其他人参与，案件引起最高法院重视，进入重审阶段。2016 年 12 月，经过极其复杂的司法环节，最高法院巡回法庭改判聂树斌无罪。此案件的王书金就是重要的工具变量，他从根本上改变了聂树斌案件的性质，假如王书金没有被抓、李树亭律师没有奔走呼号、郑成月警官没有坚持真理，那么聂树斌永远都会背上“杀人犯”的罪名。可惜从聂树斌被错误地执行死刑到被平反，已经过过去了整整 20 年，此案的真凶最终也无从查证。河北高院没有被认定王书金是此案的罪犯，且最高法院已经宣判聂树斌无罪，到底是谁杀害了这位可怜的女青年？公检法机关并没有回答这个问题。她和聂树斌都无辜地成为了我国法治改革滚滚洪流当中的一粒带血的尘埃，愿逝者安息。 CCTV 专访 锵锵三人行 访谈\n佘祥林案件：1994 年 1 月，佘祥林的其中张在玉走失；4 月，当地鱼塘里发现一具高度腐烂的女尸，公安局法医经过“检验”认为是走失的张在玉，公安局认为佘祥林具有重大犯罪嫌疑；9 月，经过简单粗暴的“调查审讯”，荆州中院判处佘祥林犯故意杀人罪，死刑缓期两年执行；1998 年 荆门中院顶住巨大社会道德法律的压力，改判佘祥林有期徒刑 15 年；2005 年 3 月，张在玉“死而复生”返回老家，证明当年鱼塘里的女尸并非张在玉本人，也证明佘祥林是完完全全的冤案；5 月，湖北高院改判佘祥林无罪。佘祥林的运气比聂树斌好太多了，佘祥林的三次审判都在逐级降刑而没有死刑立即执行；并且在他服刑的第 11 年，最关键的工具变量，即本案的“死者”张在玉突然“死而复生”，给当时的政法系统狠狠地上了一课。如果张在玉没有出现，佘祥林再服刑 4 年也就刑满释放出狱了，但余生都要背上“杀妻”的罪名。CCTV 专访 锵锵三人行 访谈"
  },
  {
    "objectID": "posts/econ/econ.html#sec-lm-heterogeneity",
    "href": "posts/econ/econ.html#sec-lm-heterogeneity",
    "title": "Practical Econometrics with R",
    "section": "",
    "text": "根据前面的内容，我们的基础模型 OLS-LM 是：\npegs_mod1 = lm(\n    formula = body_mass ~ flipper_length + species, \n    data = pegs\n)\n已知模型具有显著性，即 flipper_length对 body_mass 具有积极的促进效果。现在需要进一步思考，在不同的 species 分组中（Adelie, Chinstrap, Gentoo），模型的回归显著性是否存在差异？换言之，我们需要回答一些问题：“模型的平均效应 Average Treatment Effects，ATE 在多个分组间是否保持稳定？是否存在某个分组效应很强，但是另外分组效应很弱的情况？是否存在某个分组实际上是弱效应，但是“被平均”而成为了强效应的情况？或者反之，是否存在某个分组实际上是强效应，但是“被平均”而成为了弱效应的情况？”这些奇奇怪怪的问题所探究的本质都是同一个问题，即条件平均效应 Conditional Average Treatment Effects, CATE 的变化情况。同学们不要看到这个名词就恐惧，CATE 非常简单，只不过是 ATE 在某个特定条件下的子集而已，我们现在来拆解它。\n本案中，我们的 ATE 就是 flipper_length 对 body_mass 的平均促进效应，又因为 species 包含三个分类 Adelie, Chinstrap, Gentoo，因此如果我们不考虑全局的平均情况，而考虑每个分类下的局部情况，我们就会得到三个分类各自的回归效应。这就是 flipper_length 对 body_mass 在 Adelie, Chinstrap, Gentoo 三个分组下的促进效应，再简化即可得到三个 CATE 分别是 CATE(Ade), CATE(Chi), CATE(Gen)。它们的简化的数学表达式如下，请注意这是非常抽象的表达式，其中的 ATE 是必须通过另外的模型计算而得到的，而不是直接可以从这个表达式中计算出来。\n\\[\n\\begin{aligned}\n\\text{CATE(Ade)} &= [\\text{ATE} | \\text{species} = \\text{Adelie}] \\\\\n\\text{CATE(Chi)} &= [\\text{ATE} | \\text{species} = \\text{Chinstrap}] \\\\\n\\text{CATE(Gen)} &= [\\text{ATE} | \\text{species} = \\text{Gentoo}]\n\\end{aligned}\n\\]\n此时，我们需要思考的一个重要问题就是 ATE 和 CATE 在多个分组之间的差异程度，即以下的不等式：\n\\[\n\\text{CATE(Ade)} \\quad \\stackrel{?}{=} \\quad\n\\text{CATE(Chi)} \\quad \\stackrel{?}{=} \\quad\n\\text{CATE(Gen)} \\quad \\stackrel{?}{=}\n\\text{ATE}\n\\]\n这就顺理成章地引出了“同质性 Homogeneity”与“异质性 Heterogeneity”两个重要概念，它们的数学表达式如下：\n\n同质性 Homogeneity：条件平均效应在多个组之间不存在显著差异，即，\\(\\text{CATE}_i = \\text{CATE}_j, \\quad \\forall i,j\\)。\n异质性 Heterogeneity：条件平均效应在多个组之间存在较大差异，此时又可以继续划分为以下两种情况：\n\nGroupwise: 某个条件回归效应与平均回归效应存在较大差异，即 \\(\\text{CATE}_i \\neq \\text{ATE}, \\quad \\exists i\\)。\nPairwise: 任意两个条件平均效应之间存在较大差异，即 \\(\\text{CATE}_i \\neq \\text{CATE}_j, \\quad \\exists i,j\\)。\n\n\n至此，我们就从原理上完成了对同质性和异质性的定义和区分，至于它的实际计算会在下一小节讨论，接下来需要思考的是为什么要区分它们以及分别适用于什么样的场景？“异质性”在经济研究领域具重要意义，比如国家出台“延迟退休政策”，“职工”有很多不同分类：“公务员、事业单位、军人、民营企业职员、自由职业者、残障人士、进城务工人群、乡村农民”等等，我们必须考虑“延迟退休”政策对于不同人群的平均效果是否存在显著差异？我们当然不希望中国的年轻人成为发达国家那种不想上班的躺平状态，但更不希望中年人成为日本那种不敢退休的社会恐惧。这就要求政府机关深入研究政策效力的“异质性”，即不同组别的劳动者在受到延迟退休的政策刺激时应该具有不一样的反馈，一方面，年轻人应该积极响应认可这份政策并努力工作；另一方面，适龄离休干部可能存在一定程度的抵触和不理解，这不仅是合情合理的，而且也是完全意料之中的。政策制定者必须充分考虑到组间差异，制定出“弹性退休政策”而不是一刀切的“强制退休政策”，否则容易得到一个“失之毫厘谬之千里”的结论。总之，同质性和异质性是实证分析的第二个重点问题，它们并没有道德上的好坏之分，我们必须根据具体情况开展具体分析，更多关于异质性的权威文献包括 (Rosen1974?), (Mundlak1978?), (Heckman1979?), (Anderson1992?), (Heckman1998?)。\n\n\n\n回到本案，我们现在讨论如何检验模型的异质性。其实很简单，就是通过“交互项 Interaction Terms”，在原模型当中增加一个自变量与分类变量的乘积项目。即把 num_var + cat_var 改为 num_var * cat_var 即可，然后再分析这个新模型的交互项的显著性；假如分类变量有很多子类和很多数值变量，这时的交互项目就会非常复杂，因此出于简便角度，我们并不需要完整写清楚所有的交互项目，重点在于修改符号即可。这部分数学公式比较复杂但是算法原理很简单，因此我们继续跳过数学证明部分，直接给出广义的算法原理：\nlm(formula = Y ~ X1 + X2, data = df)\n\n\nlm(formula = Y ~ X1 * X2, data = df)\n那么根据已有的 pegs_mod1, pegs_mod2 可以推理得到 pegs_mod3, pegs_mod4：\npegs_mod3 = lm(\n    formula = body_mass ~ flipper_length * species, \n    data = pegs\n)\n\n\npegs_mod4 = ivreg(\n    formula = body_mass ~ species | species * flipper_length  | species * (bill_depth + bill_length),\n    data = pegs\n)\n由此，我们从原有的不带交互项的模型 pegs_mod1, pegs_mod2 升级得到带交互项的模型 pegs_mod3 和 pegs_mod4，完成了最核心的四个模型，它们涵盖了本硕阶段的计量经济的所有重要知识点。我们接下来调用 modelplot() 和 modelsummary() 观察它们的回归参数的差异，发现 pegs_mod3 的结果只有两个显著变量，而 pegs_mod4 的结果全都是显著变量，很显然第四模型的效果优于第三模型。究其原因是因为 bill_depth 和 bill_length 作为工具变量对模型进行了深度调节和限制作用，IV 比 OLS 模型更好地处理了异质性问题。但是它们的具体作用机制很复杂了，我们不需要考虑那些理论层次，谨记教员的训话“集中优势兵力歼敌”，我们先把小坦克的油门踩到底，用闪电战一举歼灭敌人。但是马上我们就遇到了一个严肃的问题，这个图并没有回到我们的核心问题：异质性 Heterogeneity，即虽然已经知道 pegs_mod4 的所有变量都显著，但是它并没有告诉我们每个 species 之间的差异性和显著性的程度。如果同学们能思考到这个层次，就已经到达很高的辩证思维层次，我们继续往下推动。\n\n\nShow Code\n# create two models with interaction terms\npegs_mod3 = lm(\n    formula = body_mass ~ flipper_length * species, \n    data = pegs\n)\n\n\npegs_mod4 = ivreg(\n    formula = \n        body_mass ~ \n        species | species * flipper_length | species * (bill_depth + bill_length),\n    data = pegs\n)\n\n\n# combine models\npegs_mods = list(\n    \"pegs_mod1\" = pegs_mod1, \n    \"pegs_mod2\" = pegs_mod2, \n    \"pegs_mod3\" = pegs_mod3, \n    \"pegs_mod4\" = pegs_mod4\n)\n\n\n# compare models\nmodelsummary(\n    models = pegs_mods, \n    stars = TRUE, \n    statistic = NULL,\n    coef_omit = \"Intercept\",\n    title = \"Summary of pegs_mods\"\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Summary of pegs_mods\n              \n                 \n                pegs_mod1\n                pegs_mod2\n                pegs_mod3\n                pegs_mod4\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  flipper_length\n                  0.712***\n                  1.701***\n                  0.573***\n                  2.145***\n                \n                \n                  speciesChinstrap\n                  -0.256***\n                  -0.493**\n                  -0.188+\n                  -1.256***\n                \n                \n                  speciesGentoo\n                  0.355**\n                  -1.419***\n                  0.186\n                  -1.489***\n                \n                \n                  flipper_length × speciesChinstrap\n                  \n                  \n                  0.033\n                  \n                \n                \n                  flipper_length × speciesGentoo\n                  \n                  \n                  0.377**\n                  \n                \n                \n                  speciesChinstrap × flipper_length\n                  \n                  \n                  \n                  -1.151**\n                \n                \n                  speciesGentoo × flipper_length\n                  \n                  \n                  \n                  -0.794*\n                \n                \n                  Num.Obs.\n                  333\n                  333\n                  333\n                  333\n                \n                \n                  R2\n                  0.787\n                  0.564\n                  0.794\n                  0.543\n                \n                \n                  R2 Adj.\n                  0.785\n                  0.560\n                  0.791\n                  0.536\n                \n                \n                  AIC\n                  441.7\n                  680.0\n                  435.0\n                  699.7\n                \n                \n                  BIC\n                  460.7\n                  699.1\n                  461.6\n                  726.4\n                \n                \n                  Log.Lik.\n                  -215.845\n                  \n                  -210.488\n                  \n                \n                \n                  F\n                  405.281\n                  \n                  251.731\n                  \n                \n                \n                  RMSE\n                  0.46\n                  0.66\n                  0.46\n                  0.68\n                \n        \n      \n    \n\n\n\nShow Code\n# plot models\n(\n    modelplot(pegs_mods, coef_omit = \"Intercept\") +\n    scale_color_iterm() +\n    labs(x = \"Estimate\", title = \"Fig 2.7: Summary of pegs_mods\") + \n    theme_light()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n我们前面已经得到了两个显著模型，pegs_mod3, pegs_mod4，现在需要讨论如何检测其每个组之间的回归系数的差异性，即回答“它们是同质的还是异质的？”。我们需要借助另一个神包 marginaleffects 进行一些计算，算法比较复杂，不用考虑细节，一通运算后得到数值结果，将结果导入 ggplot 并绘制四张子图。其中第一行是 pegs_mod3 的检测结果，左侧绘制总体异质性对比，即 CATE vs ATE 的对比，它反应的是某个组的效应相对于总体效应的差异性；右侧绘制组间异质性对比，即 CATE(a) vs CATE(b) vs CATE(c)，它反应的是两两成对的分组之间的差异性。第二行是 pegs_mod4 的检测结果。简要分析如下。\npegs_mod3 的 species 三个分类 (Adelie, Chinstrap, Gentoo) 相对于零效应都具有显著差异，但是 Gentoo 的系数明显高于 Adelie, Chinstrap，并且 Adelie, Chinstrap 的系数很接近，我们可以大胆推测它们二者之间的组间异质性应该是不显著的。这个推测在右侧子图得到了证实，它显示 Gentoo vs Adelie, Gentoo vs Chinstrap 两组的组间异质性显著，也就是说，这两个配对组之间的回归系数存在较大差异性；但是 Adelie vs Chinstrap 这组的组间异质性 \\(pv = 0.787\\) 不显著，也就是说，这两个配对组之间的回归系数不存在较大差异性。\npegs_mod4 的 species 三个分类的单独的异质性依然显著，但是系数的排序发生了变化，Adelie 的系数最高，Gentoo 的系数最低，而 Chinstrap 的系数居中；并且 Chinstrap, Gentoo 的系数很接近，我们也可以大胆推测它们之间的组间异质性很有可能是不显著的，即它们之间可能是同质性的。右侧的子图再次证明我们的推测，Chinstrap, Adelie 的异质性结果显著，Gentoo, Adelie 的异质性结果显著，但是 Gentoo, Chinstrap 的异质性结果不显著 \\(pv = 0.074\\)，如果它的数值再接近 \\(pv = 0.05\\)，我们或许可以谨慎地认为是“边缘显著”；但本案例它依然比较大，不应当认为是边缘显著，而应当直接认为是不显著。\n综上所述，我们在原有的 OLS-LM 和 IV-LM 当中分别加入交互项，我们不仅观察到了平均效应 ATE 更观察到了条件平均效应 CATE，并且进一步讨论了如何通过 CATE 的两两配对图形来辨析异质性；从数值层面和图表层面全方面地证明，有交互项的模型相比无交互项的模型，在异质性的分析流程当中具有更高的实证价值。这个结论提醒我们，如果条件允许，我们应当尽量构建带有交互项的回归模型，而尽量避免单纯的简单回归模型，即 cat_var * num_var 模型优先于 cat_var + num_var 模型。而至于更深度的“如何解决异质性”的问题已经超过当前的框架，我们暂且不论，继续踩着小坦克的油门突突突往纵深推进，冲鸭。\n\n\nShow Code\n# calculate groupwise heterogeneity of pegs_mod3\npegs_mod3_group_hete = (\n    slopes(\n        model = pegs_mod3, \n        variables = \"flipper_length\", \n        by = \"species\",\n        hypothesis = 0,\n        vcov = vcovHC(pegs_mod3, type = \"HC1\")\n    ) %&gt;% \n    tidy() %&gt;% \n    rename(\n        pv = p.value, \n        conf_lo = conf.low, \n        conf_hi = conf.high\n    ) %&gt;%\n    select(term, species, pv, estimate, conf_lo, conf_hi)\n)\n\n\n# plot groupwise heterogeneity of pegs_mod3\nsub1 = (\n    pegs_mod3_group_hete %&gt;%\n    ggplot(aes(x = species, y = estimate, color = species)) +\n    geom_errorbar(\n        aes(ymin = conf_lo, ymax = conf_hi), linewidth = 1, width = 0.2\n    ) +\n    geom_label(aes(label = scales::pvalue(pv, add_p = TRUE))) +\n    scale_color_iterm() +\n    labs(\n        x = \"\", y = \"Estimate\", title = \"Groupwise Test of pegs_mod3\"\n    ) +\n    theme_light() +\n    theme(\n        legend.position = \"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1)\n    )\n)\n\n\n# calculate pairwise heterogeneity of pegs_mod3\npegs_mod3_pair_hete = (\n    slopes(\n        model = pegs_mod3, \n        variables = \"flipper_length\", \n        by = \"species\",\n        hypothesis = difference ~ pairwise,\n        vcov = vcovHC(pegs_mod3, type = \"HC1\")\n    ) %&gt;% \n    tidy() %&gt;% \n    rename(\n        pairs = hypothesis,\n        pv = p.value, \n        conf_lo = conf.low, \n        conf_hi = conf.high\n    ) %&gt;%\n    select(pairs, pv, estimate, conf_lo, conf_hi)\n)\n\n\n# plot pairwise heterogeneity of pegs_mod3\nsub2 = (\n    pegs_mod3_pair_hete %&gt;% \n    ggplot(aes(x = pairs, y = estimate, color = pairs)) +\n    geom_errorbar(\n        aes(ymin = conf_lo, ymax = conf_hi), linewidth = 1, width = 0.2\n    ) +\n    geom_label(aes(label = scales::pvalue(pv, add_p = TRUE))) +\n    scale_color_iterm() +\n    labs(\n        x = \"\", y = \"Estimate\", title = \"Pairwise Test of pegs_mod3\"\n    ) +\n    theme_light() +\n    theme(\n        legend.position = \"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1)\n    )\n)\n\n\n# calculate zero heterogeneity of pegs_mod4\npegs_mod4_group_hete = (\n    slopes(\n        model = pegs_mod4, \n        variables = \"flipper_length\", \n        by = \"species\",\n        hypothesis = 0,\n        vcov = vcovHC(pegs_mod4, type = \"HC1\")\n    ) %&gt;% \n    tidy() %&gt;% \n    rename(\n        pv = p.value, \n        conf_lo = conf.low, \n        conf_hi = conf.high\n    ) %&gt;%\n    select(term, species, pv, estimate, conf_lo, conf_hi)\n)\n\n\n# plot group heterogeneity of pegs_mod4\nsub3 = (\n    pegs_mod4_group_hete %&gt;%\n    ggplot(aes(x = species, y = estimate, color = species)) +\n    geom_errorbar(\n        aes(ymin = conf_lo, ymax = conf_hi), linewidth = 1, width = 0.2\n    ) +\n    geom_label(aes(label = scales::pvalue(pv, add_p = TRUE))) +\n    scale_color_iterm() +\n    labs(\n        x = \"\", y = \"Estimate\", title = \"Group Test of pegs_mod4\"\n    ) +\n    theme_light() +\n    theme(\n        legend.position = \"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1)\n    )\n)\n\n\n# calculate pairwise heterogeneity of pegs_mod4\npegs_mod4_pair_hete = (\n    slopes(\n        model = pegs_mod4, \n        variables = \"flipper_length\", \n        by = \"species\",\n        hypothesis = difference ~ pairwise,\n        vcov = vcovHC(pegs_mod4, type = \"HC1\")\n    ) %&gt;% \n    tidy() %&gt;% \n    rename(\n        pairs = hypothesis,\n        pv = p.value, \n        conf_lo = conf.low, \n        conf_hi = conf.high\n    ) %&gt;%\n    select(pairs, pv, estimate, conf_lo, conf_hi)\n)\n\n\n# plot pairwise heterogeneity of pegs_mod4\nsub4 = (\n    pegs_mod4_pair_hete %&gt;% \n    ggplot(aes(x = pairs, y = estimate, color = pairs)) +\n    geom_errorbar(\n        aes(ymin = conf_lo, ymax = conf_hi), linewidth = 1, width = 0.2\n    ) +\n    geom_label(aes(label = scales::pvalue(pv, add_p = TRUE))) +\n    scale_color_iterm() +\n    labs(\n        x = \"\", y = \"Estimate\", title = \"Pairwise Test of pegs_mod4\"\n    ) +\n    theme_light() +\n    theme(\n        legend.position = \"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1)\n    )\n)\n\n\n# combine into single figure\n(\n    wrap_plots(sub1, sub2, sub3, sub4) + \n    plot_layout(ncol = 2, nrow = 2) + \n    plot_annotation(title = \"Fig 2.8: Heterogeneity Test\")\n)"
  },
  {
    "objectID": "posts/econ/econ.html#sec-lm-robustness",
    "href": "posts/econ/econ.html#sec-lm-robustness",
    "title": "Practical Econometrics with R",
    "section": "",
    "text": "残差检验是模型的最后一步，为了更好理解它，我们需要解释两个重要的基础概念。前面已经提过“预测矩阵”，但是没有进行精确界定，所谓的“预测矩阵 Prediction Matrix” \\(\\mathbf{\\widehat{Y}}\\) 就是当我们计算得到了回归参数矩阵 \\(\\mathbf{\\widehat{B}}\\) 之后，将这个参数矩阵与目标矩阵 \\(\\mathbf{X}\\) 相乘，即可得到基于模型的预测值。\n\\[\n\\mathbf{\\widehat{Y}} = \\mathbf{X} \\mathbf{\\widehat{B}}\n\\]\n那么根据常识我们知道，任何预测都不可能完全等于实际数据，因此预测值跟观测值必然有一定差异，而这部分差异就是模型的“残差 Residuals”。在计量经济学中，残差并不仅仅代表预测的“好坏”，它更重要的意义是衡量模型未能捕捉到的随机变动项；残差分析的目的就是检查这些未被解释的部分是否符合统计假设，从而评估模型的有效性。\n\\[\n\\mathbf{e} = \\mathbf{Y} - \\mathbf{\\hat{Y}}\n\\]\n先简单回归我们现有的四个模型：\npegs_mod1: lm(\n    formula = body_mass ~ flipper_length + species, \n    data = pegs\n)\n\n\npegs_mod2: ivreg(\n    formula = body_mass ~ flipper_length | species | bill_depth + bill_length, \n    data = pegs\n)\n\n\npegs_mod3: lm(\n    formula = body_mass ~ flipper_length * species, \n    data = pegs\n)\n\n\npegs_mod4: ivreg(\n    formula = body_mass ~ species | species * flipper_length  | species * (bill_depth + bill_length),\n    data = pegs\n)\n为了更清晰的观察每个模型的综合表现，我们分别计算每一个模型对应的残差和预测值，产生 8 个新的变量；我们将他们合并到原始数据组 pegs 当中，最后的表格结构如下。本小节的残差分析将基于这个扩展的数据组 pegs 进行，而不是原有的 pegs 进行。\n\n\nShow Code\n# calculate residuals and predictions\npegs = (\n    pegs %&gt;% \n    mutate(\n        res_mod1 = residuals(pegs_mod1), fit_mod1 = fitted(pegs_mod1),\n        res_mod2 = residuals(pegs_mod2), fit_mod2 = fitted(pegs_mod2),\n        res_mod3 = residuals(pegs_mod3), fit_mod3 = fitted(pegs_mod3),\n        res_mod4 = residuals(pegs_mod4), fit_mod4 = fitted(pegs_mod4)\n    )\n)\n\n\n# show dataframe\n(\n    pegs %&gt;% \n    slice_sample(n = 10) %&gt;% \n    datasummary_df(title = \"Appended Sample of Penguins\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Appended Sample of Penguins\n              \n                species\n                island\n                sex\n                bill_length\n                bill_depth\n                flipper_length\n                body_mass\n                res_mod1\n                fit_mod1\n                res_mod2\n                fit_mod2\n                res_mod3\n                fit_mod3\n                res_mod4\n                fit_mod4\n              \n        \n        \n        \n                \n                  Chinstrap\n                  Dream\n                  female\n                  0.47\n                  0.38\n                  -0.63\n                  -0.88\n                  -0.10\n                  -0.78\n                  0.09\n                  -0.96\n                  -0.13\n                  -0.75\n                  -0.02\n                  -0.85\n                \n                \n                  Gentoo\n                  Biscoe\n                  female\n                  0.45\n                  -1.09\n                  1.07\n                  0.62\n                  -0.43\n                  1.05\n                  -0.39\n                  1.01\n                  -0.41\n                  1.03\n                  -0.37\n                  0.99\n                \n                \n                  Gentoo\n                  Biscoe\n                  male\n                  1.00\n                  -0.68\n                  1.07\n                  0.90\n                  -0.15\n                  1.05\n                  -0.11\n                  1.01\n                  -0.13\n                  1.03\n                  -0.09\n                  0.99\n                \n                \n                  Gentoo\n                  Biscoe\n                  male\n                  1.11\n                  -0.94\n                  1.36\n                  1.68\n                  0.43\n                  1.25\n                  0.18\n                  1.50\n                  0.38\n                  1.30\n                  0.30\n                  1.38\n                \n                \n                  Chinstrap\n                  Dream\n                  male\n                  1.46\n                  1.19\n                  0.36\n                  -0.31\n                  -0.24\n                  -0.07\n                  -1.04\n                  0.73\n                  -0.17\n                  -0.15\n                  -0.45\n                  0.14\n                \n                \n                  Adelie\n                  Torgersen\n                  female\n                  -0.79\n                  0.02\n                  -0.35\n                  -0.81\n                  -0.49\n                  -0.32\n                  -0.83\n                  0.01\n                  -0.44\n                  -0.38\n                  -1.09\n                  0.28\n                \n                \n                  Gentoo\n                  Biscoe\n                  female\n                  0.34\n                  -1.49\n                  1.29\n                  0.62\n                  -0.58\n                  1.20\n                  -0.76\n                  1.38\n                  -0.61\n                  1.23\n                  -0.66\n                  1.28\n                \n                \n                  Adelie\n                  Dream\n                  female\n                  -1.36\n                  0.43\n                  -1.35\n                  -1.31\n                  -0.28\n                  -1.03\n                  0.37\n                  -1.68\n                  -0.36\n                  -0.95\n                  0.54\n                  -1.85\n                \n                \n                  Adelie\n                  Dream\n                  male\n                  -0.44\n                  0.68\n                  0.01\n                  -0.25\n                  -0.19\n                  -0.07\n                  -0.87\n                  0.62\n                  -0.08\n                  -0.17\n                  -1.30\n                  1.04\n                \n                \n                  Gentoo\n                  Biscoe\n                  male\n                  0.23\n                  -0.38\n                  1.57\n                  2.18\n                  0.78\n                  1.40\n                  0.32\n                  1.86\n                  0.68\n                  1.50\n                  0.52\n                  1.66\n                \n        \n      \n    \n\n\n\n基于这个扩展的数据组 pegs，我们对残差进行以下三项检验流程，分别是：\n\n正态性 Normality: 残差的分布应当符合正态分布规律。\n多重共线性 Multi-colinearity: 多个自变量之间应该不存在显著的线性相关。\n异方差性 Heteroskedasticity: 残差的方差在多个自变量之间应该保持基本稳定。\n\n我们希望检验的结果是，残差符合正态分布、变量不存在多重共线性、残差没有异方差性。这样就可以基本推定我们的模型具备较高的严谨性、稳健性、可靠性；我们现在逐个简要介绍这三项检验。\n\n\n\n我们现在讨论残差分析的第一个方面：正态性，即残差的分布应该呈现基本的正态分布特征。一般通过 Shapiro-Wilk 进行数值检测和 QQ Plot 进行图形观察，这部分检验的理论已经非常成熟，经典文献可以参考 (Shapiro1965?), (Royston1982?), (Royston1995?)。如果 sw_test() 结果显著，则残差不符合正态分布；反之则符合。而 QQ Plot 能直观地展示残差分布与理论正态分布的吻合程度，它揭示数据点是否紧密地沿着一条直线分布，有助于判断残差是否存在偏离正态分布。但是根据中心极限定理 (Central Limit Theorem)，当样本数量足够大 \\(n &gt; 100\\) 的情况下，即使正态性检验显著，只要 QQ 图没有显示出极端异常的偏离，我们通常也可以接受这个结果，而不必机械地纠结于它在数值上是否“显著”。\n回到本案，我们计算 pegs 四个模型的残差 res_mod1, res_mod2, res_mod3, res_mod4 并进行正态性检验，数值结果显示前两个模型 pegs_mod1 和 pegs_mod2 满足正态性要求，但是后两个模型 pegs_mod3 和 pegs_mod4 不满足；而图形结果则提示模型没有特别明显的偏离值，应该是符合正态分布。如前文所述，由于我们的样本量 \\(N=333\\) 足够大，虽然 sw_test() 呈显著结果，但是这种程度的非正态性并不会对我们的最终结论构成严重威胁；我们将此检验结果作为模型稳健性诊断的一个信号，然后踩着小坦克油门继续往下推。\n\n\nShow Code\n# test normality of residuals\n(\n    bind_rows(\n        shapiro.test(pegs$res_mod1) %&gt;% tidy(),\n        shapiro.test(pegs$res_mod2) %&gt;% tidy(),\n        shapiro.test(pegs$res_mod3) %&gt;% tidy(),\n        shapiro.test(pegs$res_mod4) %&gt;% tidy()\n    ) %&gt;%\n    rename(pv = p.value) %&gt;%\n    mutate(model = c(\"pegs_mod1\", \"pegs_mod2\", \"pegs_mod3\", \"pegs_mod4\")) %&gt;%\n    select(model, statistic, pv, -method) %&gt;%\n    datasummary_df(title = \"Normality Test\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Normality Test\n              \n                model\n                statistic\n                pv\n              \n        \n        \n        \n                \n                  pegs_mod1\n                  0.99\n                  0.20\n                \n                \n                  pegs_mod2\n                  0.99\n                  0.09\n                \n                \n                  pegs_mod3\n                  0.99\n                  0.02\n                \n                \n                  pegs_mod4\n                  0.98\n                  0.00\n                \n        \n      \n    \n\n\n\n\n\nShow Code\n# plot QQ of mod1\nsub1 = (\n    pegs %&gt;%\n    ggplot(aes(sample = res_mod1, color = species)) +\n    stat_qq() +\n    stat_qq_line() +\n    scale_color_iterm() +\n    labs(x = \"Theoretical\", y = \"Sample\", title = \"pegs_mod1\") +\n    theme_light() +\n    facet_wrap(~ species)\n)\n\n\n# plot QQ of mod2\nsub2 = (\n    pegs %&gt;% \n    ggplot(aes(sample = res_mod2, color = species)) +\n    stat_qq() +\n    stat_qq_line() +\n    scale_color_iterm() +\n    labs(x = \"Theoretical\", y = \"Sample\", title = \"pegs_mod2\") +\n    theme_light() +\n    facet_wrap(~ species)\n)\n\n\n# plot QQ of mod3\nsub3 = (\n    pegs %&gt;% \n    ggplot(aes(sample = res_mod3, color = species)) +\n    stat_qq() +\n    stat_qq_line() +\n    scale_color_iterm() +\n    labs(x = \"Theoretical\", y = \"Sample\", title = \"pegs_mod3\") +\n    theme_light() +\n    facet_wrap(~ species)\n)\n\n\n# plot QQ of mod4\nsub4 = (\n    pegs %&gt;% \n    ggplot(aes(sample = res_mod4, color = species)) +\n    stat_qq() +\n    stat_qq_line() +\n    scale_color_iterm() +\n    labs(x = \"Theoretical\", y = \"Sample\", title = \"pegs_mod4\") +\n    theme_light() +\n    facet_wrap(~ species)\n)\n\n\n# combine into single figure\n(\n    wrap_plots(sub1, sub2, sub3, sub4) + \n    plot_layout(ncol = 2, nrow = 2, guides = \"collect\") + \n    plot_annotation(title = \"Fig 2.9: Normality Test\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\n我们现在讨论残差分析的第二个话题：多重共线性，即某个自变量与另外的自变量之间存在高度相关性。在因果判断时，它的存在会让我们无法精确地辨识出每个自变量的独立影响，即结果 Y 是由因素 D1 所致，但是 D1 又跟 D2 高度线性相关，那说明 D2 也对结果 Y 产生了影响。这是我们不希望遇到的情况，各个自变量的系数估计值会变得不稳定、标准误增大，从而降低了我们对模型结果的信心和精确度。我们主要通过 vif() 函数检测共线性，这部分的经典文献来自于 (Belsley1980?), (Fox2016?)，算法得出的 GVIF 作为主要判断标准：\n\n如 GVIF &lt; 5，则自变量基本上不存在多重共线性。\n如 5 &lt; GVIF &lt; 10，则自变量具有一定共线性。\n如 10 &lt; GVIF，则自变量存在严重共线性。\n\n从模型结果来看，只有 pegs_mod1 不具有共线性，而其他三个模型都存在共线性，pegs_mod4 的最高共线性甚至冲破了 60。这个问题的主要原因是，我们的自变量当中包含了分类变量，而分别变量被当成哑变量，它与数值变量相乘，会产生很多交互数据，所以导致严重共线性。为了更好地解释模型，我们不再依赖 GVIF 而采信 adj_GVIF，其判断标准依然以 \\(\\text{adj\\_GVIF} = 10\\) 作为重度共线性的分界。另外，对于已经存在共线性的模型，有时候可以通过 scales() 函数进行处理，但如果共线性是由于自变量本身所引起的，那么即使进行了 scales() 也无法从根本上消除。我们在数据处理的第一步就执行了 scales()，这说明当前观察到的多重共线性是更深层次的结构性问题，而非简单的变量尺度差异所致，因此通过进一步的标准化也难以消除。\n回到本案当中，在我们最关心的变量 flipper_length:species 当中，pegs_mod3 具有较少的共线性，但是 pegs_mod4 具有一定的共线性；因此我们虽然可以继续使用 pegs_mod4 但必须谨慎地汇报结果，而不能过于乐观地解读。导致这种较高共线性的原因主要有两个方面：（1）模型交互项，pegs_mod3 和 pegs_mod4 都包含了交互项，交互项很容易与原始的主效应产生相关性。（2）样本量限制，总数据样本只有 3 百多行，这对于 pegs_mod4 这样一个包含了多重交互项的复杂工具变量模型而言，很容易出现高度相关性，导致了在复杂模型中难以避免的共线性问题。另外，需要注意的是，增加分类变量的维度并不会降低共线性，反而通常会因为引入更多的哑变量和交互项而增加多重共线性的风险，特别是在小样本量的情况下。\n\n\nShow Code\n# combine into a list\npegs_mods = list(\n    \"pegs_mod1\" = pegs_mod1,\n    \"pegs_mod2\" = pegs_mod2,\n    \"pegs_mod3\" = pegs_mod3,\n    \"pegs_mod4\" = pegs_mod4\n)\n\n\n# create UDF to re-run calculation\nget_gvif = function(model, model_name) {\n    vif(model) %&gt;% \n    as.data.frame() %&gt;% \n    rownames_to_column(var = \"terms\") %&gt;% \n    rename(\n        DF = Df, \n        adj_GVIF = `GVIF^(1/(2*Df))`\n    ) %&gt;%\n    mutate(\n        model = model_name,\n        adj_GVIF = (adj_GVIF)^2\n    ) %&gt;% \n    select(model, terms, DF, GVIF, adj_GVIF)\n}\n\n\n# get gvif results\npegs_gvif = map2(pegs_mods, names(pegs_mods), get_gvif) %&gt;% list_rbind()\n\n\n# show gvif table\npegs_gvif %&gt;% datasummary_df(title = \"Multi-colinearity Test\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        Multi-colinearity Test\n              \n                model\n                terms\n                DF\n                GVIF\n                adj_GVIF\n              \n        \n        \n        \n                \n                  pegs_mod1\n                  flipper_length\n                  1.00\n                  4.44\n                  4.44\n                \n                \n                  pegs_mod1\n                  species\n                  2.00\n                  4.44\n                  2.11\n                \n                \n                  pegs_mod2\n                  species\n                  2.00\n                  10.17\n                  3.19\n                \n                \n                  pegs_mod2\n                  flipper_length\n                  1.00\n                  10.17\n                  10.17\n                \n                \n                  pegs_mod3\n                  flipper_length\n                  1.00\n                  10.57\n                  10.57\n                \n                \n                  pegs_mod3\n                  species\n                  2.00\n                  12.36\n                  3.52\n                \n                \n                  pegs_mod3\n                  flipper_length:species\n                  2.00\n                  16.19\n                  4.02\n                \n                \n                  pegs_mod4\n                  species\n                  2.00\n                  76.89\n                  8.77\n                \n                \n                  pegs_mod4\n                  flipper_length\n                  1.00\n                  60.24\n                  60.24\n                \n                \n                  pegs_mod4\n                  species:flipper_length\n                  2.00\n                  88.05\n                  9.38\n                \n        \n      \n    \n\n\n\nShow Code\n# plot gvif chart\n(\n    pegs_gvif %&gt;% \n    ggplot(aes(x = terms, y = adj_GVIF, fill = model)) +\n    geom_col(position = \"dodge\") +\n    geom_hline(yintercept = 10, color = \"gray\", linetype = \"dashed\") +\n    scale_fill_iterm() +\n    labs(\n        x = \"\", y = \"Adjusted GVIF\", \n        title = \"Fig 2.10: Multi-colinearity Test\"\n    ) +\n    theme_light()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n我们现在讨论残差分析的最后一个方面：异方差性，即残差在整个样本当中的稳定性。\n\n异方差性 Heteroskedasticity: 残差的方差随自变量变化而存在显著差异，即残差的变动幅度不是恒定的。\n同方差性 Homoskedasticity: 残差的方差随自变量变化而没有显著差异，即残差的变动幅度是恒定的。\n\n在线性模型中，我们总是希望模型满足同方差性，而拒绝异方差性；可以简单的认为“异方差性”总是“不好的”、“同方差性”总是“好的”。假如我们一个模型当中有很多个残差，它们被划分为多个小组，选择任意其中的第 \\(i, \\ j\\) 两个小组，那么它们各自的方差应该非常接近样本的平均方差，即可得到异质性检验的重要标准：\n\\[\n\\sigma_i^2 = \\sigma_j^2 = \\sigma^2, \\quad \\forall i \\neq j\n\\]\n注意，\\(i,\\ j\\) 一般都是分类变量，如汽车品牌、车辆颜色、经销商名称、注册城市等等；但这并不是排斥数值变量，某些特定情况的数值变量也可以作为分组依据，但是需要谨慎对待；绝大多数经济学的实证分析依然以分类变量的维度来讨论异方差性。回到本案中，我们有四个模型，但是其中 pegs_mod1, pegs_mod3 是没有交互项的模型，而 pegs_mod2, pegs_mod4 是具有交互项的模型。因此，我们必须根据模型性质分别处理，用现成的 bptest() 检测 OLS-LM 的同方差性，再手动编写一个半自动的 phtest() 检测 IV-LM 的同方差性，有关经典文献可以参考 (Breusch1979?), (Pagan1983?), (Cameron2005?)。\n我们继续略过算法细节，跑出结果显示 pegs_mod1 显著，即模型中存在异方差；pegs_mod2 处于边缘显著，我们可以从显著或者不显著的视角同时开展分析，但是必须非常小心；pegs_mod3, pegs_mod4 都显著，即模型都存在异方差。但是需要特别注意，因为我们的模型中包含了分类变量的交互项，此时异方差性是较为常见的现象。而至于异方差在模型当中实际反应的深层问题或意义，已经牵扯到它与“研究设计 Research Design”的复杂关联，通常需要对该研究设计具有更深度的掌握和理解；我们暂且不论这些复杂的“研究设计”问题，重点在于理解异方差检验的框架，如果能掌握到这个程度，对于本硕博的论文写作已经很足够了。\n\n\nShow Code\n##### test heterosckedasticity for OLS models\n(\n    bind_rows(\n        bptest(pegs_mod1) %&gt;% tidy() %&gt;% mutate(model = \"pegs_mod1\"),\n        bptest(pegs_mod3) %&gt;% tidy() %&gt;% mutate(model = \"pegs_mod3\")\n    ) %&gt;% \n    rename(pv = p.value) %&gt;%\n    select(model, statistic, pv) %&gt;% \n    datasummary_df(title = \"Heterosckedasticity Test of OLS Models\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Heterosckedasticity Test of OLS Models\n              \n                model\n                statistic\n                pv\n              \n        \n        \n        \n                \n                  pegs_mod1\n                  8.77\n                  0.03\n                \n                \n                  pegs_mod3\n                  10.95\n                  0.05\n                \n        \n      \n    \n\n\n\nShow Code\n##### test heterosckedasticity for IV models\n# create UDF to run Pagan-Hall test\nphtest = function(iv_model) {\n\n    # calculate Pagan-Hall test statistic\n    res_squared = I(residuals(iv_model)^2)\n    instruments = model.matrix(iv_model, component = \"instruments\")\n    aux_model = lm(res_squared ~ instruments - 1)\n    n = nobs(iv_model)\n    r2 = summary(aux_model)$r.squared\n    test_stat = n * r2\n    df = ncol(instruments) - 1\n    pv_chi = pchisq(test_stat, df = df, lower.tail = FALSE)\n\n    # combine results\n    results = tibble(\n        statistic = test_stat,\n        df = df,\n        pv = pv_chi\n    )\n\n    # show results\n    return(results)\n}\n\n\n# run Pagan-Hall test\n(\n    bind_rows(\n        phtest(pegs_mod2) %&gt;% mutate(model = \"pegs_mod2\"),\n        phtest(pegs_mod4) %&gt;% mutate(model = \"pegs_mod4\")\n    ) %&gt;%\n    select(model, statistic, pv) %&gt;% \n    datasummary_df(title = \"Heterosckedasticity Test of IV Models\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Heterosckedasticity Test of IV Models\n              \n                model\n                statistic\n                pv\n              \n        \n        \n        \n                \n                  pegs_mod2\n                  111.56\n                  0.00\n                \n                \n                  pegs_mod4\n                  109.81\n                  0.00\n                \n        \n      \n    \n\n\n\n如果模型具有异方差，我们无法通过简单的模型变换以消除异方差性，最常见的处理方法是使用异方差稳健标准误 (Robust Standard Error)。虽然听起来它很强大，但它实际上并不能消除异方差性。即，它不能调整回归系数 \\(\\hat{\\beta_i}\\) 本身，它只能适当地调整回归系数的标准误。这意味着，回归系数估计保持不变，但其对应的标准误会被重新计算，以纠正异方差造成的偏差；这个调整后的标准误会用于后续的假设检验和置信区间构建，如一个 95% 的置信区间的计算方法是：\\(\\hat{\\beta_i} \\pm 1.96 * \\text{SE}(\\hat{\\beta_i})\\)。这个区间的结果，就是 RSE 所返回的经过调整后的结果；更多深入理论可以参考经典文献： (White1980?), (Zeileis2004?), (CribariNeto2011?)。\n换个类比，监狱的劳动改造无法将已经判刑的罪犯直接改判为无罪，被判刑 10 年的犯人依然需要服刑 10 年，但是监狱会对犯人进行劳动改造和就业教育，比如法律学习、电气维修、农业种植、生活服务等等，罪犯依然享受与正常人类似的基本人权。一旦他刑满释放，他的劳动技能可以为其提供基本的生活保障，而不会让他出狱后完全脱离社会节奏；异方差稳健标准误并不改变模型的基本设定或系数估计，而是通过修正标准误，确保我们对系数的假设检验和置信区间依然是有效的和可信的。经过劳动改造的罪犯出狱后应当被“合理预期”是正常人，而不应当被歧视为罪犯；经过稳健控制的实证结果应当被“合理预期”是符合要求的结果汇报在结论中。因此，本案例的四个模型对应的结果表格如下。\n\n\nShow Code\n# show robust table\nmodelsummary(\n    model = pegs_mods, \n    stars = TRUE,\n    statistic = NULL,\n    vcov = \"robust\",\n    title = \"Robust SE of pegs_mods\",\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Robust SE of pegs_mods\n              \n                 \n                pegs_mod1\n                pegs_mod2\n                pegs_mod3\n                pegs_mod4\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  -0.070\n                  0.608***\n                  -0.177*\n                  1.031***\n                \n                \n                  flipper_length\n                  0.712***\n                  1.701***\n                  0.573***\n                  2.145***\n                \n                \n                  speciesChinstrap\n                  -0.256***\n                  -0.493*\n                  -0.188+\n                  -1.256***\n                \n                \n                  speciesGentoo\n                  0.355**\n                  -1.419***\n                  0.186\n                  -1.489***\n                \n                \n                  flipper_length × speciesChinstrap\n                  \n                  \n                  0.033\n                  \n                \n                \n                  flipper_length × speciesGentoo\n                  \n                  \n                  0.377***\n                  \n                \n                \n                  speciesChinstrap × flipper_length\n                  \n                  \n                  \n                  -1.151**\n                \n                \n                  speciesGentoo × flipper_length\n                  \n                  \n                  \n                  -0.794*\n                \n                \n                  Num.Obs.\n                  333\n                  333\n                  333\n                  333\n                \n                \n                  R2\n                  0.787\n                  0.564\n                  0.794\n                  0.543\n                \n                \n                  R2 Adj.\n                  0.785\n                  0.560\n                  0.791\n                  0.536\n                \n                \n                  AIC\n                  441.7\n                  680.0\n                  435.0\n                  699.7\n                \n                \n                  BIC\n                  460.7\n                  699.1\n                  461.6\n                  726.4\n                \n                \n                  Log.Lik.\n                  -215.845\n                  \n                  -210.488\n                  \n                \n                \n                  F\n                  464.091\n                  \n                  318.591\n                  \n                \n                \n                  RMSE\n                  0.46\n                  0.66\n                  0.46\n                  0.68\n                \n                \n                  Std.Errors\n                  HC3\n                  HC3\n                  HC3\n                  HC3\n                \n        \n      \n    \n\n\n\nShow Code\n# plot robust models\n(\n    modelplot(model = pegs_mods, vcov = \"robust\") +\n    scale_color_iterm() +\n    labs(\n        x = \"Coefficient and 95% CI\", y = \"\", \n        title = \"Fig 2.11: Summary of Robust Models\"\n    ) +\n    theme_light()\n)"
  },
  {
    "objectID": "posts/econ/econ.html#conclusion",
    "href": "posts/econ/econ.html#conclusion",
    "title": "Practical Econometrics with R",
    "section": "",
    "text": "至此，我们已经跑完了一个 Linear Model 最主要的环节，有必要进行一些小结；我们开着小坦克突突突往前冲，略过了很多细节，是为了集中大家的注意力在最核心的原理上。希望同学们在学习过程中，不要被花里胡哨的数学证明所迷惑，始终关注在基础原理和逻辑思维上；抓基层、打基础、苦练基本功。\n\nModeling\n\n对原始数据进行必要的清洗，形成干净数据 tidy data；\n严肃的文章应当保留完整的数据清理代码，而不能只提供一个数据结果；\n明确研究设计的主体架构，初步梳理可能的因果关系传导机制和影响强度；\n建立简单的多元回归模型 lm(Y ~ X) 观察基准回归结果；\n\nExogeneity\n\n将简单模型扩展至工具变量模型 ivreg(Y ~ X_exo | X_edo | Z)；\n根据 2SLS 开展 Weak Instrument, Wu-Hausman, Sargan 检验；\n如果不存在内生性，则继续使用 OLS-LM；如果存在内生性，则考虑使用 IV-LM；\n工具变量需要耐心、运气、想象力，不要着急，在失败中寻找正确的路径；\n\nHeterogeneity\n\n在原有模型中增加交互项目，将 “+” 改成 “*“；\n若是 OLS-LM: lm(Y ~ X1 * X2)；\n若是 IV-LM: ivreg(Y ~ X_exo | X_exo * X_edo | X_exo * Z)；\n注意观察 Groupwise VS Pairwise 差异性，有时候会爆大装备；\n\nResiduals\n\n正态性，使用 shapiro.test() 检验，同时参考 QQ 图形；\n多重共线性，使用 vif() 检验，主要参考 adj_GVIF 的结果；\n异方差性，使用 bptest() 检验 OLS-LM，使用 phtest() 检验 IV-LM；\n模型异方差无法根治，可以通过 vcov = \"robust\"() 进行稳健性调整；"
  },
  {
    "objectID": "posts/econ/econ.html#sec-did-overview",
    "href": "posts/econ/econ.html#sec-did-overview",
    "title": "Practical Econometrics with R",
    "section": "2.1 Overview",
    "text": "2.1 Overview\n\n2.1.1 Description\n双重差分 (DID) 是当前经管领域用于评估政策或事件因果效应的核心计量工具，它对于政策分析有非常重要的意义。在深入讨论前，我们先举一个简单的例子，以感性认识带动理性认识。\n某大学经济学院希望评估“使用英文原版教材授课”的效果。我们选择两个非人为干预的学生班级，每班大约30人，具有基本相似的高考背景和学术能力，上学期期末考试的平均成绩分别是 67 和 69分。我们让其中一个班作为实验组 treat 使用英文原版教材授课，另一个班级作为控制组 control 继续沿用中国教材授课；假设两位老师交叉上课、上课水平、上课态度、上课时长都基本一致。一个学期后，进行同样试卷的考试，统计分数发现，英文教材授课的学生期末考试成绩从上学期的 67 分提高到 77 分，提高 \\(\\Delta_1 = 10\\)；中文教材授课的学生的期末考试成绩从上学期的 69 分提高到 72 分，提高 \\(\\Delta_2 = 3\\)。通过比较实验组和控制组在上学期和本学期的成绩变化，我们发现实验组在政策后期的成绩额外提高了 \\(\\Delta_1 - \\Delta_2 = 7\\)，这说明“使用英文原版教材授课”确实提高了同学的考试成绩。文字比较绕，我们可以用一个表格，非常清晰地表示如下：\n\nSimple DID Example\n\n\n\nlast-term\nthis-term\nincrease\n\n\n\n\nEng-books\n67\n77\n10\n\n\nChi-books\n69\n72\n3\n\n\n\n这个简单的案例最大程度地描述了 DID 的原理，我们现在从特例推广到一般。一个标准的 DID 模型的演化逻辑是由一项“政策冲击 Policy Shock”无差别地、从外部地、突然地影响了观测对象；其中受到政策冲击的是“实验组 Treat”，未受到政策冲击的是“控制组 Control”；政策发生之前的时间状态是 Pre-Policy，政策发生后的时间状态是 Post-Policy。那么我们通过观察实验组和控制组在政策前和政策后的差异程度，来分析政策的净效应；即如下表格所示：\n\nBasic DID Framework\n\n\n\n\n\n\n\n\ngroup/time\npre-policy\npost-policy\ndifference\n\n\n\n\ntreat\n\\(Y_{tre, pre}\\)\n\\(Y_{tre, pos}\\)\n\\(Y_{tre, pos} - Y_{tre, pre}\\)\n\n\ncontrol\n\\(Y_{con, pre}\\)\n\\(Y_{con, pos}\\)\n\\(Y_{con, pos} - Y_{con, pre}\\)\n\n\n\n从这个表格可以推导 DID 估计量，即，在排除掉控制组的时间趋势后，政策冲击对实验组的净效应，也称“对处理组的平均处理效应 Average Treatment Effect on the Treated, ATT”。专业名词太绕口了，它的原理很简单，就是先算实验组前后的变化 \\(\\Delta_{\\text{tre}}\\)，再算控制组前后的变化 \\(\\Delta_{\\text{con}}\\) ，最后用前者减去后者 \\(\\Delta_{\\text{tre}} - \\Delta_{\\text{con}}\\)，一共两次差分，所以叫“双重差分 Difference-in-Differences”，就是这么简单粗暴。可以表示如下：\n\\[\n\\text{DID} = (Y_{tre, pos} - Y_{tre, pre}) - (Y_{con, pos} - Y_{con, pre})\n\\]\n\n\n2.1.2 Advantages\n我们已经在前面详细讨论过 LM，既然它已经很完整丰富，为什么还需要另外用 DID 模型？换言之，DID 相对于传统的 LM 的必要性和先进性在哪里？是否存在一些问题是 LM 无法解决或者解决效果并不尽如人意？以及 DID 是如何解决这些困难？为了从更深度的哲学高度掌握 DID 的核心价值，我们有必要分析传统 LM 存在的三个严重局限性。\n\n选择偏误 Selection Bias: 实验组和控制组的划分并非完全随机，可能存在较大的隐形的先天差异，从而导致估计量偏误。比如，实验组的学生可能本身学习能力更强、学习态度更投入、学习资源更充沛等原因，从而导致实验组同学的成绩提升更多，而不是因为教材的原因导致的成绩增加。\n时间趋势偏误 Time Trend Bias: 在政策实施期间，影响所有个体的外部环境可能发生较大的隐形的变化，从而混淆政策效应的估计。比如，在政策实施期间，学校可能整体提升了教学质量、增加了课外辅导资源、降低了考试难度，从而会影响所有学生的成绩，最终导致成绩提升的原因并非完全是教材的变化。\n遗漏交互项偏误 Omited Interaction Bias: 即使将实验组组、控制组、政策前、政策后的所有变量都放入模型中，如果没有交互项，传统的线性模型只能估计组别之间的平均差异和一个共同的时间趋势；它无法捕捉到政策对实验组产生的额外的、区别于控制组自身的影响，即，排除了组间固有差异和共同时间趋势后的净效应。\n\nDID 模型正好完美解决了这三个局限性。首先它通过组内差分消除了不随时间变化的组间固有差异，从而解决了选择偏误；其次它通过组间差分剥离了影响所有个体的共同时间趋势，从而解决了时间趋势偏误；最后它通过交互项系数精确量化了政策冲击排除掉所有混淆因素后对处理组的净效应，从而解决了遗漏交互项偏误。综上所述，DID 自从 2010 年以来在经济管理领域中变得愈发流行，并迅速成为评估政策因果效应的“扛把子”；相较于简单的线性回归模型，它能更有效地识别因果关系；相比于复杂的机器学习模型，它更容易理解和解释。这些因素使得 DID 在理论和实践上都具有很强的生命力，我们将简要介绍它的重要假设、数学公式、算法原理，然后用一个真实案例进行实战。\n\n\n2.1.3 PTA\n在前述的 DID 模型中，它隐含了一个重要的假设，即“平行趋势假设 Parallel Trend Assumption”，我们故意把它放在本小节的最后，是为了避免过多的理论在前部分对同学的感性认识造成较大压力，现在是比较合适的时机。所谓“平行趋势假设 PTA” 是指，在无政策干预下，实验组与控制组的结果变量随时间的变化路径应基本平行；比如实验组变动了 10 个单位，那么控制组也应该是变化 10 个单位左右，控制组的变化既不能严重过大（比如 50 个单位）、过小（比如 0.1 个单位）、更不可能反向变化。用数学表示即为，实验组和控制组的斜率需要保持基本一致，即：\n\\[\n\\text{k}_{\\text{tre}} \\approx \\text{k}_{\\text{con}}\n\\]\n那么很显然，以下情况就是典型的违反了 PTA 的案例：\n\n实验组和控制组的增幅相差太大，明显超过了合理的、平行的、可接受的范围。\n实验组和控制组的增幅完全相反，即一个积极上升而另一个消极下降。\n\n可惜的是，我们很难通过某种算法精确地验证 PTA，它并不是一个可以被直接“证明或证伪”的假设，而更依赖多期趋势图、安慰剂检验、事件研究法等方法，以图表和数值联合检验，这既是政策分析的挑战也是魅力。一方面，我们终于不必被 \\(p&lt;0.05\\) 的机械标准所束缚，可以在尊重数据和逻辑的前提下更好地发挥学术洞察力与创造力；另一方面，我们失去了最权威的甄别标准，一项处于边缘显著状态 \\(0.05 &lt; p &lt; 0.06\\) 的政策具有了很多种不同的解释。而且在多种解释之间，学者们往往很难达成共识，因此我们更看重实实在在的“经济性、合理性、稳健性”，它们在社会生活中对于人民群众产生了直接影响。我们需要跳出教条主义的框架，以更加动态灵活的方式来剖析政策导致的非常复杂效应，政策效应有时候会延迟、有时候会提前、有时候会震荡波动。更多关于 PTA 的量化检验将会在后续通过代码直接计算，现在不做过多数学推理；“好读书、不求甚解”在经济学初期是非常有益处的学习方式。\n\n\n2.1.4 Formula and Code\n但是上述的 DID 原理只是提出了最广义的逻辑，它并没有回答“如何计算具体的数值”这个问题，我们需要将其转为线性回归模型以便进行量化分析。根据 (Card1994?), (Eissa1996?), (Duflo2001?), (Bertrand2004?), (Qian2008?), (Angrist2009?) 等经典文献，我们不加证明地给出 DID 的数学公式如下：\n\\[\nY_{it}\n    =\n    \\beta_0\n    + \\beta_1 \\text{Treat}_i\n    + \\beta_2 \\text{Post}_t\n    + \\beta_3 (\\text{Treat}_i \\times \\text{Post}_t)\n    + \\epsilon_{it}\n\\]\n其中：\n\n\\(Y_{it}\\): 结果变量，即个体 \\(i\\) 在时间 \\(t\\) 的观测值。\n\\(\\text{Treat}_i\\): 若 \\(i\\) 是实验组，则为 1，若 \\(i\\) 控制组则为 0。\n\\(\\text{Post}_t\\): 若 \\(t\\) 是在政策冲击之后，则为 1，若 \\(t\\) 在政策冲击之前则为 0。\n\\(\\text{Treat}_i \\times \\text{Post}_t\\): 交互项目，只有 \\(i\\) 是实验组且 \\(t\\) 是在政策之后才为 1，否则为 0。\n\\(\\epsilon_{it}\\): 残差项，表示其他未被模型解释的部分。\n\\(\\beta_1\\): 实验组相对于控制组在政策冲击之前的平均差异。\n\\(\\beta_2\\): 控制组在政策冲击前后的平均差异。\n\\(\\beta_3\\): 实验组在政策冲击后的变化与控制组在政策冲击后的变化之间的差异。\n\n我们重点讨论 \\(\\beta_3\\) 的统计学意义，在满足平行趋势假设的前提下，它衡量政策对处理组的平均处理效应（Average Treatment Effect on the Treated, ATT）。如果它显著，那就说明政策确实对实验组起到了刺激作用，这意味着该政策确实对处理组产生了显著的因果效应，即政策起到了“原因”的作用，从而产生了“结果”改变。如果它不显著，那就说明在政策冲击后，处理组和控制组的结果变量之间并未观察到显著的额外差异，可以认为该政策不具有显著的因果效应。\n为了计算这个结果，我们需要将数学公式转为 R 代码，幸运的是这部分“公式 —— 代码”的转换工作已经被神包 fixest 完成，它是专门处理 panel data 的神包，用它来应付 DID 只是小菜一碟。我们“拿来主义”直接得到一个标准 DID 算法框架，开启案例实战；对它的深入细节感兴趣的同学请参考官网 fixest。\nfeols(fml = Y ~ Treat + Post + Treat * Post | FE1 + FE2, data = df)\n其中：\n\nY: 结果变量。\nTreat: 在该列中，以 1 表示实验组，以 0 表示控制组。\nPost: 在该列中，以 1 表示政策冲击之后，以 0 表示政策冲击之前。\n*: 即 Treat * Post 的交互项，即之前多次强调的 \\(\\beta_3\\) ATT 的系数。\n|: 固定效应的分隔符，左侧是因变量和自变量，右侧是固定效应。\nFE1, FE2: Fixed Effects，约定俗成的表述是 FE1 代表个体固定效应，FE2 代表时间固定效应。\n\n特别注意的是，Treat, Post 两个变量在双向固定效应模型中通常会被组别固定效应吸收，而不会再单独估计，即，我们一般不考虑它们单独的系数大小和显著性，只关注它们联合的显著性，但它们本身应该保留，而不能贸然删去。另外，FE1, FE2 是必须要有的项目，其中 FE1 有助于控制不随时间变化的个体异质性，如学生自身的学习能力、家庭背景等；FE2 有助于控制不随个体变化但随时间变化的共同冲击，如学校整体教学改革、考试难度调整等。这两个固定效应共同构成了一个标准的“双向固定效应 Two-Way Fixed Effects”，不理解也没事，我们暂时不需要知道它具体是什么意思。至此，一个标准的 DID 流程就介绍完毕，至于深入的理论和计算暂且不论，直接转入实战。"
  },
  {
    "objectID": "posts/econ/econ.html#modeling",
    "href": "posts/econ/econ.html#modeling",
    "title": "Practical Econometrics with R",
    "section": "2.2 Modeling",
    "text": "2.2 Modeling\n\n2.2.1 Base Model\n我们现在将前面的理论与代码结合起来，看看一个完整的 DID 流程具体包括哪些步骤。2020 年某省公安厅实行“交通安全年”，从严整治酒驾行为，目的是在于尽力遏制酒驾所致的重大交通事故。该政策自 6 月 1 日生效，先从核心城市 A 和 B 市试行，暂时不在 C 和 D 市开展，如果政策效果好，则后续继续扩展试点范围；如果政策效果不太好，则停止该政策。这个数据组名为 dui，这是美国法律规定的 Driving Under Influence，它对应中国刑法第133条规定的“危险驾驶罪”的醉驾行为和刑法第144条规定的“以危险方法危害公共安全罪”的毒驾行为。我们先不深入讨论中国、美国之间的刑法原理差异性，读取数据发现它包含 5 列，简要解释如下：\n\n第一列 date 是时间列，其数据格式是来自于 lubridate (处理时间序列的神包) 的 date 格式。\n第二列 city 代表 AAA, BBB, CCC, DDD 四个已经脱密的城市名称。\n第三列 cases 代表当月查处的醉酒驾驶机动车的案件数量，毒驾行为暂时不做讨论。\n第四列 cars 代表当月机动车的平均交通压力，数据由交警支队提供，反应早晚高峰交通的拥堵情况。\n第五列 bikes 代表代表非机动车的平均交通压力，数值由交警支队提供。\n\n\n\nShow Code\n# set random seed for reproducibility\nset.seed(23)\n\n\n# create sample data\ndui = (\n    tibble(\n        date = ymd(\"2020-1-1\") + months(0:11),\n        AAA_cases = c(54, 64, 53, 45, 41, 42, 26, 22, 20, 24, 17, 16),\n        BBB_cases = c(38, 50, 41, 37, 35, 36, 24, 19, 15, 19, 11, 11),\n        CCC_cases = c(81, 92, 67, 64, 58, 54, 64, 71, 82, 93, 83, 81),\n        DDD_cases = c(101, 123, 108, 99, 83, 82, 79, 85, 106, 112, 106, 104),\n    ) %&gt;% \n    mutate(\n        AAA_cars = 1000 * AAA_cases * rnorm(12, 10, 3),\n        BBB_cars = 1000 * BBB_cases * rnorm(12, 10, 3),\n        CCC_cars = 5000 * CCC_cases * rnorm(12, 10, 3),\n        DDD_cars = 5000 * DDD_cases * rnorm(12, 10, 3),\n        AAA_bikes = 1000 * AAA_cars * rnorm(12, 1, 0.1),\n        BBB_bikes = 1000 * BBB_cars * rnorm(12, 1, 0.1),\n        CCC_bikes = 1000 * CCC_cars * rnorm(12, 1, 0.1),\n        DDD_bikes = 1000 * CCC_cars * rnorm(12, 1, 0.1),\n    ) %&gt;% \n    pivot_longer(\n        cols = -date,\n        names_to = c(\"city\", \".value\"),\n        names_sep = \"_\"\n    ) %&gt;% \n    mutate(cars = log(cars), bikes = log(bikes))\n)\n\n\n# show sample data\n(\n    dui %&gt;% \n    slice_head(n = 10) %&gt;% \n    datasummary_df(title = \"Sample of DUI\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Sample of DUI\n              \n                date\n                city\n                cases\n                cars\n                bikes\n              \n        \n        \n        \n                \n                  2020-01-01\n                  AAA\n                  54.00\n                  13.26\n                  20.21\n                \n                \n                  2020-01-01\n                  BBB\n                  38.00\n                  12.76\n                  19.62\n                \n                \n                  2020-01-01\n                  CCC\n                  81.00\n                  15.04\n                  21.94\n                \n                \n                  2020-01-01\n                  DDD\n                  101.00\n                  15.77\n                  22.00\n                \n                \n                  2020-02-01\n                  AAA\n                  64.00\n                  13.23\n                  20.11\n                \n                \n                  2020-02-01\n                  BBB\n                  50.00\n                  13.26\n                  20.16\n                \n                \n                  2020-02-01\n                  CCC\n                  92.00\n                  15.34\n                  22.35\n                \n                \n                  2020-02-01\n                  DDD\n                  123.00\n                  15.12\n                  22.02\n                \n                \n                  2020-03-01\n                  AAA\n                  53.00\n                  13.42\n                  20.45\n                \n                \n                  2020-03-01\n                  BBB\n                  41.00\n                  12.47\n                  19.34\n                \n        \n      \n    \n\n\n\n我们提取数据绘图，当前最关注的问题就是新法案 6 月份在 AAA, BBB 两市月实行后，对于当地的重大交通事故发生数量是否有显著抑制作用？另外，作为对比的 CCC, DDD 两市的重大交通事故数量是否有特殊变化？因此我们所考虑的列是 cases 的政策前后差异以及在 AAA, BBB, CCC, DDD 之间的组间差异。至于其他信息暂且不论，它们的意义在后续会开展讨论。\n\n\nShow Code\n# plot cases per city\nfig1 = (\n    dui %&gt;% \n    ggplot(aes(x = date, y = cases, color = city)) +\n    geom_line() +\n    geom_vline(xintercept = ymd(\"2020-6-1\"), linetype = \"dashed\", color = \"gray\") +\n    scale_color_iterm() +\n    scale_x_date(breaks = breaks_pretty(n = 4)) +\n    labs(title = \"DUI Cases\", x = \"\", y = \"\") +\n    theme_light()\n)\n\n\n# plot dui per city\nfig2 = (\n    dui %&gt;% \n    ggplot(aes(x = date, y = cars, color = city)) +\n    geom_line() +\n    geom_vline(xintercept = ymd(\"2020-6-1\"), linetype = \"dashed\", color = \"gray\") +\n    scale_color_iterm() +\n    scale_x_date(breaks = breaks_pretty(n = 4)) +\n    labs(title = \"Cars Index\", x = \"\", y = \"\") +\n    theme_light()\n)\n\n\n# plot petrol per city\nfig3 = (\n    dui %&gt;% \n    ggplot(aes(x = date, y = bikes, color = city)) +\n    geom_line() +\n    geom_vline(xintercept = ymd(\"2020-6-1\"), linetype = \"dashed\", color = \"gray\") +\n    scale_color_iterm() +\n    scale_x_date(breaks = breaks_pretty(n = 4)) +\n    labs(title = \"Bikes Index\", x = \"\", y = \"\") +\n    theme_light()\n)\n\n\n# combine into single figure\n(\n    wrap_plots(fig1, fig2, fig3) + \n    plot_layout(ncol = 1, nrow = 3) + \n    plot_annotation(title = \"Fig 3.1: Traffic Cases DID\")\n)\n\n\n\n\n\n\n\n\n\n我们已知 DID 的算法原理，将案例的主要参数代入，得到模型；\nfeols(fml = cases ~ treat + post + treat * post | date + city, data = dui)\n\ncases: 醉酒驾驶案件数量；\ndate: 用于固定时间效应的月份；\ncity: 用于固定单位效应的城市；\n\n跑代码结果显示 treat:post 显著 \\(p = 0.03\\) 且参数 \\(\\beta = -22.49\\)，说明严格的交管政策显著降低了酒驾案件数量，我们可以认为该市公安局的政策是显著有效的。至此，我们完成了一个最基础的 DID 模型测算，初步证明了政策效力；接下来我们在基准模型之上增加一点难度，大概思路跟 LM 一样，主要考虑内生性 Endogeneity 和异质性 Heterogeneity；但是注意，DID 模型不需要执行单独的异方差 Heteroskedasticity 的检验，因为它已经被内置的算法工具 vcov() 处理掉了。因此我们主要手动验证内生性和异质性，如果这两项检测都通过，我们再继续往下推进到事件分析 Event Study 和安慰剂检测 Placebo Tests，从而完成整个 DID 分析流程。\n\n\nShow Code\n# create treat and post columns for dui\ndui = (\n    dui %&gt;% \n    mutate(\n        treat = ifelse(city %in% c(\"AAA\", \"BBB\"), 1, 0),\n        post = ifelse(date &gt; ymd(\"2020-6-1\"), 1, 0)\n    )\n)\n\n\n# fit an OLS-DID model\ndui_mod1 = feols(\n    fml = cases ~ treat + post + treat * post | city + date,\n    vcov = ~ city,\n    data = dui\n)\n\n\n# summarize dui_mod1\n(\n    dui_mod1 %&gt;% \n    tidy() %&gt;% \n    rename(se = std.error, pv = p.value) %&gt;% \n    datasummary_df(title = \"Summary of dui_mod1\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Summary of dui_mod1\n              \n                term\n                estimate\n                se\n                statistic\n                pv\n              \n        \n        \n        \n                \n                  treat:post\n                  -30.50\n                  5.65\n                  -5.40\n                  0.01\n                \n        \n      \n    \n\n\n\n\n\n2.2.2 Exogeineity\n从 Section 1.2 我们已经知道什么是内生性和外生性，DID 模型与 LM 一样受到内生性的影响，DID 同样需要处理内生性导致的模型误差，总体处理逻辑与 LM 处理相似，不再赘述。简而言之，我们将自变量拆为内生部分和外生部分，并加入工具变量，再进行 2SLS 即可。第一阶段是求解内生变量的预测值，我们用工具变量 \\(Z_{it}\\) 和所有其他的外生变量 \\(\\text{Treat}_i, \\text{Post}_t\\) 来对内生变量 \\(\\text{Treat}_i \\times \\text{Post}_t\\) 进行回归，目的是为了得到该内生变量的预测值 \\(\\widehat{(\\text{Treat}_i \\times \\text{Post}_t)}\\)。\n\\[\n\\begin{aligned}\n    \\text{Treat}_i \\times \\text{Post}_t =\n    \\alpha_0 +\n    \\alpha_1 \\text{Treat}_i +\n    \\alpha_2 \\text{Post}_t +\n    \\alpha_3 Z_{it} +\n    \\nu_{it}\n\\end{aligned}\n\\]\n第二阶段是将第一阶段的预测结果代入主回归模型，我们用一个由工具变量生成的“干净”的预测值 \\(\\widehat{(\\text{Treat}_i \\times \\text{Post}_t)}\\) ，来替换掉模型中“被污染”的内生变量 \\(\\text{Treat}_i \\times \\text{Post}_t\\)，从而得到无偏的因果效应估计。\n\\[\n\\begin{aligned}\nY_{it} =\n    \\beta_0 +\n    \\beta_1 \\text{Treat}_i +\n    \\beta_2 \\text{Post}_t +\n    \\beta_{3} \\widehat{(\\text{Treat}_i \\times \\text{Post}_t)} +\n    \\epsilon_{it}\n\\end{aligned}\n\\]\n数学公式虽然比较复杂，但是 IV-DID 的算法原理很简单，就是在 OLS-DID 的最右侧先执行第一阶段回归，然后将结果代入到左侧原有的主回归当中。但是请注意，从形式上看，这是“增加了”两个变量，然而本质上它的作用是“替换”，而没有增加或删减任何变量。即：\nOLS-DID: feols(fml = Y ~ Treat + Post + Treat * Post | FE1 + FE2, data = df)\n\n\nIV-DID: feols(fml = Y ~ Treat + Post + Treat * Post | FE1 + FE2 | X_edo ~ Z, data = df)\n原理解释完毕，回到本案中，我们根据常识假设 cars 是内生变量、bikes 是工具变量；换言之，我们希望通过 bikes 来“清理” cars 变量对于整个模型的“污染”，从而得到一个更“干净”的判断结果。这部分内容在 Section 1.2 已经做了详细介绍，忘记的同学可以再爬楼回去翻一翻，大同小异；最后我们建立如下的 IV-DID 模型，并在代码中跑出结果。\nfeols(fml = cases ~ treat + post + treat * post | date + city | cars ~ bikes, data = dui)\n我们跑完结果得到4行数据，分别代表第一阶段的工具变量显著性和第二阶段主回归的显著性，简要解析如下。\n在第一阶段中：\n\nbikes 显著，即 bikes 对于 cars 具有显著刺激作用，说明我们的工具变量是强有效的，这是非常重要的结果；它直接决定我们的 IV-DID 能否继续往下推演，如果这个结果不显著，那必须调整模型参数，直到得到显著结果。\ntreat:post 不显著，即 treat:post 对于 cars 没有显著刺激作用，说明政策效应对于交通压力并无影响；这个结果并不很重要，我们也并不关心政策对于交通或者出行的影响，我们关心的是政策对于醉驾的影响。\n\n在第二阶段中：\n\nfit_cars 显著，即 fit_cars 对于 cases 有显著影响，说明经过工具变量“清理”后的内生变量对于最终的醉驾案件的数量具有直接重要的影响；其参数高达 20.5，即说明如果交通压力增加 1 个单位，醉驾的案件会增加 20.5 件，但实际情况不是这么简单的传导机制，我们暂时只关注这个“显著”结果接口，不深入讨论。\ntreat:post 显著，即除了上述原因之外，政策本身还从其他的维度对于醉驾案件产生显著影响，并且这个影响是负相关的，即政策加强 1 个单位，醉驾案件减少 9.06 件；但实际情况会远比这个数据更复杂，它说明政策对于醉驾的强力压制性。\n\n这个结果清晰地分离出两条截然不同的影响路径，一方面，汽车交通压力作为一种风险机制，其增加会推升醉驾案件数量，这是显而易见的结果；另一方面，在控制了上述机制后，政策干预本身通过其他渠道，独立且有效地降低了醉驾案件，印证了政策本身的效力。因此，我们可以认为尽管面临着汽车数量增加带来的不利背景，但该政策干预本身对降低醉驾案件起到了显著的直接抑制作用，且所有标准误均在城市层面进行了聚类调整，保证了估计结果的稳健性。\n\n\nShow Code\n# fit an IV-DID model\ndui_mod2 = feols(\n    fml = cases ~ treat + post + treat * post | city + date | cars ~ bikes,\n    vcov = ~ city,\n    data = dui\n)\n\n\n# summarize IV-DID\n(\n    dui_mod2 %&gt;% \n    summary(stage = 1:2) %&gt;% \n    coeftable() %&gt;% \n    rename(\n        estimate = Estimate, se = `Std. Error`, \n        tv = `t value`, pv = `Pr(&gt;|t|)`\n    ) %&gt;% \n    as_tibble() %&gt;% \n    select(-id) %&gt;% \n    datasummary_df(title = \"Summary of dui_mod2\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Summary of dui_mod2\n              \n                iv\n                coefficient\n                estimate\n                se\n                tv\n                pv\n              \n        \n        \n        \n                \n                  First stage: cars\n                  bikes\n                  0.88\n                  0.09\n                  9.86\n                  0.00\n                \n                \n                  First stage: cars\n                  treat:post\n                  0.10\n                  0.25\n                  0.38\n                  0.73\n                \n                \n                  Second stage\n                  fit_cars\n                  9.31\n                  2.07\n                  4.50\n                  0.02\n                \n                \n                  Second stage\n                  treat:post\n                  -23.51\n                  2.97\n                  -7.91\n                  0.00\n                \n        \n      \n    \n\n\n\n\n\n2.2.3 Heterogeneity\n如我们在 Section 1.3 所讨论的一样，异质性分析是研究多个分组当中的组间差异。即在本案例中，AAA, BBB, CCC, DDD 之间的差异程度，首先我们知道 CCC, DDD 没有受到政策冲击，因此它的显著性应该是0，这是很重要的基准；然后我们需要考虑几个问题：（1）AAA 相对于平均显著性的差异性？（2）BBB 相对于平均显著性的差异性？（3）AAA, BBB 之间是否有差异性？这几个问题所指向的本质都是“异质性 Heterogeneity”，具体细节我们不再赘述，忘记的同学请往前翻一翻。我们接下来不加证明直接提出 fixest 对此问题的解决方案，对深度感兴趣的同学可以参考：fixest factor，对原始理论感兴趣的同学请参考 (Berge2018?)。\n我们标准的 OLS-DID 模型和带有工具变量的 IV-DID 模型是：\nOLS-DID: feols(fml = Y ~ Treat + Post + Treat * Post | FE1 + FE2, data = df)\n\n\nIV-DID: feols(fml = Y ~ Treat + Post + Treat * Post | FE1 + FE2 | X_edo ~ Z, data = df)\n那么在 fixest 当中处理这个问题的思路也比较直接，只需要在原模型当中增加一个分类变量的交互项即可，本质上的思路跟 OLS-LM 很相似，都是通过“交互项 Interaction Terms”来实现。如果分类变量的交互项当中，经历政策冲击的组出现了显著结果，并且不同的组之间的结果存在显著差异性，那么可以认为模型存在明显的组间差异；反之则不存在组间差异。整个探究的过程很像在一堆人当中找到谁是“滥竽充数”的南郭先生，以及谁是“真材实料”的伯牙子期。我们不介绍这部分的数学公式，直接给出对应的代码原理如下：\nOLS-DID Hete: feols(fml = Y ~ Treat + Post + i(Factor_Var, Treat * Post) | FE1 + FE2, data = df)\n\n\nIV-DID Hete: feols(fml = Y ~ Treat + Post + i(Factor_Var, Treat * Post) | FE1 + FE2 | X_edo ~ Z, data = df)\n根据这个原理，我们先处理简单的不包含工具变量的 OLS 模型，即为：\ndui_mod1_hete = feols(fml = cases ~ treat + post + i(city, treat * post) | city + date, data = dui)\n但是这个模型只能返回 Groupwise 异质性，即每个组相对于总体平均显著性的差异程度，而无法解释每个组两两匹配成对的差异性，即 Pairwise 异质性。因此为了更好的观测组间异质性，我们继续借用 Section 1.3 提到过的神包 marginaleffects 然后分别进行 Groupwise 和 Pariwise 异质性检验。结果如下图所示，AAA, BBB 两组各自独立与平均效应存在显著差异，但是 AAA, BBB 之间的差异性不显著，其中原因主要是它们之间过于简单的交互项形成了“多重共线性”；我们不展开具体内容，这个表格和图已经很好的反应了 OLS-DID 模型的异质性检测结果。\n\n\nShow Code\n##### for OLS-DID\n# calculate heterogeneity test of dui_did\ndui_mod1_hete = feols(\n    fml = cases ~ treat + post + i(city, treat * post) | city + date, \n    vcov = ~ city,\n    data = dui\n)\n\n\n# calculate groupwise heterogeneity\ndui_mod1_group_hete = (\n    slopes(\n        model = dui_mod1_hete, \n        variables = \"treat\", \n        by = \"city\",\n        hypothesis = 0\n    ) %&gt;% \n    tidy() %&gt;% \n    rename(\n        se = std.error,\n        sv = s.value,\n        pv = p.value, \n        conf_lo = conf.low, \n        conf_hi = conf.high\n    )\n)\ndui_mod1_group_hete %&gt;% datasummary_df(title = \"Groupwise Heterogeneity of dui_mod1\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        Groupwise Heterogeneity of dui_mod1\n              \n                term\n                contrast\n                city\n                estimate\n                se\n                statistic\n                pv\n                sv\n                conf_lo\n                conf_hi\n              \n        \n        \n        \n                \n                  treat\n                  1 - 0\n                  AAA\n                  -16.75\n                  2.48\n                  -6.75\n                  0.00\n                  36.02\n                  -21.61\n                  -11.89\n                \n                \n                  treat\n                  1 - 0\n                  BBB\n                  -13.75\n                  2.48\n                  -5.54\n                  0.00\n                  25.02\n                  -18.61\n                  -8.89\n                \n                \n                  treat\n                  1 - 0\n                  CCC\n                  0.00\n                  NA\n                  NA\n                  NA\n                  NA\n                  NA\n                  NA\n                \n                \n                  treat\n                  1 - 0\n                  DDD\n                  0.00\n                  NA\n                  NA\n                  NA\n                  NA\n                  NA\n                  NA\n                \n        \n      \n    \n\n\n\nShow Code\n# calculate pairwise heterogeneity\ndui_mod1_pair_hete = (\n    slopes(\n        model = dui_mod1_hete, \n        variables = \"treat\", \n        by = \"city\",\n        hypothesis = difference ~ pairwise,\n    ) %&gt;% \n    tidy() %&gt;% \n    rename(\n        se = std.error,\n        sv = s.value,\n        pv = p.value, \n        conf_lo = conf.low, \n        conf_hi = conf.high\n    )\n)\ndui_mod1_pair_hete %&gt;% datasummary_df(title = \"Pairwise Heterogeneity of dui_mod1\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        Pairwise Heterogeneity of dui_mod1\n              \n                hypothesis\n                estimate\n                se\n                statistic\n                pv\n                sv\n                conf_lo\n                conf_hi\n              \n        \n        \n        \n                \n                  (BBB) - (AAA)\n                  3.00\n                  0.00\n                  100663296.00\n                  0.00\n                  \n                  3.00\n                  3.00\n                \n                \n                  (CCC) - (AAA)\n                  16.75\n                  2.48\n                  6.75\n                  0.00\n                  36.02\n                  11.89\n                  21.61\n                \n                \n                  (DDD) - (AAA)\n                  16.75\n                  2.48\n                  6.75\n                  0.00\n                  36.02\n                  11.89\n                  21.61\n                \n                \n                  (CCC) - (BBB)\n                  13.75\n                  2.48\n                  5.54\n                  0.00\n                  25.02\n                  8.89\n                  18.61\n                \n                \n                  (DDD) - (BBB)\n                  13.75\n                  2.48\n                  5.54\n                  0.00\n                  25.02\n                  8.89\n                  18.61\n                \n                \n                  (DDD) - (CCC)\n                  0.00\n                  NA\n                  NA\n                  NA\n                  NA\n                  NA\n                  NA\n                \n        \n      \n    \n\n\n\nShow Code\n# plot groupwise heterogeneity\nfig1 = (\n    dui_mod1_group_hete %&gt;% \n    ggplot(aes(x = estimate, y = city, color = city)) +\n    geom_point(size = 5) +\n    geom_errorbarh(\n        aes(xmin = conf_lo, xmax = conf_hi), linewidth = 2, height = 0.5\n    ) +\n    geom_label(aes(label = scales::pvalue(pv, add_p = TRUE))) +\n    labs(x = \"Estimate\", y = \"\", title = \"Groupwise Heterogeneity\") +\n    scale_color_iterm() +\n    theme_light() +\n    theme(legend.position = \"none\")\n)\n\n\n# plot pairwise heterogeneity\nfig2 = (\n    dui_mod1_pair_hete %&gt;%  \n    ggplot(aes(x = estimate, y = hypothesis, color = hypothesis)) +\n    geom_point(size = 5) +\n    geom_errorbarh(\n        aes(xmin = conf_lo, xmax = conf_hi), linewidth = 2, height = 0.5\n    ) +\n    geom_label(aes(label = scales::pvalue(pv, add_p = TRUE))) +\n    labs(x = \"Estimate\", y = \"\", title = \"Pairwise Heterogeneity\") +\n    scale_color_iterm() +\n    theme_light() +\n    theme(legend.position = \"none\")\n)\n\n\n# combine into one figure\n(\n    wrap_plots(fig1, fig2) + \n    plot_layout(ncol = 2, nrow = 1) +\n    plot_annotation(title = \"Fig 3.2: Heterogeneity Test of dui_mod1\")\n)\n\n\n\n\n\n\n\n\n\n我们采用几乎完全一样的算法原理计算 dui_mod2 的异质性，并且进一步拆分为总体异质性和组间异质性，得到如下结果。\n\n\nShow Code\n##### for IV-DID\n# calculate heterogeneity\ndui_mod2_hete = feols(\n    fml = cases ~ treat + post + i(city, treat * post) | city + date | cars ~ bikes, \n    vcov = ~ city,\n    data = dui\n)\n\n\n# calculate groupwise heterogeneity\ndui_mod2_group_hete = (\n    slopes(\n        model = dui_mod2_hete, \n        variables = \"treat\", \n        by = \"city\",\n        hypothesis = 0\n    ) %&gt;% \n    tidy() %&gt;% \n    rename(\n        se = std.error,\n        sv = s.value,\n        pv = p.value, \n        conf_lo = conf.low, \n        conf_hi = conf.high\n    )\n)\ndui_mod2_group_hete %&gt;% datasummary_df(title = \"Groupwise Heterogeneity of dui_mod2\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        Groupwise Heterogeneity of dui_mod2\n              \n                term\n                contrast\n                city\n                estimate\n                se\n                statistic\n                pv\n                sv\n                conf_lo\n                conf_hi\n              \n        \n        \n        \n                \n                  treat\n                  1 - 0\n                  AAA\n                  -12.94\n                  0.92\n                  -14.07\n                  0.00\n                  146.91\n                  -14.74\n                  -11.13\n                \n                \n                  treat\n                  1 - 0\n                  BBB\n                  -10.87\n                  1.13\n                  -9.58\n                  0.00\n                  69.79\n                  -13.09\n                  -8.65\n                \n                \n                  treat\n                  1 - 0\n                  CCC\n                  0.00\n                  NA\n                  NA\n                  NA\n                  NA\n                  NA\n                  NA\n                \n                \n                  treat\n                  1 - 0\n                  DDD\n                  0.00\n                  NA\n                  NA\n                  NA\n                  NA\n                  NA\n                  NA\n                \n        \n      \n    \n\n\n\nShow Code\n# calculate pairwise heterogeneity\ndui_mod2_pair_hete = (\n    slopes(\n        model = dui_mod2_hete, \n        variables = \"treat\", \n        by = \"city\",\n        hypothesis = difference ~ pairwise,\n    ) %&gt;% \n    tidy() %&gt;% \n    rename(\n        se = std.error,\n        sv = s.value,\n        pv = p.value, \n        conf_lo = conf.low, \n        conf_hi = conf.high\n    )\n)\ndui_mod2_pair_hete %&gt;% datasummary_df(title = \"Pairwise Heterogeneity of dui_mod2\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        Pairwise Heterogeneity of dui_mod2\n              \n                hypothesis\n                estimate\n                se\n                statistic\n                pv\n                sv\n                conf_lo\n                conf_hi\n              \n        \n        \n        \n                \n                  (BBB) - (AAA)\n                  2.07\n                  0.23\n                  9.11\n                  0.00\n                  63.43\n                  1.62\n                  2.51\n                \n                \n                  (CCC) - (AAA)\n                  12.94\n                  0.92\n                  14.07\n                  0.00\n                  146.91\n                  11.13\n                  14.74\n                \n                \n                  (DDD) - (AAA)\n                  12.94\n                  0.92\n                  14.07\n                  0.00\n                  146.91\n                  11.13\n                  14.74\n                \n                \n                  (CCC) - (BBB)\n                  10.87\n                  1.13\n                  9.58\n                  0.00\n                  69.79\n                  8.65\n                  13.09\n                \n                \n                  (DDD) - (BBB)\n                  10.87\n                  1.13\n                  9.58\n                  0.00\n                  69.79\n                  8.65\n                  13.09\n                \n                \n                  (DDD) - (CCC)\n                  0.00\n                  NA\n                  NA\n                  NA\n                  NA\n                  NA\n                  NA\n                \n        \n      \n    \n\n\n\nShow Code\n# plot groupwise heterogeneity\nfig1 = (\n    dui_mod2_group_hete %&gt;% \n    ggplot(aes(x = estimate, y = city, color = city)) +\n    geom_point(size = 5) +\n    geom_errorbarh(\n        aes(xmin = conf_lo, xmax = conf_hi), linewidth = 2, height = 0.5\n    ) +\n    geom_label(aes(label = scales::pvalue(pv, add_p = TRUE))) +\n    labs(x = \"Estimate\", y = \"\", title = \"Groupwise Heterogeneity of dui_mod2\") +\n    scale_color_iterm() +\n    theme_light() +\n    theme(legend.position = \"none\")\n)\n\n\n# plot pairwise heterogeneity\nfig2 = (\n    dui_mod2_pair_hete %&gt;% \n    ggplot(aes(x = estimate, y = hypothesis, color = hypothesis)) +\n    geom_point(size = 5) +\n    geom_errorbarh(\n        aes(xmin = conf_lo, xmax = conf_hi), linewidth = 2, height = 0.5\n    ) +\n    geom_label(aes(label = scales::pvalue(pv, add_p = TRUE))) +\n    labs(x = \"Estimate\", y = \"\", title = \"Pairwise Heterogeneity of dui_mod2\") +\n    scale_color_iterm() +\n    theme_light() +\n    theme(legend.position = \"none\")\n)\n\n\n# combine into one figure\n(\n    wrap_plots(fig1, fig2) + \n    plot_layout(ncol = 2, nrow = 1) +\n    plot_annotation(title = \"Fig 3.3: Heterogeneity Test of dui_mod2\")\n)"
  },
  {
    "objectID": "posts/econ/econ.html#tests",
    "href": "posts/econ/econ.html#tests",
    "title": "Practical Econometrics with R",
    "section": "2.3 Tests",
    "text": "2.3 Tests\n\n2.3.1 Event Study\n\n事件分析：主要研究政策冲击对政策之前和之后的处理组的影响；\nEvent Study: to analyze the pre- and post-significance of policy shock;\n若 pre-treat 不显著但是 post-treat 显著，则政策具有显著效果，可能是正向也可能是负向；\n并且 post-treat 尽量满足单调性，否则很容易被评审认为政策结果是偶然而不是必然；\npre-treat 不显著，post-treat 显著且满足一定单调减，证明政策具有显著效应；\n即政策实施后，抑制或降低了交通事故伤亡人数，对交通安全起了显著促进作用；\n\n\n\nShow Code\n# run event study test\ndui = dui %&gt;% mutate(month_lag = month(date) - 6)\n\n\n# plot Event Study figure\n( \n    feols(\n        fml = cases ~ i(month_lag, treat, 1) | city + date | cars ~ bikes,\n        vcov = ~city,\n        data = dui\n    ) %&gt;%\n    ggiplot(main = \"Fig 3.4: Event Study\")\n)\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Placebo Tests\nEvent Study 对原数据当中的 treat 乱序排列后形成新的数据组，然后再跑模型，观察乱序模型的显著性；如果乱序模型也显著，说明原科学模型的效果甚至不如随机抽样的效果，原模型即宣告无效。EG: 张三正在备考不定向选择题，李四每次都随机填答案，最后李四比张三还多十分，安慰剂胜，张三卒。安慰剂检测中，应该以表格为主、图表为辅，因为一张表格可以容下非常多维度的数据，远超图表；结果显示，极少数 placebo 的 post-treat 显著，绝大多数不显著，甚至还有部分 pre-treat 也显著；综上，placebo 不满足 post-treat significance 条件，原模型通过 Event Study Test。\n\n\nShow Code\n# create 6 placebos by sampling treat column\nfor (i in 1:6) {\n    dui = dui %&gt;% mutate(!!paste0(\"treat_plb_\", i) := sample(treat))\n}\n\n\n# plot placebo event study\n(\n    map(1:6,\n        function(i) {\n            dui %&gt;% \n            rename(treat_plb_temp = paste0(\"treat_plb_\", i)) %&gt;% \n            feols(\n                fml = cases ~ \n                        i(month_lag, treat_plb_temp, 0) | \n                        city + date | \n                        cars ~ bikes, \n                vcov = ~city\n            ) %&gt;% \n            ggiplot(main = paste(\"Placebo: \",i)) + \n            labs(x = \"\", y = \"\")\n        }\n    ) %&gt;% \n    wrap_plots() + plot_layout(2, 3)\n)\n\n\n\n\n\n\n\n\n\nShow Code\n# calculate placesbo table\n(\n    map(1:6, \n        function(i) {\n            dui %&gt;% \n            rename(treat_plb_temp = paste0(\"treat_plb_\", i)) %&gt;% \n            feols(\n                fml = cases ~ \n                        i(month_lag, treat_plb_temp, 0) | \n                        city + date | \n                        cars ~ bikes,\n                vcov = ~ city\n            ) %&gt;% \n            tidy() %&gt;% \n            # keep term and dynamic rename pvalues and drop all others\n            mutate(\n                term = term,\n                !!paste0(\"plb_\", i, \"_pv\") := p.value,\n                .keep = \"none\"\n            )\n        }\n    ) %&gt;% \n    reduce(left_join, by = \"term\") %&gt;% \n    datasummary_df(title = \"Event Study of Placebos\")\n)\n\n\n\n\n    \n\n    \n    \n      \n        \n        Event Study of Placebos\n              \n                term\n                plb_1_pv\n                plb_2_pv\n                plb_3_pv\n                plb_4_pv\n                plb_5_pv\n                plb_6_pv\n              \n        \n        \n        \n                \n                  fit_cars\n                  0.00\n                  0.00\n                  0.00\n                  0.01\n                  0.00\n                  0.02\n                \n                \n                  month_lag::-5:treat_plb_temp\n                  0.43\n                  0.75\n                  0.81\n                  0.30\n                  0.36\n                  0.05\n                \n                \n                  month_lag::-4:treat_plb_temp\n                  0.17\n                  0.44\n                  0.87\n                  0.48\n                  0.21\n                  0.47\n                \n                \n                  month_lag::-2:treat_plb_temp\n                  0.79\n                  0.83\n                  0.47\n                  NA\n                  0.31\n                  0.09\n                \n                \n                  month_lag::-1:treat_plb_temp\n                  0.00\n                  0.00\n                  0.19\n                  0.08\n                  NA\n                  NA\n                \n                \n                  month_lag::2:treat_plb_temp\n                  0.27\n                  0.75\n                  0.43\n                  0.61\n                  0.89\n                  0.86\n                \n                \n                  month_lag::3:treat_plb_temp\n                  0.06\n                  0.10\n                  0.34\n                  0.34\n                  0.49\n                  0.31\n                \n                \n                  month_lag::4:treat_plb_temp\n                  0.31\n                  0.21\n                  NA\n                  0.18\n                  0.03\n                  0.06\n                \n                \n                  month_lag::5:treat_plb_temp\n                  0.56\n                  0.08\n                  0.03\n                  0.75\n                  0.80\n                  1.00"
  },
  {
    "objectID": "posts/econ/econ.html#staggered-did",
    "href": "posts/econ/econ.html#staggered-did",
    "title": "Practical Econometrics with R",
    "section": "2.4 Staggered DID",
    "text": "2.4 Staggered DID\nStaggered DID 本质是比较多个样本组经历了多次政策冲击前、冲击后的差异性；策在 \\(t_1\\) 对 \\(x_1\\) 生效，在 \\(t_2\\) 对 \\(x_2\\) 生效，在 \\(t_3\\) 对 \\(x_3\\) 生效，以此类推；这样的情况，标准 DID 会遇到 negative weighting, heterogeneity, dynamic effects 等技术挑战。于是 Callaway & Sant’Anna 2020, Sun and Abraham 2021 都提出了非常优秀的解决方案。\n某市公安局 2020 年开展“精神文明城市”建设行动，重点在于切实降低本小市的刑事案件发生数量；市局决定 5～7月重点防控 A 市，7～9月重点防控 B 市，C 和 D 市作为对照组按常规流程检查；加密数据并绘图如下，发现 A 和 B 市虽然有过一定程度下降，但是很快又在攀升过程。\n\n\nShow Code\n# create sample data\ncars2 = (\n    tibble(\n        month = seq(from = as.yearmon(\"2020-1\"), to = as.yearmon(\"2020-12\"), by = 1/12),\n        AAA = c(207, 228, 205, 187, 152, 113, 97, 91, 99, 108, 119, 126),\n        BBB = c(175, 199, 181, 164, 171, 165, 128, 107, 82, 96, 102, 113),\n        CCC = c(71, 88, 64, 59, 61, 68, 71, 61, 59, 79, 68, 59),\n        DDD = c(94, 107, 87, 71, 78, 69, 72, 80, 72, 91, 82, 77),\n    ) %&gt;% \n    pivot_longer(cols = -month, names_to = \"city\", values_to = \"cases\") %&gt;% \n    mutate(cases = log(cases))\n)\n\n\n# plot sample figure\n(\n    cars2 %&gt;% \n    ggplot(aes(x = month, y = cases, color = city)) +\n    geom_line() +\n    geom_vline(\n        aes(xintercept = as.yearmon(\"2020-5\")), linetype = \"dashed\", color = \"gray\"\n    ) +\n    geom_vline(\n        aes(xintercept = as.yearmon(\"2020-7\")), linetype = \"dashed\", color = \"gray\"\n    ) +\n    scale_color_iterm() +\n    labs(title = \"Criminal Cases\", x = \"\", y = \"Cases Num\") +\n    theme_light()\n)\n\n\n基于标准 DID 模型，Staggered DID 考虑了时间和样本的动态变化，提出如下公式；\n\\[\nY_{it} =\n    \\sum_{k=-K}^{-2} \\beta_k \\cdot D_{it}^k +\n    \\sum_{k=0}^{L} \\beta_k \\cdot D_{it}^k +\n    \\alpha_i + \\gamma_t + \\epsilon_{it}\n\\]\n\n\\(D_{it}^k\\): relative time indicators;\n\\(G_i\\): treatment adoption period;\n\\(k = -K,...,L\\): time relative to treatment;\n\\(\\alpha_i\\): unit fixed effects;\n\\(\\gamma_t\\): time fixed effects;\n\\(k = -1\\): reference period;\n\n参考标准 DID 的数据结构，建模如下：\ny ~ sunab(treat * post) | month + city\nsunab() 是 Laurent Berge 在 2022 年将 Sun and Abraham 的理论移植在了 fixest 当中；对比 2018 年发布的标准 DID 模型，我们就能够看到 tidyverse 的底层架构的前瞻性，只需要修改一点细微之处，就可以让旧的模型适应最新理论发展，并无缝衔接原有的庞大生态链；绕过复杂的代码架构和数学理论，我们战练结合，直接跑代码看结果。\ny ~ treat * post | month + city\nEvent Study 分析过程跟标准 DID 基本一致，不再逐个讨论；下图显示 pre-treat 不显著，但是 post-treat 显著，即政策显著降低了刑事案件数量。综上，模型在 treat-post 的对比中存在显著差异性，通过事件检验；但是随着政策时间拉长，刑事案件数量逐渐回升，说明政策效力在逐渐衰退，这是另一个深度的问题了。\n\n\nShow Code\n# create three fundamental columns for cars2\ncars2 = (\n    cars2 %&gt;% \n    mutate(\n        month_num = month(month),\n        treat_num = \n            case_when(\n                city == \"AAA\" ~ 5,\n                city == \"BBB\" ~ 7,\n                city == \"CCC\" ~ 999,\n                city == \"DDD\" ~ 999,\n            ),\n        post = month_num &gt; treat_num,\n    )\n)\n\n\n# plot event study figure\n(\n    cars2 %&gt;% \n    feols(\n        cases ~ sunab(treat_num, month_num) | city + month_num,\n        vcov = ~ city\n    ) %&gt;% \n    ggiplot(main = \"Event Study of cars2\")\n)\n\n\nHetetogeneity 重要说明，当样本的空间划分和时间采样都很小的时候，该测试并不严谨，非常容易在边缘徘徊；A 和 B 相对于 control 都单独显著，注意此处的参照物是 control 整体，而不是 CCC 或 DDD。既然 A 和 B 独立跟 control 显著，那么 treat 总体必然与 control 显著；综上，模型在 treat 的子类当中不存在较大异质性，通过异质检验。\n\n\nShow Code\n# plot heterogeneity figure\n(\n    cars2 %&gt;% \n    feols(\n        cases ~ i(city, treat_num * post) | city + month_num, \n        vcov = ~ city\n    ) %&gt;% \n    ggiplot(main = \"Heterogeneity of cars2\")\n)\n\n\nEvent Study 此处的安慰剂检测跟标准 DID 检测一样，略过细节，关注结论；几乎所有的 post-treat 都不显著，极少数显著的组，可能正好选中了 False Positive；综上，安慰剂不满足 post-treat significance 条件。\n\n\nShow Code\n# the procedures are very similar to standard DID part \n# first create placebo columns and then run ES and Hete tests\n\n\n# create 6 placebo columns by sampling treat column, for consistency\nfor (i in 1:6) {\n    cars2 = \n        cars2 %&gt;% \n        mutate(!!paste0(\"treat_plb_\", i) := sample(treat_num))\n}\n\n\n# plot event study table\n(\n    map(1:6,\n        function(i) {\n            cars2 %&gt;% \n            rename(treat_plb_temp = paste0(\"treat_plb_\", i)) %&gt;% \n            feols(\n                cases ~ sunab(treat_plb_temp, month_num) | city + month_num,\n                vcov = ~ city\n            ) %&gt;% \n            tidy() %&gt;% \n            mutate(\n                term = term,\n                !!paste0(\"plb_\", i, \"_pv\") := p.value,\n                .keep = \"none\"\n            )\n        }\n    ) %&gt;% \n    reduce(left_join, by = \"term\") %&gt;% \n    datasummary_df(title = \"Event Study of Placebos of cars2\")\n)\n\n\nHeterogeneity 无论是 treat 当中的 A 或 B，它们对应的安慰剂组几乎都不显著；至于 C 或 D，它们本身就是 control 组，其显著性并无任何意义；综上，安慰剂不满足 cross-group significance 条件。\n\n\nShow Code\n# plot heterogeneity table\n(\n    map(1:6,\n        function(i) {\n            cars2 %&gt;% \n            rename(treat_plb_temp = paste0(\"treat_plb_\", i)) %&gt;% \n            feols(\n                cases ~ i(city, treat_plb_temp * post) | city + month_num,\n                vcov = ~ city\n            ) %&gt;% \n            tidy() %&gt;% \n            mutate(\n                term = term,\n                !!paste0(\"plb_\", i, \"_pv\") := p.value,\n                .keep = \"none\"\n            )\n        }\n    ) %&gt;% \n    reduce(left_join, by = \"term\") %&gt;% \n    datasummary_df(title = \"Event Study of Placebos of cars2\")\n)\n\n\n\n\nShow Code\n#| fig-width: 10\n#| fig-height: 10\n\n\n# plot event study figure\n(\n    map(1:6,\n    function(i) {\n        cars2 %&gt;% \n        rename(treat_plb_temp = paste0(\"treat_plb_\", i)) %&gt;% \n        feols(\n            cases ~ sunab(treat_plb_temp, month_num) | city + month_num,\n            vcov = ~city\n        ) %&gt;% \n        ggiplot(main = paste(\"Placebo: \", i)) + labs(x = \"\", y = \"\")\n    }) %&gt;% \n    wrap_plots() +\n    plot_layout(2, 3) +\n    plot_annotation(title = \"Event Study of Placebos  of cars2\")\n)\n\n\n\n\nShow Code\n#| fig-width: 10\n#| fig-height: 10\n\n\n# plot heterogeneity figure\n(\n    map(1:6,\n        function(i) {\n            cars2 %&gt;% \n            rename(treat_plb_temp = paste0(\"treat_plb_\", i)) %&gt;% \n            feols(\n                cases ~ i(city, treat_plb_temp * post) | city + month_num,\n                vcov = ~ city\n            ) %&gt;% \n            ggiplot(main = paste(\"Placebo: \",i)) + labs(x = \"\", y = \"\")\n        }\n    ) %&gt;% \n    wrap_plots() + \n    plot_layout(2, 3) + \n    plot_annotation(title = \"Heterogeneity Test of Placebos  of cars2\")\n)"
  },
  {
    "objectID": "posts/econ/econ.html#overview",
    "href": "posts/econ/econ.html#overview",
    "title": "Practical Econometrics with R",
    "section": "3.1 Overview",
    "text": "3.1 Overview\n将 wide-table 清理为适合需要的 long-table 格式；Quater: 时间变量，可见数值从 1980 Q1 开始，但看不到结束的季度；Category: 分类变量，可见数值有 Beer, Tobacco, Bricks, Cement 等；Value: 数值变量，代表对应分类变量在当前季度的产量，已经取对数；\n\n\nShow Code\n# clean raw data to long data\nprod_long = (\n    aus_production %&gt;% \n    pivot_longer(\n        cols = -Quarter,\n        names_to = \"Category\",\n        values_to = \"Value\"\n    ) %&gt;% \n    mutate(Value = log(Value)) %&gt;% \n    filter((year(Quarter) &gt;= 1990) & (year(Quarter) &lt;= 2000))\n)\n\n# get wide data\nprod_wide = (\n    prod_long %&gt;% \n    pivot_wider(\n        id_cols = Quarter, names_from = \"Category\", values_from = \"Value\"\n    )\n)\n\n\n\n# show tidy table\nprod_wide %&gt;% slice_head(n = 5) %&gt;% datasummary_df(title = \"Wide Table of Production\")\n\n\n\n\nprod_long %&gt;% slice_head(n = 10) %&gt;% datasummary_df(title = \"Long Table of Production\")\n\n\n\n\nShow Code\n#| fig-width: 10\n#| fig-height: 10\n\n\n# plot overview\nfig1 = (\n    prod_long %&gt;% \n    autoplot(Value) +\n    scale_color_iterm() +\n    labs(title = \"Commodity Production\", x = \"\", y = \"Value\") +\n    theme_light()\n)\n\n\n\n# plot sub-seasonal values\nfig2 = (\n    prod_long %&gt;% \n    gg_subseries(y = Value) + \n    labs(title = \"Subseasonal Plot\", x = \"\", y = \"Value\") +\n    theme_light() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 0.5))\n)\n\n\n# plot lag  \nfig3 = (\n    prod_long %&gt;% \n    filter(Category == \"Beer\") %&gt;% \n    gg_lag(Value, geom = \"point\") +\n    scale_color_iterm() +\n    labs(title = \"Lag of Beer\", x = \"\", y = \"Lag\") +\n    theme_light()\n)\n\n\n# plot autocorrelation\nfig4 = (\n    prod_long %&gt;% \n    ACF(y = Value) %&gt;%\n    autoplot() +\n    labs(title = \"Autocorrelation of All\", x = \"Lag\", y = \"ACF\") +\n    theme_light() +\n    facet_wrap(~ Category)\n)\n\n\n# combine into one figure\nwrap_plots(fig1, fig2, fig3, fig4)\n\n\n对一个具有真正时间性质的时间序列数据，我们基于 Season-Trend-Loess Decomposition 的方法，可以将其拆解为如下结构：\n\\[\nY_t = T_t + S_t + C_t + R_t\n\\]\n\n\\(T_t\\) : trend，长期的增长或降低趋势；\n\\(S_t\\) : seasonality，短期的波动；\n\\(C_t\\) : circle，全局的在数据之外的周期特征；\n\\(R_t\\) : remainder，原始数据与预测数据之间的残差；\n\n\n\nShow Code\n# decompose components\nprod_long_comp = (\n    prod_long %&gt;% \n    model(mod_stl = STL(Value)) %&gt;% \n    components()\n)\n\n\n# show decomposition df\n(\n    prod_long_comp %&gt;% \n    slice_head(n = 10) %&gt;% \n    datasummary_df(title = \"Decomposition\")\n)\n\n\ntrend 反应 Beer 总体上确实在缓慢地下降； seasonal_year 反应 Beer 在下降过程中保持了稳定的周期性； remainder 反应 Beer 的下降过程比较平稳顺滑，不存在突然暴跌； remainder 是重中之重，将在下一页深入分析；\n\n\nShow Code\n# plot components of Beer\n(\n    prod_long_comp %&gt;% \n    filter(Category == \"Beer\") %&gt;% \n    autoplot(Value) + \n    labs(title = \"Decomposition of Beer\", x = \"\", y = \"Value\") +\n    theme_light()\n)"
  },
  {
    "objectID": "posts/econ/econ.html#modeling-1",
    "href": "posts/econ/econ.html#modeling-1",
    "title": "Practical Econometrics with R",
    "section": "3.2 Modeling",
    "text": "3.2 Modeling\nARIMA(p,d,q) 表示因变量只受到自己往期值的影响，没有其他外界变量影响；\n\\[\ny_t \\sim \\text{ARIMA(p,d,q)}\n\\]\n三条支线任务同时推进，将一个有较大周期波动的数据平滑为一个相对线性的数据；\nDif(d): 对原始数据进行 d 次差分计算，消除毛刺，趋于平滑；\n\\[\n\\Delta^{(d)} y_t = \\sum_{k=0}^{d} \\binom{d}{k} (-1)^k y_{t-k}\n\\]\nAR(p): 对原始数据进行 p 次自相关回归，消除毛刺，趋于平滑；\n\\[\nAR(p) = \\sum_{i=1}^{p} \\phi_i y_{t-i}\n\\]\nMA(q): 对原始数据进行 q 次移动回归计算，消除毛刺，趋于平滑；\n\\[\nMA(q) = \\sum_{j = 1}^q \\theta_j \\epsilon_{t - j} + \\epsilon_t\n\\]\n它的线性方程是：\n\\[\n\\begin{aligned}\n\\text{Dif(d)} &= \\text{AR}(p) + \\text{MA}(q) \\\\\n\\Delta^{(d)} y_t &=\n    c + \\sum_{i=1}^{p} \\phi_i \\Delta^d y_{t-i} +\n    \\sum_{j=1}^{q} \\theta_j \\epsilon_{t-j} + \\epsilon_t\n\\end{aligned}\n\\]\n代码显示 summary，coeffecients，accuracy 三张表格；总之，模型相关性和准确性较高，暂不展开，继续往下推导。\n\n\nShow Code\n# fit an ARIMA model\nprod_arima = prod_long %&gt;% model(mod_arima = ARIMA(Value))\n\n\n# show ARIMA summary\n(\n    prod_arima %&gt;% \n    glance() %&gt;% \n    select(-ar_roots, -ma_roots) %&gt;% \n    datasummary_df(title = \"Summary of ARIMA\")\n)\n\n\n# show coefficients\n(\n    prod_arima %&gt;% \n    coef() %&gt;% \n    slice_sample(n = 5) %&gt;% \n    datasummary_df(title = \"Samples of Coef of ARIMA\")\n)\n\n\n# show accuracy\n#prod_arima %&gt;% accuracy() %&gt;% datasummary_df(title = \"Accuracy of ARIMA\")"
  },
  {
    "objectID": "posts/econ/econ.html#prediction",
    "href": "posts/econ/econ.html#prediction",
    "title": "Practical Econometrics with R",
    "section": "3.3 Prediction",
    "text": "3.3 Prediction\n\n\nShow Code\n# extract predictions\nprod_arima_pred = (\n    forecast(prod_arima, h = 8) %&gt;% \n    mutate(pi80 = hilo(Value, 80)) %&gt;% \n    unpack_hilo(\"pi80\") %&gt;% \n    rename(pred_mean = .mean, pred_80lo = pi80_lower, pred_80hi = pi80_upper) %&gt;% \n    select(Quarter, Category, pred_mean, pred_80lo, pred_80hi)\n)\n\n\n# show in table\n(\n    prod_arima_pred %&gt;% \n    slice_head(n = 5) %&gt;% \n    datasummary_df(title = \"Predictions of ARIMA\")\n)\n\n\n\n\nShow Code\n# plot predictions with raw data\n(\n    ggplot() +\n    geom_line(\n        data = prod_long,\n        aes(x = Quarter, y = Value, color = Category)\n    ) +\n    geom_line(\n        data = prod_arima_pred, \n        aes(x = Quarter, y = pred_mean, color = Category),\n        linetype = \"dashed\"\n    ) +\n    geom_ribbon(\n        data = prod_arima_pred, \n        aes(x = Quarter, ymin = pred_80lo, ymax = pred_80hi, fill = Category),\n        alpha = 0.3\n    ) +\n    scale_color_iterm() +\n    labs(title = \"Predictions of 8 Quarters by ARIMA\", x = \"\", y = \"Value\") +\n    theme_light()\n)"
  },
  {
    "objectID": "posts/econ/econ.html#residuals-1",
    "href": "posts/econ/econ.html#residuals-1",
    "title": "Practical Econometrics with R",
    "section": "3.4 Residuals",
    "text": "3.4 Residuals\n我们在 1.3 对 GLM 进行了残差检验，包括正态性，独立性，同方差性；简要回顾，希望 AD, DW, BP Test 都满足 \\(p &gt; 0.05\\)，注意是不显著。为了保证 ARIMA 的准确性和严谨性，需要进行残差检验，理论过程一样；代码逻辑也跟 GLM 的逻辑一致，只是 ARIMA 在提取参数的流程上更加复杂；不讨论深层次的技术细节，让代码先跑起来，结果在后面列报。\n自相关性 Autocorrelation 是时间序列独有的残差检验；本期的残差应该跟过往的任何一期的残差无关，by Lung-Box test；\\(H_0\\): 残差之间是独立分布，即不存在自相关，反之则存在自相关；若 \\(p &lt; 0.05\\)，则拒绝原假设，即残差存在自相关，反之则无法拒绝原假设。\n总体来看，数据通过独立性、同方差、自相关检验，但未通过正态性检验；分类来看，Gas 未通过正态性检验，其余变量通过全部检验，考虑对 Gas 进行单独处理；对于严谨的学术论文，不能仅仅汇报总体结果，必须将分类变量拆分检测，同时汇报拆分的结果。\n\n\nShow Code\n# append residuals and fitted values to source\nprod_arima_aug = (\n    prod_arima %&gt;% \n    augment() %&gt;% \n    rename(arima_res = .resid, arima_fit = .fitted)\n)\n\n\n# check residuals of all categories\nprod_resi_all = (\n    prod_arima_aug %&gt;% \n    as_tibble() %&gt;% \n    summarise(\n        AD_pv = ad.test(arima_res) %&gt;% pluck(\"p.value\"),\n        DW_pv = dwtest(arima_res ~ 1) %&gt;% pluck(\"p.value\"),\n        BP_pv = bptest(arima_res ~ arima_fit) %&gt;% pluck(\"p.value\"),\n        LB_pv = ljung_box(arima_res) %&gt;% pluck(\"lb_pvalue\")\n    ) %&gt;% \n    mutate(Category = \"All\") %&gt;% \n    select(Category, everything())\n)\n\n\n# chech residuals of each category\nprod_resi_each = (\n    prod_arima_aug %&gt;% \n    as_tibble() %&gt;% \n    group_by(Category) %&gt;% \n    summarise(\n        AD_pv = ad.test(arima_res) %&gt;% pluck(\"p.value\"),\n        DW_pv = dwtest(arima_res ~ 1) %&gt;% pluck(\"p.value\"),\n        BP_pv = bptest(arima_res ~ arima_fit) %&gt;% pluck(\"p.value\"),\n        LB_pv = ljung_box(arima_res) %&gt;% pluck(\"lb_pvalue\")\n    )\n)\n\n\n# combine into single tibble\n(\n    bind_rows(prod_resi_all, prod_resi_each) %&gt;% \n    datasummary_df(title = \"Diagnostics of ARIMA\")\n)\n\n\n\n\nShow Code\n#| fig-width: 10\n#| fig-height: 10\n\n\n# check normality\nfig1 = (\n    prod_arima_aug %&gt;% \n    ggplot(aes(sample = arima_res, color = Category)) +\n    geom_qq() +\n    geom_qq_line() +\n    scale_color_iterm() +\n    labs(title = \" Normality\", x = \"Theoretical QQ\", y = \"Sample QQ\") +\n    theme_light()\n)\n\n\n# check independence\nfig2 = (\n    prod_arima_aug %&gt;% \n    ggplot(aes(x = seq_along(arima_res), y = arima_res, color = Category)) +\n    geom_point() +\n    scale_color_iterm() +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n    labs(title = \"Independence\", x = \"Index\", y = \"Residuals\") +\n    theme_light()\n)\n\n\n# check homosedasticity\nfig3 = (\n    prod_arima_aug %&gt;% \n    ggplot(aes(x = arima_fit, y = arima_res, color = Category)) +\n    geom_point() +\n    scale_color_iterm() +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n    labs(title = \"Homoscedasticity\", x = \"Fitted Values\", y = \"Residuals\") +\n    theme_light()\n)\n\n\n# check autocorrelation\nfig4 = (\n    prod_arima_aug %&gt;% \n    ACF(arima_res) %&gt;% \n    ggplot(aes(x = lag, y = acf, color = Category)) +\n    geom_segment(aes(xend = lag, yend = 0), linewidth = 1) +\n    geom_hline(aes(yintercept = 0, color = Category)) +\n    scale_color_iterm() +\n    geom_hline(aes(yintercept = 0.2), color = \"black\", linetype = \"dashed\") +\n    geom_hline(aes(yintercept = -0.2), color = \"black\", linetype = \"dashed\") +\n    labs(title = \"Autocorrelation\", x = \"Lags\", y = \"Autocorrelation\") +\n    theme_light() +\n    facet_wrap(~ Category, ncol = 2)\n)\n\n\n# combine into one figure\nwrap_plots(fig1, fig2, fig3, fig4)"
  },
  {
    "objectID": "posts/econ/econ.html#stationarity",
    "href": "posts/econ/econ.html#stationarity",
    "title": "Practical Econometrics with R",
    "section": "3.5 Stationarity",
    "text": "3.5 Stationarity\nStationarity 平稳性：数据的方差在整个时间维度内保持基本稳定，constant variance over time；几乎所有宏观经济数据都可以轻易处理为平稳，但是微观金融数据非常难以处理为平稳；平稳性检验是时间序列最重要的特征之一，是判断模型质量的重要标准，也是区分论文水平的标准。\n建模之前，KPSS 作用于原始数据，建模之后，KPSS 作用于残差数据；\\(H_0\\): 数据具有平稳性，反之则不具有平稳性；若 \\(p &lt; 0.05\\)，则拒绝原假设，认为数据不具有平稳性，反之则具有平稳性。\nARIMA 建模完成后，会在模型内存储一对 AR 和 MA 的实根和虚根；当且仅当 AR 和 MA 的根都在单位圆外，即 \\(r &gt; 1\\)，说明模型不随时间变化，即满足平稳性。若 AR 或 MA 的跟在单位圆内或圆周上，即 \\(r \\le 1\\)，说明模型随着时间会变化，即不满足平稳性；具体原理涉及到高维多项式求解，非常复杂，不展开，记住主要结论即可。\n建模前，只有 Bricks 满足平稳性，其余变量不满足平稳性，建模后，所有残差数据都满足平稳性；AR 和 MA 的根都在单位元外，满足平稳性要求，ARIMA 模型实现了数据从不平稳到平稳的转型。\n\n\nShow Code\n# test pre-and-post stationarity\nkpss_pre = (\n    prod_arima_aug %&gt;% \n    features(Value, unitroot_kpss) %&gt;% \n    rename(KPSS_pv_pre = kpss_pvalue) %&gt;% \n    select(Category, KPSS_pv_pre)\n)\n\n\nkpss_post = (\n    prod_arima_aug %&gt;% \n    features(arima_res, unitroot_kpss) %&gt;% \n    rename(KPSS_pv_post = kpss_pvalue) %&gt;% \n    select(Category, KPSS_pv_post)\n)\n\n\n# join into single table\nleft_join(kpss_pre, kpss_post) %&gt;% datasummary_df(title = \"Pre-and-Post Stationarity\")\n\n\n\n# plot by built-in function\n# prod_arima %&gt;% gg_arma() + theme_light()\n\n\n# plot by manually function\n# extract unit roots from ARIMA\nprod_arima_roots = (\n    prod_arima %&gt;% \n    glance() %&gt;% \n    unnest(ar_roots, keep_empty = TRUE) %&gt;% \n    unnest(ma_roots, keep_empty = TRUE) %&gt;% \n    mutate(\n        ar_roots_real = Re(ar_roots),\n        ar_roots_imag = Im(ar_roots),\n        ma_roots_real = Re(ma_roots),\n        ma_roots_imag = Im(ma_roots),\n    ) %&gt;% \n    select(-.model, -sigma2, -log_lik, -AIC, -AICc, -BIC) %&gt;% \n    pivot_longer(\n        cols = c(ar_roots_real, ar_roots_imag, ma_roots_real, ma_roots_imag),\n        names_to = \"roots_type\",\n        values_to = \"roots_value\"\n    ) %&gt;% \n    mutate(\n        roots_part = ifelse(grepl(\"real\", roots_type), \"roots_real\", \"roots_imag\"),\n        roots_type = ifelse(grepl(\"ar\", roots_type), \"AR\", \"MA\")\n    ) %&gt;% \n    pivot_wider(\n        names_from = roots_part,\n        values_from = roots_value\n    ) %&gt;% \n    select(Category, roots_type, roots_real, roots_imag)\n)\n\n\n# plot unit circle\n(\n    prod_arima_roots %&gt;%\n    ggplot(aes(x = roots_real, y = roots_imag, color = roots_type, shape = Category)) +\n    geom_point() +\n    geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = \"lightgray\") +\n    scale_color_iterm() +\n    labs(title = \"Inverse Unit Circle Test\", x = \"Real Parts\", y = \"Imaginary Parts\") +\n    theme_light() +\n    coord_equal(ratio = 1)\n)"
  },
  {
    "objectID": "posts/econ/econ.html#darimax",
    "href": "posts/econ/econ.html#darimax",
    "title": "Practical Econometrics with R",
    "section": "3.6 DARIMAX",
    "text": "3.6 DARIMAX\nDynamic ARIMA with Regressors，它在 ARIMAX 的基础上再进化一层；DARIMAX 不考虑实值，而考虑自变量经过 ARIMA 处理之后的残差 res 和预测 fit。残差 res 用来检验自变量对因变量的短期冲击效应 short-term shocks；预测 fit 用来检验自变量对因变量的长期影响效应 long-term effects；它的公式和模型为：\n\\[\ny_t \\sim \\text{ARIMA(p,d,q)} + \\text{Res} (X) + \\text{Fit} (X)\n\\]\nBeer ~ \n    ARIMAX(p, d, q) + \n    res_Tobacco + res_Bricks + res_Cement + res_Electricity + res_Gas +\n    fit_Tobacco + fit_Bricks + fit_Cement + fit_Electricity + fit_Gas\nCement 对 Beer 有短期负向冲击效果，其余变量对 Beer 短期效果不显著；Cement 对 Beer 有长期正向影响作用，其余变量对 Beer 长期影响不显著。\n\n\nShow Code\n# create dynamic ARIMA with regressors (DARIMAX)\n# \"res_xxx\": test short-term shocks between variables and y\n# \"fit_xxx\": test long-term effects between variables and y\nprod_darimax = (\n    prod_wide %&gt;% \n    mutate(\n        res_bricks = prod_wide %&gt;% model(ARIMA(Bricks)) %&gt;% residuals() %&gt;% pluck(\".resid\"),\n        res_cement = prod_wide %&gt;% model(ARIMA(Cement)) %&gt;% residuals() %&gt;% pluck(\".resid\"),\n        res_elec = prod_wide %&gt;% model(ARIMA(Electricity)) %&gt;% residuals() %&gt;% pluck(\".resid\"),\n        res_gas = prod_wide %&gt;% model(ARIMA(Gas)) %&gt;% residuals() %&gt;% pluck(\".resid\"),\n        res_tobacco = prod_wide %&gt;% model(ARIMA(Tobacco)) %&gt;% residuals() %&gt;% pluck(\".resid\"),\n        fit_bricks = prod_wide %&gt;% model(ARIMA(Bricks)) %&gt;% fitted() %&gt;% pluck(\".fitted\"),\n        fit_cement = prod_wide %&gt;% model(ARIMA(Cement)) %&gt;% fitted() %&gt;% pluck(\".fitted\"),\n        fit_elec = prod_wide %&gt;% model(ARIMA(Electricity)) %&gt;% fitted() %&gt;% pluck(\".fitted\"),\n        fit_gas = prod_wide %&gt;% model(ARIMA(Gas)) %&gt;% fitted() %&gt;% pluck(\".fitted\"),\n        fit_tobacco = prod_wide %&gt;% model(ARIMA(Tobacco)) %&gt;% fitted() %&gt;% pluck(\".fitted\")\n    ) %&gt;% \n    model(\n        mod_darimax = ARIMA(\n            Beer ~ \n                res_bricks + res_cement + res_elec + res_gas + res_tobacco + \n                fit_bricks + fit_cement + fit_elec + fit_gas + fit_tobacco\n            )\n    )\n)\n\n\n# show model summary\n(\n    prod_darimax %&gt;% \n    glance() %&gt;% \n    select(-ar_roots, -ma_roots) %&gt;% \n    datasummary_df(title = \"Summary of DARIMAX\")\n)\n\n\n# show model coef\nprod_darimax %&gt;% coef() %&gt;% datasummary_df(title = \"Coef of DARIMAX\")\n\n\n# show model accuracy\n#prod_darimax %&gt;% accuracy() %&gt;% datasummary_df(title = \"Accuracy of DARIMAX\")"
  },
  {
    "objectID": "posts/ml/ml.html",
    "href": "posts/ml/ml.html",
    "title": "Practical ML with R",
    "section": "",
    "text": "首先调取如下的工具包，暂时不做过多解释，每个都有其独特的功能，后续章节会逐步用到它们。\n\n\nShow Code\n# install and load pacman\nif (!requireNamespace(\"pacman\", quietly = TRUE)) {\n    install.packages(\"pacman\")\n}\n\n\n# load all common packages\npacman::p_load(\n    tidyverse, modelsummary, flextable, ggdag, dagitty, ggsci, ivreg, \n    marginaleffects, scales, pandoc, car, broom, nortest, lmtest, \n    patchwork, sandwich, GGally, fixest, ggfixest, fpp3, ggforce, urca, \n    readxl, writexl, tidyquant, PortfolioAnalytics, tsgarch, equatags,\n    ROI.plugin.quadprog, ROI.plugin.glpk, here\n)\n\n\n\n\nShow Code\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n# write a simple linear regression model in tidymodels\ncars = mtcars\n\n\n# set up recipe\ncars_reci = recipe(mpg ~ wt + hp + qsec, data = cars)\n\n\n# set up the model\ncars_mod = linear_reg() %&gt;% set_engine(\"lm\") %&gt;% set_mode(\"regression\")\n\n\n# set up the workflow\ncars_work = workflow() %&gt;% add_model(cars_mod) %&gt;% add_recipe(cars_reci)\n\n\n# fit the model\ncars_fit = cars_work %&gt;% \n    fit_resamples(\n        resamples = vfold_cv(cars, 10),\n        metrics = metric_set(rmse, rsq),\n        control = control_resamples(save_pred = TRUE)\n)\n\n\n# collect metrics\ncollect_metrics(cars_fit) %&gt;% datasummary_df()\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                .metric\n                .estimator\n                mean\n                n\n                std_err\n                .config\n              \n        \n        \n        \n                \n                  rmse\n                  standard\n                  2.59\n                  10.00\n                  0.36\n                  Preprocessor1_Model1\n                \n                \n                  rsq\n                  standard\n                  0.89\n                  10.00\n                  0.05\n                  Preprocessor1_Model1"
  }
]